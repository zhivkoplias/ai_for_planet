"key","title","year","month","day","journal","issn","volume","issue","pages","authors","url","language","publisher","location","abstract","notes","doi","keywords","pubmed_id","pmc_id"
"rayyan-100676904","A systematic review of trustworthy artificial intelligence applications in natural disasters",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Artificial intelligence (AI) holds significant promise for advancing natural disaster management through the use of predictive models that analyze extensive datasets, identify patterns, and forecast potential disasters. These models facilitate proactive measures such as early warning systems (EWSs), evacuation planning, and resource allocation, addressing the substantial challenges associated with natural disasters. This study offers a comprehensive exploration of trustworthy AI applications in natural disasters, encompassing disaster management, risk assessment, and disaster prediction. This research is underpinned by an extensive review of reputable sources, including Science Direct (SD), Scopus, IEEE Xplore (IEEE), and Web of Science (WoS). Three queries were formulated to retrieve 981 papers from the earliest documented scientific production until February 2024. After meticulous screening, deduplication, and application of the inclusion and exclusion criteria, 108 studies were included in the quantitative synthesis. This study provides a specific taxonomy of AI applications in natural disasters and explores the motivations, challenges, recommendations, and limitations of recent advancements. It also offers an overview of recent techniques and developments in disaster management using explainable artificial intelligence (XAI), data fusion, data mining, machine learning (ML), deep learning (DL), fuzzy logic, and multicriteria decision-making (MCDM). This systematic contribution addresses seven open issues and provides critical solutions through essential insights, laying the groundwork for various future works in trustworthiness AI-based natural disaster management. Despite the potential benefits, challenges persist in the application of AI to natural disaster management. In these contexts, this study identifies several unused and used areas in natural disaster-based AI theory, collects the disaster datasets, ML, and DL techniques, and offers a valuable XAI approach to unravel the complex relationships and dynamics involved and the utilization of data fusion techniques in decision-making processes related to natural disasters. Finally, the study extensively analyzed ethical considerations, bias, and consequences in natural disaster-based AI.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1016/j.compeleceng.2024.109409",NA,NA,NA
"rayyan-100676910","Machine learning for numerical weather and climate modelling: a review",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract. Machine learning (ML) is increasing in popularity in the field of weather and climate modelling. Applications range from improved solvers and preconditioners, to parameterization scheme emulation and replacement, and more recently even to full ML-based weather and climate prediction models. While ML has been used in this space for more than 25 years, it is only in the last 10 or so years that progress has accelerated to the point that ML applications are becoming competitive with numerical knowledge-based alternatives. In this review, we provide a roughly chronological summary of the application of ML to aspects of weather and climate modelling from early publications through to the latest progress at the time of writing. We also provide an overview of key ML terms, methodologies, and ethical considerations. Finally, we discuss some potentially beneficial future research directions. Our aim is to provide a primer for researchers and model developers to rapidly familiarize and update themselves with the world of ML in the context of weather and climate models.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.5194/gmd-16-6433-2023",NA,NA,NA
"rayyan-100676916","Cumulative absolute velocity prediction for earthquake early warning with deep learning",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate | USER-NOTES: {""Maria""=>[""A rapid and accurate estimate of earthquake damage is a key component in a successful earthquake early warning (EEW) system. The cumulative absolute velocity (CAV) is an important and widely used parameter to measure ground motion intensity, but it cannot be correctly estimated via the traditional approach with the limited information available in typical EEW systems. Therefore, current EEW systems cannot effectively use CAV to predict earthquake damage. Herein, a CAV prediction model (DLcav) based on convolutional neural networks was proposed for EEW systems. DLcav is an end-to-end solution to continuously predict CAV using arriving seismic waves of increasing length and supplemented with additional auxiliary information."", ""The effectiveness of DLcav to predict CAV was tested based on Japanese ground motion records, and the generalization ability of DLcav was assessed using the ground motion records from Chile. The results demonstrate that DLcav can rapidly predict CAV with good accuracy, which will help better estimate earthquake damage in EEW systems.""]}","https://doi.org/10.1111/mice.13065",NA,NA,NA
"rayyan-100676917","Harnessing AI and computing to advance climate modelling and prediction",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"There are contrasting views on how to produce the accurate predictions that are needed to guide climate change adaptation. Here, we argue for harnessing artificial intelligence, building on domain-specific knowledge and generating ensembles of moderately high-resolution (10–50 km) climate simulations as anchors for detailed hazard models.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1038/s41558-023-01769-3",NA,NA,NA
"rayyan-100676920","Prediction of PGA in earthquake early warning using a long short-term memory neural network",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate | USER-NOTES: {""Maria""=>[""Peak ground acceleration (PGA) is a key parameter used in earthquake early warning systems to measure the ground motion strength and initiate emergency protocols at major projects. The traditional P-wave peak displacement-dependent PGA prediction model (Pd-PGA model) tends to underestimate the PGA for large earthquakes because it cannot make full use of the fault continuity rupture information hidden in the time-varying process of ground motion. In this paper, a continuous PGA prediction long short-term memory (LSTM) neural network model is proposed. The model takes eight sequential features of stations that are proxies of the energy and other physical parameters as input and provides the recorded PGA at the station as the target output. "", ""A total of 5961 records from 119 earthquakes recorded by the Japanese Strong-Motion Earthquake Network (K-NET) in Japan are used to train the neural network and 3433 records from 73 earthquakes are used as the test set to verify the model’s generalization ability. The results show that within the same data set, the residuals of the predicted PGA for the proposed model are smaller than those of the Pd-PGA model and that the problem of PGA underestimation is resolved. The prediction accuracy also improves with increasing sequence length, which indicates that the LSTM neural network learns the rules hidden in the time series. To further verify the model’s generalization ability, the model performance is analyzed for an M 7.3 earthquake that was not included in the training or test data sets. The results show that the residuals of the predicted PGA for the event are consistent with those for the test data set, indicating that the model has good generalization ability.""]}","https://doi.org/10.1093/gji/ggad067",NA,NA,NA
"rayyan-100676923","Making food systems more resilient to food safety risks by including artificial intelligence, big data, and internet of things into food safety early warning and emerging risk identification tools",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract To enhance the resilience of food systems to food safety risks, it is vitally important for national authorities and international organizations to be able to identify emerging food safety risks and to provide early warning signals in a timely manner. This review provides an overview of existing and experimental applications of artificial intelligence (AI), big data, and internet of things as part of early warning and emerging risk identification tools and methods in the food safety domain. There is an ongoing rapid development of systems fed by numerous, real‐time, and diverse data with the aim of early warning and identification of emerging food safety risks. The suitability of big data and AI to support such systems is illustrated by two cases in which climate change drives the emergence of risks, namely, harmful algal blooms affecting seafood and fungal growth and mycotoxin formation in crops. Automation and machine learning are crucial for the development of future real‐time food safety risk early warning systems. Although these developments increase the feasibility and effectiveness of prospective early warning and emerging risk identification tools, their implementation may prove challenging, particularly for low‐ and middle‐income countries due to low connectivity and data availability. It is advocated to overcome these challenges by improving the capability and capacity of national authorities, as well as by enhancing their collaboration with the private sector and international organizations.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.1111/1541-4337.13296",NA,NA,NA
"rayyan-100676924","flood forecasting based on machine learning pattern recognition and dynamic migration of parameters",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Typical basin in semi-arid and semi humid areas in the middle reaches of the Yellow River Floods are among the most devastating natural disasters. Timely and accurate forecasting of runoff is crucial to safeguard human lives, minimize property damage, enhance the efficiency of reservoir power generation, and ensure the safety of water supply. In this study, the rainfall and flow data of 98 floods occurring between 1971 and 2014 in the Jingle sub-basin, a tributary of the Yellow River basin, China, were analyzed using dynamic clustering and random forest techniques to identify flood types and select appropriate model parameters. The Xin'anjiang model was then used for real-time flood forecasting. The results indicate that the rainfall characteristic indicators developed by the model can effectively identify potential flood types, and the model parameters determined by historical flood rates can be adapted and utilized for new forecasting tasks based on similarities. Ensemble forecasting results, which consider the probability of flood types, are superior to single fractal forecasting outcomes and diminish uncertainty. The proposed method can identify extreme flood events, facilitate flood classification and prediction, promote basin disaster mitigation, and enable the efficient use of water resources. The proposed flood classification and identification method effectively analyzes flood events in the basin. The floods that may occur are characterized by relevant statistical characteristics of rainfall, allowing for selection of corresponding flood forecasting model parameters for improved accuracy in real-time flood forecasting and extended prediction period.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1016/j.ejrh.2023.101406",NA,NA,NA
"rayyan-100676931","Earthquake Early Warning Starting From 3 s of Records on a Single Station With Machine Learning",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract We introduce the Ensemble Earthquake Early Warning System (E3WS), a set of Machine Learning (ML) algorithms designed to detect, locate, and estimate the magnitude of an earthquake starting from 3 s of P ‐waves recorded by a single station. The system is made of six Ensemble ML algorithms trained on attributes computed from ground acceleration time series in the temporal, spectral, and cepstral domains. The training set comprises data sets from Peru, Chile, Japan, and the STEAD global data set. E3WS consists of three sequential stages: detection, P‐phase picking, and source characterization. The latter involves magnitude, epicentral distance, depth, and back azimuth estimation. E3WS achieves an overall success rate in the discrimination between earthquakes and noise of 99.9%, with no false positive (noise mis‐classified as earthquakes) and very few false negatives (earthquakes mis‐classified as noise). All false negatives correspond to M ≤ 4.3 earthquakes, which are unlikely to cause any damage. For P‐phase picking, the Mean Absolute Error is 0.14 s, small enough for earthquake early warning purposes. For source characterization, the E3WS estimates are virtually unbiased, have better accuracy for magnitude estimation than existing single‐station algorithms, and slightly better accuracy for earthquake location. By updating estimates every second, the approach gives time‐dependent magnitude estimates that follow the earthquake source time function. E3WS gives faster estimates than present alert systems relying on multiple stations, providing additional valuable seconds for potential protective actions.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1029/2023jb026575",NA,NA,NA
"rayyan-100676933","Attention mechanism based neural networks for structural post-earthquake damage state prediction and rapid fragility analysis",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate | USER-NOTES: {""Maria""=>[""This paper is devoted to the research on applying the deep learning method to nonlinear structural post-disaster damage state assessment. Transformer and Informer networks with a classification network customized according to the adopted damage assessment framework are proposed for data-driven structural seismic response and damage state modeling. Compared with recurrent neural network and convolution neural network, the networks in this paper can predict the elastoplastic response of nonlinear structures more effectively. In addition, this paper presents a method for rapid structural fragility analysis, which can consider multiple damage assessment indexes at the same time."", ""The performance of the proposed approach is successfully demonstrated through two examples, including a numerical analysis validation and a field sensing validation. The results show that the Transformer network used in this paper is a reliable and computationally efficient approach for predicting the structural seismic response and damage category, and appears great potential in structural health monitoring and rapid assessment on post-disaster structural resilience.""]}","https://doi.org/10.1016/j.compstruc.2023.107038",NA,NA,NA
"rayyan-100676939","Deep Learning Regional Climate Model Emulators: A Comparison of Two Downscaling Training Frameworks",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract Regional climate models (RCMs) have a high computational cost due to their higher spatial resolution compared to global climate models (GCMs). Therefore, various downscaling approaches have been developed as a surrogate for the dynamical downscaling of GCMs. This study assesses the potential of using a cost‐efficient machine learning alternative to dynamical downscaling by using the example case study of emulating surface mass balance (SMB) over the Antarctic Peninsula. More specifically, we determine the impact of the training framework by comparing two training scenarios: (a) a perfect and (b) an imperfect model framework. In the perfect model framework, the RCM‐emulator learns only the downscaling function; therefore, it was trained with upscaled RCM (UPRCM) features at GCM resolution. This emulator accurately reproduced SMB when evaluated on UPRCM, but its predictions on GCM data conserved RCM‐GCM inconsistencies and led to underestimation. In the imperfect model framework, the RCM‐emulator was trained with GCM features and downscaled the GCM while exposed to RCM‐GCM inconsistencies. This emulator predicted SMB close to the truth, showing it learned the underlying inconsistencies and dynamics. Our results suggest that a deep learning RCM‐emulator can learn the proper GCM to RCM downscaling function while working directly with GCM data. Furthermore, the RCM‐emulator presents a significant computational gain compared to an RCM simulation. We conclude that machine learning emulators can be applied to produce fast and fine‐scaled predictions of RCM simulations from GCM data.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1029/2022ms003593",NA,NA,NA
"rayyan-100676944","Environmental drivers and spatial prediction of forest fires in the Western Ghats biodiversity hotspot, India: An ensemble machine learning approach",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"In ecologically sensitive areas like the Western Ghats, fire is largely considered to be the most significant management concern. Therefore, identifying and predicting the fire susceptible areas is crucial for managing protected area networks. In this study, we investigated the factors affecting forest fires and modelled the fire susceptibility in a human-dominated landscape in the central Western Ghats, India, by employing remote sensing, geographic information systems and machine learning techniques. We used six machine learning models (MLMs), including artificial neural network, random forest, generalized linear model, maximum entropy, multivariate adaptive regression splines, gradient boosting machine and the ensemble model, to relate the occurrence data of fires to 14 predictor variables categorized as climate, topography, vegetation, and anthropogenic disturbances to generate fire susceptibility maps. The area under the receiver operating characteristic (AUC-ROC), true skill statistic (TSS) curves, accuracy and continuous Boyce index (CBI) were used to assess the model's accuracy. We found that all models achieved acceptable performance while validating the independent test data. However, ensemble model had the best overall performance with an AUC-ROC = 0.93; TSS = 0.72; ACCURACY = 0.89; and CBI = 0.95. The land-use and land-cover, and the distance to the agricultural fields and settlements were the key determinants of fire occurrences in the study area based on the ensemble model. The results showed that the landscape has a high to very high fire risk for about 26.5% of the total area. Forest fires have mostly occurred in the northeastern regions of the landscape, which are dominated by deciduous forests and plantations, whereas western regions are less susceptible to fires. Finally, we developed an ensemble-based fire susceptibility map with a resolution of 10 m that may be used as a tool for the prevention of forest fires. Further, it aids the relevant authorities in mitigating the disastrous effects and safeguarding the environment. The results demonstrated that ensemble-based MLMs can be employed effectively for fire prediction in other regions.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1016/j.foreco.2023.121057",NA,NA,NA
"rayyan-100676950","Prediction of maximum air temperature for defining heat wave in Rajasthan and Karnataka states of India using machine learning approach",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate | USER-NOTES: {""Maria""=>[""Temperature rise, associated with global warming, has increased the severity and frequency of heat waves around the world. Rajasthan and Karnataka are two major states of India and these have been facing frequent heat waves in the past few years. The present study used machine learning approach to predict the maximum air temperature (AT) for defining the heat wave occurrences in these two states. The analysis was based on the monthly data of 13 parameters, collected from NASA's Giovanni and ERA5 reanalysis data during the summer season for the past 10-years (2013–2022). The data obtained were at different resolutions, which was resampled to 5 km × 5 km spatial resolution. Pearson's correlation and sensitivity analysis were used to check the dominant input parameters. Three machine learning approaches were used in the study: multiple linear regression (MLR), support vector regression (SVR), and random forest (RF)."", ""3-fold cross validation was used to evaluate the model performance. The data was divided into three periods for the purpose of testing, training and validation. It was observed that the maximum AT in both states was above the limits of IMD criteria for defining heat wave occurrences. The performance of the models was evaluated using statistical metrics, comprising of root mean square error (RMSE) and coefficient of determination (R2). Good correlation of AT with land surface temperature (LST), black carbon (BC), AOD, and CO was found for both the states. AT was found to be more sensitive to the change in LST and BC compared to other parameters. The machine learning results indicated that RF outperformed MLR and SVR in predicting AT in both states."", "" The adjusted R2 was 0.90 and 0.92, for Rajasthan and Karnataka, respectively while RMSE was 2.36% and 1.44%, for Rajasthan and Karnataka, respectively. Overall, the study shows that machine learning-based approaches can predict maximum AT for defining heat wave conditions with high accuracy. The study can have significantly applications in different fields like climate modelling studies, urban planning and infrastructure, agriculture etc. and it can help to implement appropriate measures to mitigate the adverse impacts of temperature rise.""]}","https://doi.org/10.1016/j.rsase.2023.101048",NA,NA,NA
"rayyan-100676954","LSTM-CM: a hybrid approach for natural drought prediction based on deep learning and climate models",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate | USER-NOTES: {""Maria""=>[""Droughts cause severe damage to the economy, society, and environment. Drought forecasting plays an important role in establishing mitigation drought damage plans. In this study, a hybrid model involving long short-term memory and a climate model (LSTM-CM) is constructed for drought prediction. LSTM-CM was compared to the long short-term model stand-alone (LSTM-SA) and climate prediction model GloSea5 (GS5). The performance of models was evaluated based on the Pearson correlation coefficient (CC), mean absolute error (MAE), root mean squared error (RMSE), and skill score (SS). GS5 displayed physical robustness in predictions and did not reduce the amplitude or shift results."", ""However, GS5 prediction tends to have a large bias caused by the inputs, model structure, and parameters. The MAEs of GS5 at 1, 2 and 3 months (0.41, 0.68, and 0.89) were higher than those of LSTM-SA (0.38, 0.61, and 0.89). The LSTM-SA reduced bias, but predictions were characterized by shifts, small variance, and failure to capture drought occurrences in long-lead-time cases. LSTM-CM yielded enhanced drought predictions by encompassing the low bias of LSTM-SA and the physical process simulation ability of GS5; thus, it inherited the good features of these models and limited the poor features. The SS values based on the CC, MAE, and RMSE of LSTM-CM compared to those of GS5 for 1-, 2-, and 3-month lead time predictions were improved from 29.17 to 54.29, 22.47 to 34.15, and 1.75 to 35.09%, respectively. LSTM-CM can accurately detect drought events and displayed less uncertainty in prediction than LSTM-SA and GS5.""]}","https://doi.org/10.1007/s00477-022-02378-w",NA,NA,NA
"rayyan-100676972","SBAS-InSAR based validated landslide susceptibility mapping along the Karakoram Highway: a case study of Gilgit-Baltistan, Pakistan",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Geological settings of the Karakoram Highway (KKH) increase the risk of natural disasters, threatening its regular operations. Predicting landslides along the KKH is challenging due to limitations in techniques, a challenging environment, and data availability issues. This study uses machine learning (ML) models and a landslide inventory to evaluate the relationship between landslide events and their causative factors. For this, Extreme Gradient Boosting (XGBoost), Random Forest (RF), Artificial Neural Network (ANN), Naive Bayes (NB), and K Nearest Neighbor (KNN) models were used. A total of 303 landslide points were used to create an inventory, with 70% for training and 30% for testing. Susceptibility mapping used Fourteen landslide causative factors. The area under the curve (AUC) of a receiver operating characteristic (ROC) is employed to compare the accuracy of the models. The deformation of generated models in susceptible regions was evaluated using SBAS-InSAR (Small-Baseline subset-Interferometric Synthetic Aperture Radar) technique. The sensitive regions of the models showed elevated line-of-sight (LOS) deformation velocity. The XGBoost technique produces a superior Landslide Susceptibility map (LSM) for the region with the integration of SBAS-InSAR findings. This improved LSM offers predictive modeling for disaster mitigation and gives a theoretical direction for the regular management of KKH.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1038/s41598-023-30009-z",NA,NA,NA
"rayyan-100676979","A systematic review of natural language processing applications for hydrometeorological hazards assessment",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Natural language processing (NLP) is a promising tool for collecting data that are usually hard to obtain during extreme weather, like community response and infrastructure performance. Patterns and trends in abundant data sources such as weather reports, news articles, and social media may provide insights into potential impacts and early warnings of impending disasters. This paper reviews the peer-reviewed studies (journals and conference proceedings) that used NLP to assess extreme weather events, focusing on heavy rainfall events. The methodology searches four databases (ScienceDirect, Web of Science, Scopus, and IEEE Xplore) for articles published in English before June 2022. The preferred reporting items for systematic reviews and meta-analysis reviews and meta-analysis guidelines were followed to select and refine the search. The method led to the identification of thirty-five studies. In this study, hurricanes, typhoons, and flooding were considered. NLP models were implemented in information extraction, topic modeling, clustering, and classification. The findings show that NLP remains underutilized in studying extreme weather events. The review demonstrated that NLP could potentially improve the usefulness of social media platforms, newspapers, and other data sources that could improve weather event assessment. In addition, NLP could generate new information that should complement data from ground-based sensors, reducing monitoring costs. Key outcomes of NLP use include improved accuracy, increased public safety, improved data collection, and enhanced decision-making are identified in the study. On the other hand, researchers must overcome data inadequacy, inaccessibility, nonrepresentative and immature NLP approaches, and computing skill requirements to use NLP properly.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1007/s11069-023-05842-0",NA,NA,NA
"rayyan-100676982","Deep Learning and Machine Learning Models for Landslide Susceptibility Mapping with Remote Sensing Data",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Karakoram Highway (KKH) is an international route connecting South Asia with Central Asia and China that holds socio-economic and strategic significance. However, KKH has extreme geological conditions that make it prone and vulnerable to natural disasters, primarily landslides, posing a threat to its routine activities. In this context, the study provides an updated inventory of landslides in the area with precisely measured slope deformation (Vslope), utilizing the SBAS-InSAR (small baseline subset interferometric synthetic aperture radar) and PS-InSAR (persistent scatterer interferometric synthetic aperture radar) technology. By processing Sentinel-1 data from June 2021 to June 2023, utilizing the InSAR technique, a total of 571 landslides were identified and classified based on government reports and field investigations. A total of 24 new prospective landslides were identified, and some existing landslides were redefined. This updated landslide inventory was then utilized to create a landslide susceptibility model, which investigated the link between landslide occurrences and the causal variables. Deep learning (DL) and machine learning (ML) models, including convolutional neural networks (CNN 2D), recurrent neural networks (RNNs), random forest (RF), and extreme gradient boosting (XGBoost), are employed. The inventory was split into 70% for training and 30% for testing the models, and fifteen landslide causative factors were used for the susceptibility mapping. To compare the accuracy of the models, the area under the curve (AUC) of the receiver operating characteristic (ROC) was used. The CNN 2D technique demonstrated superior performance in creating the landslide susceptibility map (LSM) for KKH. The enhanced LSM provides a prospective modeling approach for hazard prevention and serves as a conceptual reference for routine management of the KKH for risk assessment and mitigation.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.3390/rs15194703",NA,NA,NA
"rayyan-100676984","A comparative study of Machine Learning and Deep Learning methods for flood forecasting in the Far-North region, Cameroon",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Flood crises are the consequence of climate change and global warming, which lead to an increase in the frequency and intensity of heavy rainfall. Floods are, and remain, natural disasters that result in huge loss of lives and material damage. Flood risks threaten all countries of the globe in general. The Far-North region of Cameroon has suffered of flood crises on several occasions, resulting in significant loss of human lives, infrastructural and socio-economic damage, with the destruction of homes, crops and grazing areas, and the halting of economic activities. The models used for flood forecasting in this region are generally physical-based, and produce unsatisfactory results. The use of artificial intelligence based methods for flood forecasting in order to limit its consequences is a way to be explored in the Far-North region of Cameroon. The aims of the present research work is to design and compare the performance of Machine Learning and Deep Learning based models such as one dimensional Convolutional Neural Network, Long and Short Term Memory and Multi Layer Perceptron for short-term and long-term flood forecasting in the Far-North region of Cameroon. The models designed take as input the temperature and rainfall time series recorded in this region. Performance criteria used for evaluating models are Nash–SutcliffeEfficiency, Percent Bias, Coefficient of Determination and Root Mean Squared Error. As the results of the design and performance comparison of the models, the best model for short-term flood forecasting is the LSTM model , and the best model for long-term flood forecasting is still the LSTM model. The best models obtained from the comparisons have satisfactory performance and good generalization capabilities, as reflected by the performance criteria. The results of our research work can be used for implementation of floods warning systems and for definition of an effective and efficient flood risk management policies in order to make the Far-North region of Cameroon more resilient to flood crises.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1016/j.sciaf.2023.e02053",NA,NA,NA
"rayyan-100676985","A New Graph-Based Deep Learning Model to Predict Flooding with Validation on a Case Study on the Humber River",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Floods are one of the most lethal natural disasters. It is crucial to forecast the timing and evolution of these events and create an advanced warning system to allow for the proper implementation of preventive measures. This work introduced a new graph-based forecasting model, namely, graph neural network sample and aggregate (GNN-SAGE), to estimate river flooding. It then validated the proposed model in the Humber River watershed in Ontario, Canada. Using past precipitation and stage data from reference and neighboring stations, the proposed GNN-SAGE model could estimate the river stage for flooding events up to 24 h ahead, improving its forecasting performance by an average of 18% compared with the persistence model and 9% compared with the graph-based model residual gated graph convolutional network (GNN-ResGated), which were used as baselines. Furthermore, GNN-SAGE generated smaller errors than those reported in the current literature. The Shapley additive explanations (SHAP) revealed that prior data from the reference station was the most significant factor for all prediction intervals, with seasonality and precipitation being more influential for longer-range forecasts. The findings positioned the proposed GNN-SAGE model as a cutting-edge solution for flood forecasting and a valuable resource for devising early flood-warning systems.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.3390/w15101827",NA,NA,NA
"rayyan-100676986","IoT-Driven Image Recognition for Microplastic Analysis in Water Systems using Convolutional Neural Networks",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene | USER-NOTES: {""Maria""=>[""Microplastic pollution in water systems is growing, requiring novel detection and analysis methods. This research presents an Internet of Things (IoT)-driven image identification system using Convolutional Neural Networks (CNNs) to detect and quantify microplastics in water samples. The suggested method is more scalable and responsive due to IoT real-time data capture and remote monitoring of water infrastructure. An innovative CNN architecture for image processing allows the system to accurately identify micro plastics. The CNN model is trained and validated using a large dataset of micro plastic-containing water samples. The trained model can recognize various sizes, shapes, and colors of micro plastics, making it responsive to different environmental situations."", ""The IoT architecture also allows image recognition modules in dispersed sensor nodes to cover water systems. Extensive studies prove the system can analyze vast amounts of image data quickly and reliably. Edge computing also minimizes latency and improves micro plastic analysis system responsiveness. The suggested IoT-driven image recognition method for continuous micro plastic pollution monitoring and evaluation in water systems seems promising. Scalability, realtime capabilities, and accuracy make it useful for environmental monitoring agencies and academics trying to reduce microplastics’ influence on aquatic ecosystems. This system advances IoT applications in environmental and pollution management.""]}","https://doi.org/10.1109/ic457434.2024.10486490",NA,NA,NA
"rayyan-100676987","AI-enabled strategies for climate change adaptation: protecting communities, infrastructure, and businesses from the impacts of climate change",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract Climate change is one of the most pressing global challenges we face today. The impacts of rising temperatures, sea levels, and extreme weather events are already being felt around the world and are only expected to worsen in the coming years. To mitigate and adapt to these impacts, we need innovative, data-driven solutions. Artificial intelligence (AI) has emerged as a promising tool for climate change adaptation, offering a range of capabilities that can help identify vulnerable areas, simulate future climate scenarios, and assess risks and opportunities for businesses and infrastructure. With the ability to analyze large volumes of data from climate models, satellite imagery, and other sources, AI can provide valuable insights that can inform decision-making and help us prepare for the impacts of climate change. However, the use of AI in climate change adaptation also raises important ethical considerations and potential biases that must be addressed. As we continue to develop and deploy these solutions, it is crucial to ensure that they are transparent, fair, and equitable. In this context, this article explores the latest innovations and future directions in AI-enabled climate change adaptation strategies, highlighting both the potential benefits and the ethical considerations that must be considered. By harnessing the power of AI for climate change adaptation, we can work towards a more resilient, sustainable, and equitable future for all.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1007/s43762-023-00100-2",NA,NA,NA
"rayyan-100676988","Hydrological drought forecasting and monitoring system development using artificial neural network (ANN) in Ethiopia",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"The objective of this study is to investigate and perform long-term forecasting of both streamflow and hydrological drought over Ethiopia. Observed streamflow and precipitation data are collected from 17 streamflow stations and 34 rainfall gauge stations to forecast future streamflow and hydrological drought from 2026 to 2099. Streamflow forecasting is performed using an artificial neural network (ANN) in conjunction with python software. Observed precipitation and streamflow data from 1973 to 2014 are used to train and test the ANN model by 70 and 30% ratios, respectively. After training the model, future downscaled precipitation data from regional climate models (RCM) have been used as input data to forecast future streamflow. Three RCM models were used to downscale historical and future climate data. RACMO is found a good downscaling model for all selected stations. The linear scaling bias correction technique results in less than 2% error compared to other alternative techniques. The result indicates that ANN is a good tool to forecast streamflow in areas having a good correlation between precipitation and streamflow such as Abbay, Awash, Baro, Omo Gibe, and Tekeze river basins. But in arid areas for example Genale Dawa, Wabishebele, and Rift Valley basins, the model is not suitable because the input data (precipitation) have high variation than the output variable (streamflow). In such areas, meteorological drought analysis and forecasting are better than hydrological drought analysis. Finally, future hydrological drought is analyzed using forecasted streamflow data as input to the streamflow drought index (SDI). The result indicates that 2028, 2036, 2042, 2044, 2062, and 2063 are the expected extreme drought years in most river basins of Ethiopia in the future. This shows that at least one extreme drought is expected in each decade in the future. Therefore, extensive research in drought analysis and forecasting is needed to develop an effective drought early warning system, and water resource management policy.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1016/j.heliyon.2023.e13287",NA,NA,NA
"rayyan-100676997","Creation of wildfire susceptibility maps in the Mediterranean Region (Turkey) using convolutional neural networks and multilayer perceptron techniques",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate | USER-NOTES: {""Maria""=>[""Considering the natural disasters that have developed in the world in recent years, it is known that there is an increase in wildfire disasters with the effects of climate change. In this study, wildfire susceptible areas were determined in the provinces of Muğla, Antalya, Mersin, Adana, Osmaniye, and Hatay in the Mediterranean region (Turkey). Within the scope of this purpose, Convolutional Neural Network (CNN) and Multilayer Perceptron (MLP) methods, the most widely used deep learning techniques in the literature in recent years, were preferred to create Wildfire susceptibility models. Seventeen environmental variables were used in the analyses, and these variables were grouped as topographic factors, anthropological and environmental factors, climatic factors, and vegetation factors. "", ""In addition, the number of fire inventory data has been balanced with the help of the Synthetic Minority Oversampling Technique (SMOTE) used to increase the model result performance of the scarce inventory data. In the maps obtained by CNN and MLP methods, 17% and 28% of the study area were determined as high and very high susceptible areas, respectively. The results demonstrated that the CNN model had superior performance in Wildfire susceptibility assessment with accuracy (%85.8), precision (%98.7), sensitivity (%85.5), F- Score (%91.6), and ROC curve (%78.6). This model was followed by the MLP model with slightly lower accuracy values, which indicates that the CNN models can reach considerably better prediction capability than the MLP models. Finally, the wildfire susceptibility maps produced by deep learning methods could aid decision-makers and government organizations in the Mediterranean region in preventing future natural disasters.""]}","https://doi.org/10.1016/j.foreco.2023.121006",NA,NA,NA
"rayyan-100677000","Impact of extreme weather in production economics: Extracting evidence from user-generated content",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"The last decade has witnessed an increase in the number of extreme weather events globally. In addition, the economic output around the world is at all-time high in terms of production and profitability. However, global warming and extreme weather are modifying the natural ecosystem and the human social system, leading to the appearance of extreme climate events that have an adverse impact on the world economy. To address this challenge, the present study identifies the main impacts of extreme weather on production economics based on the analysis of user-generated content (UGC) on the social network Twitter. Methodologically, a sentiment analysis with machine learning is developed and applied to analyze a sample of 1.4 m tweets; in addition, computing experiments to calculate the accuracy with Support Vector Classifier, Multinomial Naïve Bayes, Logistic Regression, and Random Forest Classifier are conducted. Second, a topic modeling known as latent Dirichlet allocation is applied to divide sentiment-classified tweets into topics. To complement these approaches, we also use the technique of textual analysis. These approaches are used under the framework of computer-aided test analysis system and natural language processing. The results are discussed and linked to appraisal theory. A total of 7 topics are identified, including positive (Sustainable energies and Green Entrepreneurs), neutral (Climate economy, Producer's productivity and Stock market), and negative (Economy and policy and Climate emergence). Finally, the present study discusses how the recent trend of an increase in extreme weather conditions has significantly impacted international markets, leading companies to adapt their business models and production systems accordingly. The results show that the climate economy and policy, producers' productivity, and the stock market are all heavily influenced by extreme weather and can have significant effects on the global economy.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1016/j.ijpe.2023.108861",NA,NA,NA
"rayyan-100677001","Flood Forecasting Using Hybrid LSTM and GRU Models with Lag Time Preprocessing",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Climate change and urbanization have increased the frequency of floods worldwide, resulting in substantial casualties and property loss. Accurate flood forecasting can offer governments early warnings about impending flood disasters, giving them a chance to evacuate and save lives. Deep learning is used in flood forecasting to improve the timeliness and accuracy of flood water level predictions. While various deep learning models similar to Long Short-Term Memory (LSTM) have achieved notable results, they have complex structures with low computational efficiency, and often lack generalizability and stability. This study applies a spatiotemporal Attention Gated Recurrent Unit (STA-GRU) model for flood prediction to increase the models’ computing efficiency. Another salient feature of our methodology is the incorporation of lag time during data preprocessing before the training of the model. Notably, for 12-h forecasting, the STA-GRU model’s R-squared (R2) value increased from 0.8125 to 0.9215. Concurrently, the model manifested reduced root mean squared error (RMSE) and mean absolute error (MAE) metrics. For a more extended 24-h forecasting, the R2 value of the STA-GRU model improved from 0.6181 to 0.7283, accompanied by diminishing RMSE and MAE values. Seven typical deep learning models—the LSTM, the Convolutional Neural Networks LSTM (CNNLSTM), the Convolutional LSTM (ConvLSTM), the spatiotemporal Attention Long Short-Term Memory (STA-LSTM), the GRU, the Convolutional Neural Networks GRU (CNNGRU), and the STA-GRU—are compared for water level prediction. Comparative analysis delineated that the use of the STA-GRU model and the application of the lag time pre-processing method significantly improved the reliability and accuracy of flood forecasting.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.3390/w15223982",NA,NA,NA
"rayyan-100677017","Deep learning of model- and reanalysis-based precipitation and pressure mismatches over Europe",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Physically based numerical weather prediction and climate models provide useful information for a large number of end users, such as flood forecasters, water resource managers, and farmers. However, due to model uncertainties arising from, e.g., initial value and model errors, the simulation results do not match the in situ or remotely sensed observations to arbitrary accuracy. Merging model-based data with observations yield promising results benefiting simultaneously from the information content of the model results and observations. Machine learning (ML) and/or deep learning (DL) methods have been shown to be useful tools in closing the gap between models and observations due to the capacity in the representation of the non-linear space–time correlation structure. This study focused on using UNet encoder–decoder convolutional neural networks (CNNs) for extracting spatiotemporal features from model simulations for predicting the actual mismatches (errors) between the simulation results and a reference data set. Here, the climate simulations over Europe from the Terrestrial Systems Modeling Platform (TSMP) were used as input to the CNN. The COSMO-REA6 reanalysis data were used as a reference. The proposed merging framework was applied to mismatches in precipitation and surface pressure representing more and less chaotic variables, respectively. The merged data show a strong average improvement in mean error (~ 47%), correlation coefficient (~ 37%), and root mean square error (~22%). To highlight the performance of the DL-based method, the results were compared with the results obtained by a baseline method, quantile mapping. The proposed DL-based merging methodology can be used either during the simulation to correct model forecast output online or in a post-processing step, for downstream impact applications, such as flood forecasting, water resources management, and agriculture.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.3389/frwa.2023.1178114",NA,NA,NA
"rayyan-100677021","Skillful Extended-Range Forecast of Rainfall and Extreme Events in East China Based on Deep Learning",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate | USER-NOTES: {""Maria""=>[""The extended-range forecast with a lead time of 10–30 days is the gap between weather (<10 days) and climate (>30 days) predictions. Improving the forecast skill of extreme weather events at the extended range is crucial for risk management of disastrous events. In this study, three deep learning (DL) models based on the methods of convolutional neural networks and gate recurrent units are constructed to predict the rainfall anomalies and associated extreme events in East China at lead times of 1–6 pentads. All DL models show skillful prediction of the temporal variation of rainfall anomalies (in terms of temporal correlation coefficient skill) over most regions in East China beyond 4 pentads, outperforming the dynamical models from the China Meteorological Administration (CMA) and the European Centre for Medium-Range Weather Forecasts (ECMWF). "", ""The spatial distribution of the rainfall anomalies is also better predicted by the DL models than the dynamical models; and the DL models show higher pattern correlation coefficients than the dynamical models at lead times of 3–6 pentads. The higher skill of DL models in predicting the rainfall anomalies will help to improve the accuracy of extreme-event predictions. The Heidke skill scores of the extreme rainfall event forecast performed by the DL models are also superior to those of the dynamical models at a lead time beyond about 4 pentads. Heat map analysis for the DL models shows that the predictability sources are mainly the large-scale factors modulating the East Asian monsoon rainfall.""]}","https://doi.org/10.1175/waf-d-22-0132.1",NA,NA,NA
"rayyan-100677033","Explainable artificial intelligence in disaster risk management: Achievements and prospective futures",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Disasters can have devastating impacts on communities and economies, underscoring the urgent need for effective strategic disaster risk management (DRM). Although Artificial Intelligence (AI) holds the potential to enhance DRM through improved decision-making processes, its inherent complexity and ""black box"" nature have led to a growing demand for Explainable AI (XAI) techniques. These techniques facilitate the interpretation and understanding of decisions made by AI models, promoting transparency and trust. However, the current state of XAI applications in DRM, their achievements, and the challenges they face remain underexplored. In this systematic literature review, we delve into the burgeoning domain of XAI-DRM, extracting 195 publications from the Scopus and ISI Web of Knowledge databases, and selecting 68 for detailed analysis based on predefined exclusion criteria. Our study addresses pertinent research questions, identifies various hazard and disaster types, risk components, and AI and XAI methods, uncovers the inherent challenges and limitations of these approaches, and provides synthesized insights to enhance their explainability and effectiveness in disaster decision-making. Notably, we observed a significant increase in the use of XAI techniques for DRM in 2022 and 2023, emphasizing the growing need for transparency and interpretability. Through a rigorous methodology, we offer key research directions that can serve as a guide for future studies. Our recommendations highlight the importance of multi-hazard risk analysis, the integration of XAI in early warning systems and digital twins, and the incorporation of causal inference methods to enhance DRM strategy planning and effectiveness. This study serves as a beacon for researchers and practitioners alike, illuminating the intricate interplay between XAI and DRM, and revealing the profound potential of AI solutions in revolutionizing disaster risk management.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1016/j.ijdrr.2023.104123",NA,NA,NA
"rayyan-100677036","An ensemble neural network approach to forecast Dengue outbreak based on climatic condition",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Dengue fever is a virulent disease spreading over 100 tropical and subtropical countries in Africa, the Americas, and Asia. This arboviral disease affects around 400 million people globally, severely distressing the healthcare systems. The unavailability of a specific drug and ready-to-use vaccine makes the situation worse. Hence, policymakers must rely on early warning systems to control intervention-related decisions. Forecasts routinely provide critical information for dangerous epidemic events. However, the available forecasting models (e.g., weather-driven mechanistic, statistical time series, and machine learning models) lack a clear understanding of different components to improve prediction accuracy and often provide unstable and unreliable forecasts. This study proposes an ensemble wavelet neural network with exogenous factor(s) (XEWNet) model that can produce reliable estimates for dengue outbreak prediction for three geographical regions, namely San Juan, Iquitos, and Ahmedabad. The proposed XEWNet model is flexible and can easily incorporate exogenous climate variable(s) confirmed by statistical causality tests in its scalable framework. The proposed model is an integrated approach that uses wavelet transformation into an ensemble neural network framework that helps in generating more reliable long-term forecasts. The proposed XEWNet allows complex non-linear relationships between the dengue incidence cases and rainfall; however, mathematically interpretable, fast in execution, and easily comprehensible. The proposal's competitiveness is measured using computational experiments based on various statistical metrics and several statistical comparison tests. In comparison with statistical, machine learning, and deep learning methods, our proposed XEWNet performs better in 75% of the cases for short-term and long-term forecasting of dengue incidence.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.1016/j.chaos.2023.113124",NA,NA,NA
"rayyan-100677042","Drought Prediction: A Comprehensive Review of Different Drought Prediction Models and Adopted Technologies",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Precipitation deficit conditions and temperature anomalies are responsible for the occurrence of various types of natural disasters that cause tremendous loss of human life and economy of the country. Out of all natural disasters, drought is one of the most recurring and complex phenomenons. Prediction of the onset of drought poses significant challenges to societies worldwide. Drought occurrences occur across the world due to a variety of hydro-meteorological causes and anomalies in sea surface temperature. This article aims to provide a comprehensive overview of the fundamental concepts and characteristics of drought, its complex nature, and the various factors that influence drought, drought indicators, and advanced drought prediction models. An extensive survey is presented in the different drought prediction models employed in the literature, ranging from statistical approaches to machine learning and deep learning models. It has been found that advanced techniques like machine learning and deep learning models outperform traditional models by improving drought prediction accuracy. This review article critically examines the advancements in technology that have facilitated improved drought prediction, identifies the key challenges and opportunities in the field of drought prediction, and identifies the key trends and topics that are likely to give new directions to the future of drought prediction research. It explores the integration of remote sensing data, meteorological observations, hydrological modeling, and climate indices for enhanced accuracy. Under the frequently changing climate conditions, this comprehensive review provides a valuable resource for researchers, practitioners, and policymakers engaged in drought prediction and management and fosters a deeper understanding of their capabilities and limitations. This article paves the way for more accurate and effective drought prediction strategies, contributing to improved resilience and sustainable development in drought-prone regions.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.3390/su151511684",NA,NA,NA
"rayyan-100677048","Deep learning for early warning signals of tipping points",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Many natural systems exhibit tipping points where slowly changing environmental conditions spark a sudden shift to a new and sometimes very different state. As the tipping point is approached, the dynamics of complex and varied systems simplify down to a limited number of possible ""normal forms"" that determine qualitative aspects of the new state that lies beyond the tipping point, such as whether it will oscillate or be stable. In several of those forms, indicators like increasing lag-1 autocorrelation and variance provide generic early warning signals (EWS) of the tipping point by detecting how dynamics slow down near the transition. But they do not predict the nature of the new state. Here we develop a deep learning algorithm that provides EWS in systems it was not explicitly trained on, by exploiting information about normal forms and scaling behavior of dynamics near tipping points that are common to many dynamical systems. The algorithm provides EWS in 268 empirical and model time series from ecology, thermoacoustics, climatology, and epidemiology with much greater sensitivity and specificity than generic EWS. It can also predict the normal form that characterizes the oncoming tipping point, thus providing qualitative information on certain aspects of the new state. Such approaches can help humans better prepare for, or avoid, undesirable state transitions. The algorithm also illustrates how a universe of possible models can be mined to recognize naturally occurring tipping points.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1073/pnas.2106140118",NA,NA,NA
"rayyan-100677051","Artificial intelligence, systemic risks, and sustainability",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Automated decision making and predictive analytics through artificial intelligence, in combination with rapid progress in technologies such as sensor technology and robotics are likely to change the way individuals, communities, governments and private actors perceive and respond to climate and ecological change. Methods based on various forms of artificial intelligence are already today being applied in a number of research fields related to climate change and environmental monitoring. Investments into applications of these technologies in agriculture, forestry and the extraction of marine resources also seem to be increasing rapidly. Despite a growing interest in, and deployment of AI-technologies in domains critical for sustainability, few have explored possible systemic risks in depth. This article offers a global overview of the progress of such technologies in sectors with high impact potential for sustainability like farming, forestry and the extraction of marine resources. We also identify possible systemic risks in these domains including a) algorithmic bias and allocative harms; b) unequal access and benefits; c) cascading failures and external disruptions, and d) trade-offs between efficiency and resilience. We explore these emerging risks, identify critical questions, and discuss the limitations of current governance mechanisms in addressing AI sustainability risks in these sectors.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.1016/j.techsoc.2021.101741",NA,NA,NA
"rayyan-100677053","Physics-informed machine learning: case studies for weather and climate modelling",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Machine learning (ML) provides novel and powerful ways of accurately and efficiently recognizing complex patterns, emulating nonlinear dynamics, and predicting the spatio-temporal evolution of weather and climate processes. Off-the-shelf ML models, however, do not necessarily obey the fundamental governing laws of physical systems, nor do they generalize well to scenarios on which they have not been trained. We survey systematic approaches to incorporating physics and domain knowledge into ML models and distill these approaches into broad categories. Through 10 case studies, we show how these approaches have been used successfully for emulating, downscaling, and forecasting weather and climate processes. The accomplishments of these studies include greater physical consistency, reduced training time, improved data efficiency, and better generalization. Finally, we synthesize the lessons learned and identify scientific, diagnostic, computational, and resource challenges for developing truly robust and reliable physics-informed ML models for weather and climate processes. This article is part of the theme issue ‘Machine learning for weather and climate modelling’.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1098/rsta.2020.0093",NA,NA,NA
"rayyan-100677056","Machine learning dismantling and early-warning signals of disintegration in complex systems",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract From physics to engineering, biology and social science, natural and artificial systems are characterized by interconnected topologies whose features – e.g., heterogeneous connectivity, mesoscale organization, hierarchy – affect their robustness to external perturbations, such as targeted attacks to their units. Identifying the minimal set of units to attack to disintegrate a complex network, i.e. network dismantling, is a computationally challenging (NP-hard) problem which is usually attacked with heuristics. Here, we show that a machine trained to dismantle relatively small systems is able to identify higher-order topological patterns, allowing to disintegrate large-scale social, infrastructural and technological networks more efficiently than human-based heuristics. Remarkably, the machine assesses the probability that next attacks will disintegrate the system, providing a quantitative method to quantify systemic risk and detect early-warning signals of system’s collapse. This demonstrates that machine-assisted analysis can be effectively used for policy and decision-making to better quantify the fragility of complex systems and their response to shocks.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.1038/s41467-021-25485-8",NA,NA,NA
"rayyan-100677063","Stable machine-learning parameterization of subgrid processes for climate modeling at a range of resolutions",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Global climate models represent small-scale processes such as clouds and convection using quasi-empirical models known as parameterizations, and these parameterizations are a leading cause of uncertainty in climate projections. A promising alternative approach is to use machine learning to build new parameterizations directly from high-resolution model output. However, parameterizations learned from three-dimensional model output have not yet been successfully used for simulations of climate. Here we use a random forest to learn a parameterization of subgrid processes from output of a three-dimensional high-resolution atmospheric model. Integrating this parameterization into the atmospheric model leads to stable simulations at coarse resolution that replicate the climate of the high-resolution simulation. The parameterization obeys physical constraints and captures important statistics such as precipitation extremes. The ability to learn from a fully three-dimensional simulation presents an opportunity for learning parameterizations from the wide range of global high-resolution simulations that are now emerging.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1038/s41467-020-17142-3",NA,NA,NA
"rayyan-100677069","Environmental sustainability technologies in biodiversity, energy, transportation and water management using artificial intelligence: A systematic review",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Artificial Intelligence (AI) has become an important area to tackle most environmental sustainability issues such as biodiversity, energy, transportation and water management. Biodiversity research has developed machine learning or natural language processing solutions to predict ecosystem services. Artificial intelligence applications and machine learning models have been increasingly used for predicting and optimizing water resource conservation. Area neural network, expert systems, pattern recognition, and fuzzy logic models are the main focus areas in energy. Applications of computer vision and decision support were found in transportation. Timely monitoring of interventions is required to improve environmental sustainability.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.1016/j.sftr.2022.100068",NA,NA,NA
"rayyan-100677078","Can we detect trends in natural disaster management with artificial intelligence? A review of modeling practices",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"There has been an unsettling rise in the intensity and frequency of natural disasters due to climate change and anthropogenic activities. Artificial intelligence (AI) models have shown remarkable success and superiority to handle huge and nonlinear data owing to their higher accuracy and efficiency, making them perfect tools for disaster monitoring and management. Accordingly, natural disaster management (NDM) with the usage of AI models has received increasing attention in recent years, but there has been no systematic review so far. This paper presents a systematic review on how AI models are applied in different NDM stages based on 278 studies retrieved from Elsevier Science, Springer LINK and Web of Science. The review: (1) enables increased visibility into various disaster types in different NDM stages from the methodological and content perspective, (2) obtains many general results including the practicality and gaps of extant studies and (3) provides several recommendations to develop innovative AI models and improve the quality of modeling. Overall, a comprehensive assessment and evaluation for the reviewed studies are performed, which tracked all stages of NDM research with the applications of AI models.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1007/s11069-020-04429-3",NA,NA,NA
"rayyan-100677079","Artificial intelligence and systemic risk",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Artificial intelligence (AI) is rapidly changing how the financial system is operated, taking over core functions for both cost savings and operational efficiency reasons. AI will assist both risk managers and the financial authorities. However, it can destabilize the financial system, creating new tail risks and amplifying existing ones due to procyclicality, unknown-unknowns, the need for trust, and optimization against the system.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.1016/j.jbankfin.2021.106290",NA,NA,NA
"rayyan-100677082","A Deep Learning Model for Earthquake Parameters Observation in IoT System-Based Earthquake Early Warning",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate | USER-NOTES: {""Maria""=>[""Earthquake early-warning system (EEWS) is inevitable for saving human lives. The fast determination of the Earthquake’s (EQ’s) magnitude and its location is significant in disaster management and EQ risk mitigation. These parameters can be conveyed over the Internet-of-Things (IoT) network to alleviate an EQ disaster. In this article, a deep learning model based on integrating autoencoder (AE) and convolutional neural network (CNN) for a swift pinpointing of EQ magnitude and location after 3 s from the onset of the P-wave is proposed. Thus, we name it 3 s AE and CNN (3S-AE-CNN). The employed data set is observed by three stations from the Japanese Hi-net seismic network. "", ""We have trained our model on 12200 events (109.80 thousand 3-s-three-component seismic windows). The model facilitates the extraction of waveforms’ significant features leading to robust estimation of the EQ parameters. The proposed model predicts the magnitude and location of EQ with errors in magnitude, latitude, and longitude that reach 0.000028, 0.0000033, and 0.0001, respectively. The EQ’s parameters calculated by the proposed 3S-AE-CNN model are swiftly sent to a centralized IoT system that in turn directs the involved entity to take suitable action. The obtained results of the 3S-AE-CNN are compared to the conventional manual solution method, which represents the optimum solution mean. The 3S-AE-CNN shows an enhanced performance for the magnitude and location determination as compared with the benchmark method, which proves its effectiveness for EEWS.""]}","https://doi.org/10.1109/jiot.2021.3114420",NA,NA,NA
"rayyan-100677085","Deep Learning Approach for Earthquake Parameters Classification in Earthquake Early Warning System",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Magnitude determination of earthquakes is a mandatory step before an earthquake early warning (EEW) system sends an alarm. Beneficiary users of EEW systems dependon how far they are located from such strong events. Therefore,determining the locations of these shakes is an important is sue for the tranquility of citizens as well. In light of that, this article proposes a magnitude, location, depth, and origin timecategorization using earthquake Ml magnitudes between 2 and 9.The dataset used is the fore and aftershocks of the great Tohokuearthquake of March 11,2011, recorded by three stations fromthe Japanese Hi-net seismic network. The proposed algorithmdepends on a convolutional neural network (CNN) which hasthe ability to extract significant features from waveforms thatenabled the classifier to reach a robust performance in the required earthquake parameters. The classification accuracies ofthe suggested approach for magnitude, origin time, depth, andlocation are 93.67%,89.55%,92.54%,and 89.50%, respectively.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1109/lgrs.2020.2998580",NA,NA,NA
"rayyan-100677088","Natural Disasters Intensity Analysis and Classification Based on Multispectral Images Using Multi-Layered Deep Convolutional Neural Network",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Natural disasters not only disturb the human ecological system but also destroy the properties and critical infrastructures of human societies and even lead to permanent change in the ecosystem. Disaster can be caused by naturally occurring events such as earthquakes, cyclones, floods, and wildfires. Many deep learning techniques have been applied by various researchers to detect and classify natural disasters to overcome losses in ecosystems, but detection of natural disasters still faces issues due to the complex and imbalanced structures of images. To tackle this problem, we propose a multilayered deep convolutional neural network. The proposed model works in two blocks: Block-I convolutional neural network (B-I CNN), for detection and occurrence of disasters, and Block-II convolutional neural network (B-II CNN), for classification of natural disaster intensity types with different filters and parameters. The model is tested on 4428 natural images and performance is calculated and expressed as different statistical values: sensitivity (SE), 97.54%; specificity (SP), 98.22%; accuracy rate (AR), 99.92%; precision (PRE), 97.79%; and F1-score (F1), 97.97%. The overall accuracy for the whole model is 99.92%, which is competitive and comparable with state-of-the-art algorithms.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.3390/s21082648",NA,NA,NA
"rayyan-100677090","International migration management in the age of artificial intelligence",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract Artificial intelligence (AI) has the potential to revolutionise the way states and international organisations seek to manage international migration. AI is gradually going to be used to perform tasks, including identity checks, border security and control, and analysis of data about visa and asylum applicants. To an extent, this is already a reality in some countries such as Canada, which uses algorithmic decision-making in immigration and asylum determination, and Germany, which has piloted projects using technologies such as face and dialect recognition for decision-making in asylum determination processes. The article’s central hypothesis is that AI technology can affect international migration management in three different dimensions: (1) by deepening the existing asymmetries between states on the international plane; (2) by modernising states’ and international organisations’ traditional practices; and (3) by reinforcing the contemporary calls for more evidence-based migration management and border security. The article examines each of these three hypotheses and reflects on the main challenges of using AI solutions for international migration management. It draws on legal, political and technology-facing academic literature, examining the current trends in technological developments and investigating the consequences that these can have for international migration. Most particularly, the article contributes to the current debate about the future of international migration management, informing policymakers in this area of growing importance and fast development.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.1093/migration/mnaa003",NA,NA,NA
"rayyan-100677095","ClimateNet: an expert-labeled open dataset and deep learning architecture for enabling high-precision analyses of extreme weather",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract. Identifying, detecting, and localizing extreme weather events is a crucial first step in understanding how they may vary under different climate change scenarios. Pattern recognition tasks such as classification, object detection, and segmentation (i.e., pixel-level classification) have remained challenging problems in the weather and climate sciences. While there exist many empirical heuristics for detecting extreme events, the disparities between the output of these different methods even for a single event are large and often difficult to reconcile. Given the success of deep learning (DL) in tackling similar problems in computer vision, we advocate a DL-based approach. DL, however, works best in the context of supervised learning – when labeled datasets are readily available. Reliable labeled training data for extreme weather and climate events is scarce. We create “ClimateNet” – an open, community-sourced human-expert-labeled curated dataset that captures tropical cyclones (TCs) and atmospheric rivers (ARs) in high-resolution climate model output from a simulation of a recent historical period. We use the curated ClimateNet dataset to train a state-of-the-art DL model for pixel-level identification – i.e., segmentation – of TCs and ARs. We then apply the trained DL model to historical and climate change scenarios simulated by the Community Atmospheric Model (CAM5.1) and show that the DL model accurately segments the data into TCs, ARs, or “the background” at a pixel level. Further, we show how the segmentation results can be used to conduct spatially and temporally precise analytics by quantifying distributions of extreme precipitation conditioned on event types (TC or AR) at regional scales. The key contribution of this work is that it paves the way for DL-based automated, high-fidelity, and highly precise analytics of climate data using a curated expert-labeled dataset – ClimateNet. ClimateNet and the DL-based segmentation method provide several unique capabilities: (i) they can be used to calculate a variety of TC and AR statistics at a fine-grained level; (ii) they can be applied to different climate scenarios and different datasets without tuning as they do not rely on threshold conditions; and (iii) the proposed DL method is suitable for rapidly analyzing large amounts of climate model output. While our study has been conducted for two important extreme weather patterns (TCs and ARs) in simulation datasets, we believe that this methodology can be applied to a much broader class of patterns and applied to observational and reanalysis data products via transfer learning.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.5194/gmd-14-107-2021",NA,NA,NA
"rayyan-100677110","The Long-Term Prediction of Landslide Processes within the Precarpathian Depression of the Cernivtsi Region of Ukraine",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"The purpose of this publication was the long-term forecasting of the landslide processes activation for the territory of the Precarpathian depression within the Chernivtsi region, taking into account the complex effect of natural factors.On the basis of statistical analysis and processing of long-term observations of landslide activation and natural time factors in particular solar activity, seismicity, groundwater levels, precipitation and air temperature, the relationship was analysed, the main periods of landslide activation were determined, the contribution of each time factor to the complex probability indicator of landslide development was estimated and long-term forecasting was carried out.An analysis of the influence of geomorphology on the landslide development was performed by using GIS MapІnfo.By means of cross-correlation, Fourier spectral analysis, the periodicities were analysed and the relationships between the parameters were established.It was found that the energy of earthquakes precedes the activation of landslides by 1 year, which indicates the ""preparatory"" effect of earthquakes as a factor that reduces the stability of rocks.The main periodicities of the forecast parameters of 9-11, 19-21, 28-31 years were highlighted, which are consistent with the rhythms of solar activity.The forecasting was carried out using artificial neural networks and the prediction function of the Mathematical package Mathcad, based on the received data, the activation of landslides is expected in 2023-2026, 2030-2035, 2040-2044 with some short periods of calm.The main periods of the dynamics of the time series of landslides and natural factors for the territory of the Precarpathian depression within the Chernivtsi region were determined, and a long-term forecast of landslides was made.Taking into account the large areas of the spread of landslide processes, forecasting the likely activation is an important issue for this region, the constructed predictive time models make it possible to assess the danger of the geological environment for the purpose of early warning and making management decisions aimed at reducing the consequences of a natural disaster.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.12911/22998993/164753",NA,NA,NA
"rayyan-100677114","Forecasting disruptions in global food value chains to tackle food insecurity: The role of AI and big data analytics – A bibliometric and scientometric analysis",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Globalization and interconnected supply chains have led to complex disruptions in global value chains, caused by various factors such as natural disasters, climate events, geopolitical conflicts, and economic crises. Recent breakthroughs in AI, machine learning, blockchain, and big data analytics offer new possibilities for forecasting and managing these disruptions effectively. This study examines the role of AI in forecasting and managing disruptions within global value chain to tackle food insecurity. We conducted a bibliometric and scientometric analysis using comprehensive data from Scopus and Web of Science to explore emerging research trends, influential publications, leading institutions, collaborations, themes, policy implications, and future research avenues. The research revealed an average yearly growth rate of 13.78 % in publications from 1973 to 2022. China, the United Kingdom, and the United States lead in AI applications to address supply chain disruptions, particularly concerning food insecurity. Frequently used keywords include ""food security,"" ""supply chain management,"" ""agriculture,"" ""modelling,"" ""climate change,"" and ""COVID-19."" Themes identified focus on the impact of COVID-19 on food supply chains, achieving food security amidst climate change, leveraging predictive models in agriculture, and assessing the impact of disruptions on food price volatility and global supply chain risk assessment approaches. The insights gained from this research offer valuable guidance for policymakers and researchers to enhance food security. The identified themes provide direction for future research efforts in advancing food security amidst uncertainties and disruptions in global value chains.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.1016/j.jafr.2023.100819",NA,NA,NA
"rayyan-100677126","A data‐driven intelligent model for landslide displacement prediction",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate | USER-NOTES: {""Maria""=>[""Landslides with step-like deformation features are widely distributed in the Three Gorges Reservoir area (TGR) of China, posing a severe hazard to the inhabitants of this region. This paper proposes a multi-input and multi-output intelligent integrated displacement prediction model for landslides with step-like displacement patterns. In this new model, three interconnected and information-transmitted functional sub-models are integrated. Unsupervised learning is used to identify different landslide deformation states automatically, and the imbalance classification and explainable artificial intelligence techniques are introduced for qualitative prediction and information filtering."", ""Probability theory and deep machine learning are adopted to provide deterministically predicted values and quantify their uncertainty. The case study of the Baijiabao landslide in the TGR region proves that the proposed model performs satisfactorily in both point and interval predictions. The intelligent integrated model can also provide the forecast of landslide deformation states, visual input information filtering and back analysis of influencing factors, which are valuable to landslide early warning and risk management.""]}","https://doi.org/10.1002/gj.4675",NA,NA,NA
"rayyan-100677130","Probabilistic forecasts of extreme heatwaves using convolutional neural networks in a regime of lack of data",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Forecasting extreme climate events, for instance extreme heat waves, is key for society and a scientific challenge. In this paper we propose a novel machine learning approach that successfully forecasts extreme heat waves up to 45 days before the end of the event. The approach allows for dynamical process studies. A key message is that optimal machine learning forecasts require a large amount of data. The image shows temperature (colors) and geopotential height (lines) anomalies for a typical atmospheric situation.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1103/physrevfluids.8.040501",NA,NA,NA
"rayyan-100677133","Frontiers of thermobarometry: GAIA, a novel Deep Learning-based tool for volcano plumbing systems",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate | USER-NOTES: {""Maria""=>[""The anatomy of the plumbing system of active volcanoes is fundamental to understand how magma is stored and channeled to the surface. Reliable geothermobarometric estimates are, therefore, critical to assess the depths and temperatures of the complex system of magmatic reservoirs that form a volcano apparatus. Here, we developed a novel Machine Learning approach (named GAIA, Geo Artificial Intelligence thermobArometry) based upon Feedforward Neural Networks to estimate P-T conditions of magma (clinopyroxene) storage and migration within the crust. "", ""Our Feedforward Neural Network method applied to clinopyroxene compositions yields better uncertainties (Root-Mean-Square Error and R2 score) than previous Machine Learning methods and set the basis for a novel generation of reliable geothermobarometers, which extends beyond the paradigm associated to crystal-liquid equilibrium. Also, the bootstrap procedure, inherent to the Feedforward Neural Network architecture, permits to perform a rigorous assessment of the P-T uncertainty associated to each clinopyroxene composition, as opposed to the Root-Mean-Square Error representing the P-T uncertainty of whole set of clinopyroxene compositions."", ""As a test, we applied GAIA to assess P-T conditions of five Italian volcanoes (Somma-Vesuvius, Campi Flegrei, Etna, Stromboli, Volcano), which are among the most dangerous volcanic centres in Europe. The results on the depths of the plumbing systems are in excellent agreement with those obtained with independent geophysical and geodetic surveys, and provide further evidence to current models of volcano plumbing systems consisting of physically-separated reservoirs interconnected by a network of conduits channelling magma en route to the surface. The results on the magma (clinopyroxene crystallization) temperatures are also in agreement with other estimates, albeit obtained considering - mainly but not only - thermodynamically-based clinopyroxene-liquid geothermometers."", ""GAIA can set robust estimates of magma storage, segregation, and ascent conditions within the plumbing system of active volcanoes, helping to unravel P-T variations, if any, during their eruptive history and providing robust clues to volcanic hazard assessment.""]}","https://doi.org/10.1016/j.epsl.2023.118352",NA,NA,NA
"rayyan-100677140","Omni-Dimensional Dynamic Convolution Meets Bottleneck Transformer: A Novel Improved High Accuracy Forest Fire Smoke Detection Model",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"The frequent occurrence of forest fires in recent years has not only seriously damaged the forests’ ecological environments but also threatened the safety of public life and property. Smoke, as the main manifestation of the flame before it is produced, has the advantage of a wide diffusion range that is not easily obscured. Therefore, timely detection of forest fire smoke with better real-time detection for early warnings of forest fires wins valuable time for timely firefighting and also has great significance and applications for the development of forest fire detection systems. However, existing forest fire smoke detection methods still have problems, such as low detection accuracy, slow detection speed, and difficulty detecting smoke from small targets. In order to solve the aforementioned problems and further achieve higher accuracy in detection, this paper proposes an improved, new, high-accuracy forest fire detection model, the OBDS. Firstly, to address the problem of insufficient extraction of effective features of forest fire smoke in complex forest environments, this paper introduces the SimAM attention mechanism, which makes the model pay more attention to the feature information of forest fire smoke and suppresses the interference of non-targeted background information. Moreover, this paper introduces Omni-Dimensional Dynamic Convolution instead of static convolution and adaptively and dynamically adjusts the weights of the convolution kernel, which enables the network to better extract the key features of forest fire smoke of different shapes and sizes. In addition, to address the problem that traditional convolutional neural networks are not capable of capturing global forest fire smoke feature information, this paper introduces the Bottleneck Transformer Net (BoTNet) to fully extract global feature information and local feature information of forest fire smoke images while improving the accuracy of small target forest fire target detection of smoke, effectively reducing the model’s computation, and improving the detection speed of model forest fire smoke. Finally, this paper introduces the decoupling head to further improve the detection accuracy of forest fire smoke and speed up the convergence of the model. Our experimental results show that the model OBDS for forest fire smoke detection proposed in this paper is significantly better than the mainstream model, with a computational complexity of 21.5 GFLOPs (giga floating-point operations per second), an improvement of 4.31% compared with the YOLOv5 (YOLO, you only look once) model mAP@0.5, reaching 92.10%, and an FPS (frames per second) of 54, which is conducive to the realization of early warning of forest fires.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.3390/f14040838",NA,NA,NA
"rayyan-100677145","Universal early warning signals of phase transitions in climate systems",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"The potential for complex systems to exhibit tipping points in which an equilibrium state undergoes a sudden and often irreversible shift is well established, but prediction of these events using standard forecast modelling techniques is quite difficult. This has led to the development of an alternative suite of methods that seek to identify signatures of critical phenomena in data, which are expected to occur in advance of many classes of dynamical bifurcation. Crucially, the manifestations of these critical phenomena are generic across a variety of systems, meaning that data-intensive deep learning methods can be trained on (abundant) synthetic data and plausibly prove effective when transferred to (more limited) empirical datasets. This paper provides a proof of concept for this approach as applied to lattice phase transitions: a deep neural network trained exclusively on two-dimensional Ising model phase transitions is tested on a number of real and simulated climate systems with considerable success. Its accuracy frequently surpasses that of conventional statistical indicators, with performance shown to be consistently improved by the inclusion of spatial indicators. Tools such as this may offer valuable insight into climate tipping events, as remote sensing measurements provide increasingly abundant data on complex geospatially resolved Earth systems.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1098/rsif.2022.0562",NA,NA,NA
"rayyan-100677150","A Distributed Multi-Sensor Machine Learning Approach to Earthquake Early Warning",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Our research aims to improve the accuracy of Earthquake Early Warning (EEW) systems by means of machine learning. EEW systems are designed to detect and characterize medium and large earthquakes before their damaging effects reach a certain location. Traditional EEW methods based on seismometers fail to accurately identify large earthquakes due to their sensitivity to the ground motion velocity. The recently introduced high-precision GPS stations, on the other hand, are ineffective to identify medium earthquakes due to its propensity to produce noisy data. In addition, GPS stations and seismometers may be deployed in large numbers across different locations and may produce a significant volume of data consequently, affecting the response time and the robustness of EEW systems.In practice, EEW can be seen as a typical classification problem in the machine learning field: multi-sensor data are given in input, and earthquake severity is the classification result. In this paper, we introduce the Distributed Multi-Sensor Earthquake Early Warning (DMSEEW) system, a novel machine learning-based approach that combines data from both types of sensors (GPS stations and seismometers) to detect medium and large earthquakes. DMSEEW is based on a new stacking ensemble method which has been evaluated on a real-world dataset validated with geoscientists. The system builds on a geographically distributed infrastructure, ensuring an efficient computation in terms of response time and robustness to partial infrastructure failures. Our experiments show that DMSEEW is more accurate than the traditional seismometer-only approach and the combined-sensors (GPS and seismometers) approach that adopts the rule of relative strength.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1609/aaai.v34i01.5376",NA,NA,NA
"rayyan-100677162","Artificial intelligence and cloud-based Collaborative Platforms for Managing Disaster, extreme weather and emergency operations",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Natural disasters are often unpredictable and therefore there is a need for quick and effective response to save lives and infrastructure. Hence, this study is aimed at achieving timely, anticipated and effective response throughout the cycle of a disaster, extreme weather and emergency operations management with the help of advanced technologies. This study proposes a novel, evidence-based framework (4-AIDE) that highlights the role of artificial intelligence (AI) and cloud-based collaborative platforms in disaster, extreme weather and emergency situations. A qualitative approach underpinned by organizational information processing theory (OIPT) is employed to design, develop and conduct semi-structured interviews with 33 respondents having experience in AI and cloud computing industries during emergency and extreme weather situations. For analysing the collected data, axial, open and selective coding is used that further develop themes, propositions and an evidence-based framework. The study findings indicate that AI and cloud-based collaborative platforms offer a structured and logical approach to enable two-way, algorithm-based communication to collect, analyse and design effective management strategies for disaster and extreme weather situations. Managers of public systems or businesses can collect and analyse data to predict possible outcomes and take necessary actions in an extreme weather situation. Communities and societies can be more resilient by transmitting and receiving data to AI and cloud-based collaborative platforms. These actions can also help policymakers identify critical pockets and guide administration for their necessary preparation for unexpected, extreme weather, and emergency events.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1016/j.ijpe.2022.108642",NA,NA,NA
"rayyan-100677163","Enhancing the reliability of landslide early warning systems by machine learning",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate | USER-NOTES: {""Maria""=>[""This paper submits a report on the effective adoption of machine learning algorithms for enhancing the reliability of rainfall-induced landslides. The challenges involved in the design of reliable landslide early warning systems (LEWS) and the data-driven context for overcoming these challenges have been presented. The operation of LEWS is explained using the chain of five major components (i) Data collection, (ii) Data transmission, (iii) Modelling, analysis and forecasting, (iv) Warning, and (v) Response. Failure of any of these major components of the LEWS will break the chain of operation of LEWS and the ensued consequences of each component failure are reviewed."", ""Inferences drawn from the analysis of the reliability measures incorporated in 12 LEWS deployments across a dozen locations around the world are also presented. Based on the investigations from 12 LEWS and the real-world experience, we identified that an alternate solution is required for ensuring the reliability of LEWS, especially during disaster scenarios when warnings are crucial, but data availability is a constraint. We recognized that machine learning algorithms can provide an alternate solution and in this paper, we have discussed two machine learning approaches nowcasting and forecasting for enhancing the reliability. Both the algorithms employ historic data of the landslide monitoring parameters to learn the changes materializing in slope leading to landslide incidences. "", ""The learned knowledge is used to nowcast and forecast the real-time and future conditions of the slope from the real-time landslide monitoring parameters. In terms of ensuring reliability, (i) Nowcasting algorithm provides an alternate solution if either the Data collection component or Data transmission component of a LEWS fails. (ii) Forecasting algorithm provides extra lead-time for early warning and solves the problem of less lead-time during early warning process. The breakthrough is even when the real-time landslide monitoring parameters are not available for various reasons, these algorithms take the minimal input of rainfall forecast information for nowcasting and forecasting thus restoring the broken chain of operation of LEWS.""]}","https://doi.org/10.1007/s10346-020-01453-z",NA,NA,NA
"rayyan-100677166","Training machine learning models on climate model output yields skillful interpretable seasonal precipitation forecasts",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract A barrier to utilizing machine learning in seasonal forecasting applications is the limited sample size of observational data for model training. To circumvent this issue, here we explore the feasibility of training various machine learning approaches on a large climate model ensemble, providing a long training set with physically consistent model realizations. After training on thousands of seasons of climate model simulations, the machine learning models are tested for producing seasonal forecasts across the historical observational period (1980-2020). For forecasting large-scale spatial patterns of precipitation across the western United States, here we show that these machine learning-based models are capable of competing with or outperforming existing dynamical models from the North American Multi Model Ensemble. We further show that this approach need not be considered a ‘black box’ by utilizing machine learning interpretability methods to identify the relevant physical processes that lead to prediction skill.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1038/s43247-021-00225-4",NA,NA,NA
"rayyan-100677173","Facilitating adoption of AI in natural disaster management through collaboration",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Artificial intelligence can enhance our ability to manage natural disasters. However, understanding and addressing its limitations is required to realize its benefits. Here, we argue that interdisciplinary, multistakeholder, and international collaboration is needed for developing standards that facilitate its implementation.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1038/s41467-022-29285-6",NA,NA,NA
"rayyan-100677174","Early warning and control of food safety risk using an improved AHC-RBF neural network integrating AHP-EW",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene | USER-NOTES: {""Maria""=>[""Food safety is an important issue affecting social development. Early warning analysis and risk control of food safety is of great significance in managing food safety risks, thereby ensuring food safety. In this paper, we propose an improved early warning approach for assessing and controlling food safety risk based on the agglomerative hierarchical clustering-radial basis function (AHC-RBF) neural network integrating an analytic hierarchy process approach and the entropy weight (AHP-EW). Different risk values of the detection data are fused by the AHP-EW to obtain the risk fusion value which is the output of the AHC-RBF. The detection data are set as the input of the AHC-RBF to build the early warning model. Moreover, prediction and control of food safety risk are analyzed. Finally, a case study of meat products detection data in China is carried out based on the proposed model. "", ""We compared our model with the back propagation (BP) and the RBF neural network, and the results verify the effectiveness of our proposed early warning model. The proposed early warning analysis is helpful for food safety supervision departments to control food safety risk.""]}","https://doi.org/10.1016/j.jfoodeng.2020.110239",NA,NA,NA
"rayyan-100677177","Process‐Based Climate Model Development Harnessing Machine Learning: I. A Calibration Tool for Parameterization Improvement",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract The development of parameterizations is a major task in the development of weather and climate models. Model improvement has been slow in the past decades, due to the difficulty of encompassing key physical processes into parameterizations, but also of calibrating or “tuning” the many free parameters involved in their formulation. Machine learning techniques have been recently used for speeding up the development process. While some studies propose to replace parameterizations by data‐driven neural networks, we rather advocate that keeping physical parameterizations is key for the reliability of climate projections. In this paper we propose to harness machine learning to improve physical parameterizations. In particular, we use Gaussian process‐based methods from uncertainty quantification to calibrate the model free parameters at a process level. To achieve this, we focus on the comparison of single‐column simulations and reference large‐eddy simulations over multiple boundary‐layer cases. Our method returns all values of the free parameters consistent with the references and any structural uncertainties, allowing a reduced domain of acceptable values to be considered when tuning the three‐dimensional (3D) global model. This tool allows to disentangle deficiencies due to poor parameter calibration from intrinsic limits rooted in the parameterization formulations. This paper describes the tool and the philosophy of tuning in single‐column mode. Part 2 shows how the results from our process‐based tuning can help in the 3D global model tuning.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1029/2020ms002217",NA,NA,NA
"rayyan-100677193","Predicting landslide susceptibility based on decision tree machine learning models under climate and land use changes",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate | USER-NOTES: {""Maria""=>[""Landslides are most catastrophic and frequently occurred across the world. In mountainous areas of the globe, recurrent occurrences of landslide have caused huge amount of economic losses and a large number of casualties. In this research, we attempted to estimate the potential impact of climate and LULC on future landslide susceptibility in of Markazi Province of Iran. We considered the boosted tree (BT), random forest (RF) and extremely randomized tree (ERT) models for landslide susceptibility assessment in Markazi Province. The results of evaluation criteria showed that ERT model is most optimal than other models used in this study with AUC values of 0.99 and 0.93 for the training and validation datasets, respectively. "", ""According to the ERT model, the spatial coverage of the very high and high land slide susceptible zones for the current period, 2050s considering RCP 2.6 and 2050s considering RCP 8.5 are 428.5 km2, 439.6 km2 and 465.2 km2, respectively. From this analysis it is clear that the impact of climate and LULC changes on future landslide susceptibility is prominent. The results of the present study help managers to reduce landslide damages, not only for current but also for future conditions, based on climate and LULC changes.""]}","https://doi.org/10.1080/10106049.2021.1986579",NA,NA,NA
"rayyan-100677207","A graph deep learning method for landslide displacement prediction based on global navigation satellite system positioning",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"The accurate prediction of displacement is crucial for landslide deformation monitoring and early warning. This study focuses on a landslide in Wenzhou Belt Highway and proposes a novel multivariate landslide displacement prediction method that relies on graph deep learning and Global Navigation Satellite System (GNSS) positioning. First model the graph structure of the monitoring system based on the engineering positions of the GNSS monitoring points and build the adjacent matrix of graph nodes. Then construct the historical and predicted time series feature matrixes using the processed temporal data including GNSS displacement, rainfall, groundwater table and soil moisture content and the graph structure. Last introduce the state-of-the-art graph deep learning GTS (Graph for Time Series) model to improve the accuracy and reliability of landslide displacement prediction which utilizes the temporal-spatial dependency of the monitoring system. This approach outperforms previous studies that only learned temporal features from a single monitoring point and maximally weighs the prediction performance and the priori graph of the monitoring system. The proposed method performs better than SVM, XGBoost, LSTM and DCRNN models in terms of RMSE (1.35 mm), MAE (1.14 mm) and MAPE (0.25) evaluation metrics, which is provided to be effective in future landslide failure early warning.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate | USER-NOTES: {""Maria""=>[""Incorrectly labeled as \""No abstract?\""""]}","https://doi.org/10.1016/j.gsf.2023.101690",NA,NA,NA
"rayyan-100677208","Construction of an Integrated Drought Monitoring Model Based on Deep Learning Algorithms",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Drought is one of the major global natural disasters, and appropriate monitoring systems are essential to reveal drought trends. In this regard, deep learning is a very promising approach for characterizing the non-linear nature of drought factors. We used multi-source remote sensing data such as the Moderate Resolution Imaging Spectroradiometer (MODIS) and Climate Hazards Group Infrared Precipitation with Station (CHIRPS) data to integrate drought impact factors such as precipitation, vegetation, temperature, and soil moisture. The application of convolutional long short-term memory (ConvLSTM) to construct an integrated drought monitoring model was proposed and tested, using the Xinjiang Uygur Autonomous Region as an example. To better compare the monitoring performance of ConvLSTM models, three other classical deep learning models and three classical machine learning models were also used for comparison. The results show that the composite drought index (CDI) output by the ConvLSTM model had a consistent high correlation with the drought rating of the multi-scale standardized precipitation evapotranspiration index (SPEI). The correlation coefficients between the CDI and the multi-scale standardized precipitation index (SPI) were all above 0.5 (p &lt; 0.01), which was highly significant, and the correlation coefficient between CDI-1 and the monthly soil relative humidity at a 10 cm depth was above 0.45 (p &lt; 0.01), which was well correlated. In addition, the spatial distribution of the CDI-6 simulated by the model was highly correlated with the degree of drought expressed by the SPEI-6 observations at the stations. This study provides a new approach for integrated regional drought monitoring.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.3390/rs15030667",NA,NA,NA
"rayyan-100677220","Subseasonal Prediction of Central European Summer Heatwaves with Linear and Random Forest Machine Learning Models",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract Heatwaves are extreme near-surface temperature events that can have substantial impacts on ecosystems and society. Early warning systems help to reduce these impacts by helping communities prepare for hazardous climate-related events. However, state-of-the-art prediction systems can often not make accurate forecasts of heatwaves more than two weeks in advance, which are required for advance warnings. We therefore investigate the potential of statistical and machine learning methods to understand and predict central European summer heatwaves on time scales of several weeks. As a first step, we identify the most important regional atmospheric and surface predictors based on previous studies and supported by a correlation analysis: 2-m air temperature, 500-hPa geopotential, precipitation, and soil moisture in central Europe, as well as Mediterranean and North Atlantic sea surface temperatures, and the North Atlantic jet stream. Based on these predictors, we apply machine learning methods to forecast two targets: summer temperature anomalies and the probability of heatwaves for 1–6 weeks lead time at weekly resolution. For each of these two target variables, we use both a linear and a random forest model. The performance of these statistical models decays with lead time, as expected, but outperforms persistence and climatology at all lead times. For lead times longer than two weeks, our machine learning models compete with the ensemble mean of the European Centre for Medium-Range Weather Forecast’s hindcast system. We thus show that machine learning can help improve subseasonal forecasts of summer temperature anomalies and heatwaves. Significance Statement Heatwaves (prolonged extremely warm temperatures) cause thousands of fatalities worldwide each year. These damaging events are becoming even more severe with climate change. This study aims to improve advance predictions of summer heatwaves in central Europe by using statistical and machine learning methods. Machine learning models are shown to compete with conventional physics-based models for forecasting heatwaves more than two weeks in advance. These early warnings can be used to activate effective and timely response plans targeting vulnerable communities and regions, thereby reducing the damage caused by heatwaves.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1175/aies-d-22-0038.1",NA,NA,NA
"rayyan-100677221","FFM: Flood Forecasting Model Using Federated Learning",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Floods are one of the most common natural disasters that occur frequently causing massive damage to property, agriculture, economy and life. Flood prediction offers a huge challenge for researchers struggling to predict floods since long time. In this article, flood forecasting model using federated learning technique has been proposed. Federated Learning is the most advanced technique of machine learning (ML) that guarantees data privacy, ensures data availability, promises data security, and handles network latency trials inherent in prediction of floods by prohibiting data to be transferred over the network for model training. Federated Learning technique urges for onsite training of local data models, and focuses on transmission of these local models on the network instead of sending huge data set towards central server for local model aggregation and training of global data model at the central server. In this article, the proposed model integrates locally trained models of eighteen clients, investigates at which station flooding is about to happen and generates flood alert towards a specific client with five days lead time. A local feed forward neural network (FFNN) model is trained at the client station where the flood has been expected. Flood forecasting module of local FFNN model predicts the expected water level by taking multiple regional parameters as input. The dataset of five different rivers and barrages has been collected from 2015 to 2021 considering four aspects including snow melting, rainfall-runoff, flow routing and hydrodynamics. The proposed flood forecasting model has successfully predicted previous floods happened in the selected zone during 2010 to 2015 with 84 % accuracy.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1109/access.2023.3252896",NA,NA,NA
"rayyan-100677227","The use of machine learning techniques for a predictive model of debris flows triggered by short intense rainfall",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"The Alpine region of Aosta Valley has an early warning system to issue hydrogeological alerts up to 36 h in advance based on the output of hydrological models and rainfall thresholds. However, those thresholds generally do not apply to the debris flows triggered by local summer thunderstorms, which typically are intense rainfalls of short duration, with cumulative precipitation lower than 20 mm. Therefore, it is necessary to formulate a specific predictive debris-flow model, which takes into account other possible triggering factors. In this study, we have developed a predictive model for debris flows with machine learning techniques, using a detailed dataset composed by a variety of geomorphological and hydro-meteorological variables. The variables of the dataset were collected from daily measured and modelled data for all of the 91 drainage basins in which at least one debris-flow event was generated during the time period considered in this study (2009–2019). The performance of the model, using different machine learning techniques, was evaluated, and the most suitable model was chosen to be experimentally implemented in the existing early warning system of the region. The output of the model provides a debris-flow probability (DFP) for individual basins computed from the geomorphological and hydro-meteorological input variables.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1007/s11069-023-05853-x",NA,NA,NA
"rayyan-100677241","Seismic Intensity Estimation for Earthquake Early Warning Using Optimized Machine Learning Model",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate | USER-NOTES: {""Maria""=>[""The need for an earthquake early-warning system (EEWS) is unavoidable to save lives. In terms of managing earthquake disasters and achieving effective risk mitigation, the quick identification of the earthquake’s intensity is a valuable factor. In light of this, the on-site intensity measurement can be transmitted over an Internet of Things (IoT) network. In this regard, a machine learning (ML) strategy based on numerous linear and nonlinear models is proposed in this study for a quick determination of earthquake intensity after 2 s from the P-wave onset. We call this model an on-site 2-s ML model-based earthquake intensity determination (2S-ML-EIOS). The used dataset INSTANCE for this model is observed by the number of 386 stations from the Italian national seismic network."", ""Our model has been trained on 50000 occurrences (150 000 of 2-s three-component (3C) seismic windows). The model has the ability to deal with limited features of the waveform traces leading to reliable estimation of the earthquake intensity. The suggested model has a 98.59% accuracy rate in predicting earthquake intensity. The suggested 2S-ML-EIOS model can be used with a centralized IoT system to promptly send the alarm, and the IoT system will then instruct the affected administration to take the appropriate action. The 2S-ML-EIOS results are contrasted with those from the traditional manual solution approach, which corresponds to the ideal solution mean. "", ""Based on the extreme gradient boosting (XGB) model, the 2S-ML-EIOS can achieve the best intensity determination, and this improved performance demonstrates the methodology’s efficacy for EEWS.""]}","https://doi.org/10.1109/tgrs.2023.3296520",NA,NA,NA
"rayyan-100677246","Data-driven landslide forecasting: Methods, data completeness, and real-time warning",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate | USER-NOTES: {""Maria""=>[""Various data-driven methods, including empirical, statistical, and machine learning methods, have been developed to promptly forecast rain-induced landslides. Their abilities differ considerably in spatio-temporal landslide prediction and in handling datasets of varying qualities. A challenging issue that significantly hinders the applications of data-driven methods is the data incompleteness in most landslide inventories, particularly the lack of accurate landslide time that is a vital link between each landslide and its triggering rainstorm. This study systematically compares the performances of three categories of data-driven methods for landslide prediction and proposes a novel machine learning model featured by probabilistic landslide modelling for spatio-temporal landslide prediction. "", ""The integrated machine learning model can be developed on a realistic landslide database, regardless of whether the landslide timing information is known or not. It not only promptly predicts the spatio-temporal evolution of landslides during a rainstorm but also reliably characterises the factual landslide risk, which provides a powerful real-time decision-making tool for landslide early warning and risk management. The model is validated against the landslide incidents in Hong Kong in the past 35 years both spatially and temporally, and outperforms other data-driven models in both prediction ability and accuracy.""]}","https://doi.org/10.1016/j.enggeo.2023.107068",NA,NA,NA
"rayyan-100677257","Process‐Based Climate Model Development Harnessing Machine Learning: II. Model Calibration From Single Column to Global",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract We demonstrate a new approach for climate model tuning in a realistic situation. Our approach, the mathematical foundations and technical details of which are given in Part I, systematically uses a single‐column configuration of a global atmospheric model on test cases for which reference large‐eddy‐simulations are available. The space of free parameters is sampled running the single‐column model from which metrics are estimated in the full parameter space using emulators. The parameter space is then reduced by retaining only the values for which the emulated metrics match large eddy simulations within a given tolerance to error. The approach is applied to the 6A version of the LMDZ model which results from a long investment in the development of physics parameterizations and by‐hand tuning. The boundary layer is revisited by increasing the vertical resolution and varying parameters that were kept fixed so far, which improves the representation of clouds at process scale. The approach allows us to automatically reach a tuning of this modified configuration as good as that of the 6A version. We show how this approach helps accelerate the introduction of new parameterizations. It allows us to maintain the physical foundations of the model and to ensure that the improvement of global metrics is obtained for a reasonable behavior at process level, reducing the risk of error compensations that may arise from over‐fitting some climate metrics. That is, we get things right for the right reasons.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1029/2020ms002225",NA,NA,NA
"rayyan-100677259","Assessing the Potential of Deep Learning for Emulating Cloud Superparameterization in Climate Models With Real‐Geography Boundary Conditions",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract We explore the potential of feed‐forward deep neural networks (DNNs) for emulating cloud superparameterization in realistic geography, using offline fits to data from the superparameterized community atmospheric model. To identify the network architecture of greatest skill, we formally optimize hyperparameters using ∼250 trials. Our DNN explains over 70% of the temporal variance at the 15‐min sampling scale throughout the mid‐to‐upper troposphere. Autocorrelation timescale analysis compared against DNN skill suggests the less good fit in the tropical, marine boundary layer is driven by neural network difficulty emulating fast, stochastic signals in convection. However, spectral analysis in the temporal domain indicates skillful emulation of signals on diurnal to synoptic scales. A closer look at the diurnal cycle reveals correct emulation of land‐sea contrasts and vertical structure in the heating and moistening fields, but some distortion of precipitation. Sensitivity tests targeting precipitation skill reveal complementary effects of adding positive constraints versus hyperparameter tuning, motivating the use of both in the future. A first attempt to force an offline land model with DNN emulated atmospheric fields produces reassuring results further supporting neural network emulation viability in real‐geography settings. Overall, the fit skill is competitive with recent attempts by sophisticated Residual and Convolutional Neural Network architectures trained on added information, including memory of past states. Our results confirm the parameterizability of superparameterized convection with continents through machine learning and we highlight the advantages of casting this problem locally in space and time for accurate emulation and hopefully quick implementation of hybrid climate models.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1029/2020ms002385",NA,NA,NA
"rayyan-100677260","MALMI: An Automated Earthquake Detection and Location Workflow Based on Machine Learning and Waveform Migration",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract Robust automatic event detection and location is central to real-time earthquake monitoring. With the increase of computing power and data availability, automated workflows that utilize machine learning (ML) techniques have become increasingly popular; however, ML-based classical workflows still face challenges when applied to the analysis of microseismic data. These seismic sequences are often characterized by short interevent times and/or low signal-to-noise ratio (SNR). Full waveform methods that do not rely on phase picking and association are suitable for processing such datasets, but are computationally costly and lack clear event identification criteria, which is not ideal for real-time processing. To leverage the advantages of both the methods, we propose a new workflow—MAchine Learning aided earthquake MIgration location (MALMI), which integrates ML and waveform migration to perform automated event detection and location. The new workflow uses a pretrained ML model to generate continuous phase probabilities that are then backprojected and stacked to locate seismic sources using migration. We applied the workflow to one month of continuous data collected in the Hengill geothermal area of Iceland to monitor induced earthquakes around two geothermal production sites. With a ML model (EQ-Transformer) pretrained using a global distribution of earthquakes, the proposed workflow automatically detects and locates 250 additional seismic events (accounting for 36% events in the obtained catalog) compared to a reference catalog generated using the SeisComP software. Most of the new events are microseismic events with a magnitude less than 0. Visual inspection of the waveforms of the newly detected events indicates that they are real seismic events of low SNR and are only reliably recorded by very few stations in the array. Further comparison with the conventional migration method based on short-term average over long-term average confirms that MALMI can produce much clearer stacked images with higher resolution and reliability, especially for events with low SNR. The workflow is freely available on GitHub, providing an automated tool for simultaneous event detection and location from continuous seismic data.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1785/0220220071",NA,NA,NA
"rayyan-100677261","Regional climate model emulator based on deep learning: concept and first evaluation of a novel hybrid downscaling approach",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract Providing reliable information on climate change at local scale remains a challenge of first importance for impact studies and policymakers. Here, we propose a novel hybrid downscaling method combining the strengths of both empirical statistical downscaling methods and Regional Climate Models (RCMs). In the longer term, the final aim of this tool is to enlarge the high-resolution RCM simulation ensembles at low cost to explore better the various sources of projection uncertainty at local scale. Using a neural network, we build a statistical RCM-emulator by estimating the downscaling function included in the RCM. This framework allows us to learn the relationship between large-scale predictors and a local surface variable of interest over the RCM domain in present and future climate. The RCM-emulator developed in this study is trained to produce daily maps of the near-surface temperature at the RCM resolution (12 km). The emulator demonstrates an excellent ability to reproduce the complex spatial structure and daily variability simulated by the RCM, particularly how the RCM refines the low-resolution climate patterns. Training in future climate appears to be a key feature of our emulator. Moreover, there is a substantial computational benefit of running the emulator rather than the RCM, since training the emulator takes about 2 h on GPU, and the prediction takes less than a minute. However, further work is needed to improve the reproduction of some temperature extremes, the climate change intensity and extend the proposed methodology to different regions, GCMs, RCMs, and variables of interest.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1007/s00382-022-06343-9",NA,NA,NA
"rayyan-100677262","Forecasting asylum-related migration flows with machine learning and data at scale",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"The sudden and unexpected migration flows that reached Europe during the so-called 'refugee crisis' of 2015-2016 left governments unprepared, exposing significant shortcomings in the field of migration forecasting. Forecasting asylum-related migration is indeed problematic. Migration is a complex system, drivers are composite, measurement incorporates uncertainty, and most migration theories are either under-specified or hardly actionable. As a result, approaches to forecasting generally focus on specific migration flows, and the results are often inconsistent and difficult to generalise. Here we present an adaptive machine learning algorithm that integrates administrative statistics and non-traditional data sources at scale to effectively forecast asylum-related migration flows. We focus on asylum applications lodged in countries of the European Union (EU) by nationals of all countries of origin worldwide, but the same approach can be applied in any context provided adequate migration or asylum data are available. Uniquely, our approach (a) monitors drivers in countries of origin and destination to detect early onset change; (b) models individual country-to-country migration flows separately and on moving time windows; (c) estimates the effects of individual drivers, including lagged effects; (d) delivers forecasts of asylum applications up to four weeks ahead; (e) assesses how patterns of drivers shift over time to describe the functioning and change of migration systems. Our approach draws on migration theory and modelling, international protection, and data science to deliver what is, to our knowledge, the first comprehensive system for forecasting asylum applications based on adaptive models and data at scale. Importantly, this approach can be extended to forecast other social processes.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.1038/s41598-022-05241-8",NA,NA,NA
"rayyan-100677263","Modified SEIR and AI prediction of the epidemics trend of COVID-19 in China under public health interventions",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Background: The coronavirus disease 2019 (COVID-19) outbreak originating in Wuhan, Hubei province, China, coincided with chunyun, the period of mass migration for the annual Spring Festival. To contain its spread, China adopted unprecedented nationwide interventions on January 23 2020. These policies included large-scale quarantine, strict controls on travel and extensive monitoring of suspected cases. However, it is unknown whether these policies have had an impact on the epidemic. We sought to show how these control measures impacted the containment of the epidemic. Methods: We integrated population migration data before and after January 23 and most updated COVID-19 epidemiological data into the Susceptible-Exposed-Infectious-Removed (SEIR) model to derive the epidemic curve. We also used an artificial intelligence (AI) approach, trained on the 2003 SARS data, to predict the epidemic. Results: We found that the epidemic of China should peak by late February, showing gradual decline by end of April. A five-day delay in implementation would have increased epidemic size in mainland China three-fold. Lifting the Hubei quarantine would lead to a second epidemic peak in Hubei province in mid-March and extend the epidemic to late April, a result corroborated by the machine learning prediction. Conclusions: Our dynamic SEIR model was effective in predicting the COVID-19 epidemic peaks and sizes. The implementation of control measures on January 23 2020 was indispensable in reducing the eventual COVID-19 epidemic size.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.21037/jtd.2020.02.64",NA,NA,NA
"rayyan-100677267","Artificial intelligence for sustainability: Challenges, opportunities, and a research agenda",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Artificial intelligence (AI) will transform business practices and industries and has the potential to address major societal problems, including sustainability. Degradation of the natural environment and the climate crisis are exceedingly complex phenomena requiring the most advanced and innovative solutions. Aiming to spur groundbreaking research and practical solutions of AI for environmental sustainability, we argue that AI can support the derivation of culturally appropriate organizational processes and individual practices to reduce the natural resource and energy intensity of human activities. The true value of AI will not be in how it enables society to reduce its energy, water, and land use intensities, but rather, at a higher level, how it facilitates and fosters environmental governance. A comprehensive review of the literature indicates that research regarding AI for sustainability is challenged by (1) overreliance on historical data in machine learning models, (2) uncertain human behavioral responses to AI-based interventions, (3) increased cybersecurity risks, (4) adverse impacts of AI applications, and (5) difficulties in measuring effects of intervention strategies. The review indicates that future studies of AI for sustainability should incorporate (1) multilevel views, (2) systems dynamics approaches, (3) design thinking, (4) psychological and sociological considerations, and (5) economic value considerations to show how AI can deliver immediate solutions without introducing long-term threats to environmental sustainability.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.1016/j.ijinfomgt.2020.102104",NA,NA,NA
"rayyan-100677275","Deep learning neural networks for spatially explicit prediction of flash flood probability",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Flood probability maps are essential for a range of applications, including land use planning and developing mitigation strategies and early warning systems. This study describes the potential application of two architectures of deep learning neural networks, namely convolutional neural networks (CNN) and recurrent neural networks (RNN), for spatially explicit prediction and mapping of flash flood probability. To develop and validate the predictive models, a geospatial database that contained records for the historical flood events and geo-environmental characteristics of the Golestan Province in northern Iran was constructed. The step-wise weight assessment ratio analysis (SWARA) was employed to investigate the spatial interplay between floods and different influencing factors. The CNN and RNN models were trained using the SWARA weights and validated using the receiver operating characteristics technique. The results showed that the CNN model (AUC = 0.832, RMSE = 0.144) performed slightly better than the RNN model (AUC = 0.814, RMSE = 0.181) in predicting future floods. Further, these models demonstrated an improved prediction of floods compared to previous studies that used different models in the same study area. This study showed that the spatially explicit deep learning neural network models are successful in capturing the heterogeneity of spatial patterns of flood probability in the Golestan Province, and the resulting probability maps can be used for the development of mitigation plans in response to the future floods. The general policy implication of our study suggests that design, implementation, and verification of flood early warning systems should be directed to approximately 40% of the land area characterized by high and very susceptibility to flooding.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1016/j.gsf.2020.09.007",NA,NA,NA
"rayyan-100677277","Deep learning for geological hazards analysis: Data, models, applications, and opportunities",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"As natural disasters are induced by geodynamic activities or abnormal changes in the environment, geological hazards tend to wreak havoc on the environment and human society. Recently, the dramatic increase in the volume of various types of Earth observation ‘big data’ from multiple sources, and the rapid development of deep learning as a state-of-the-art data analysis tool, have enabled novel advances in geological hazard analysis, with the ultimate aim to mitigate the devastation associated with these hazards. Motivated by numerous applications, this paper presents an overview of the advances in the utilization of deep learning for geological hazard analysis. First, six commonly available Earth observation data sources are described, e.g., unmanned aerial vehicles, satellite platforms, and in-situ monitoring systems. Second, the deep learning background and six typical deep learning models are introduced, such as convolutional neural networks and recurrent neural networks. Third, focusing on six typical geological hazards, i.e., landslides, debris flows, rockfalls, avalanches, earthquakes, and volcanoes, the deep learning applications for geological hazard analysis are reviewed, and common application paradigms are summarized. Finally, the challenges and opportunities for the application of deep learning models for geological hazard analysis are highlighted, with the aim to inspire further related research.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1016/j.earscirev.2021.103858",NA,NA,NA
"rayyan-100677279","Prediction of droughts over Pakistan using machine learning algorithms",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate | USER-NOTES: {""Maria""=>[""Climate change has increased frequency, severity and areal extent of droughts across the world in the last few decades magnifying their adverse impacts. Prediction of droughts is immensely helpful in early warning and preparing the most vulnerable communities to their adverse impacts. For the first time, this study investigated the potential of developing drought prediction models over Pakistan using three state-of-the-art Machine Learning (ML) techniques; Support Vector Machine (SVM), Artificial Neural Network (ANN) and k-Nearest Neighbour (KNN). Three categories of droughts; moderate, severe, and extreme considering two major cropping seasons called Rabi and Kharif were estimated using Standardized Precipitation Evaporation Index (SPEI) and then predicted using the predictor data obtained from the National Centres for Environmental Prediction/National Centre for Atmospheric Research (NCEP/NCAR) reanalysis database."", ""Also, for the first time in drought modelling, a novel feature selection approach called Recursive Feature Elimination (RFE) was used for identifying optimum sets of predictors. In validation, SVM-based models were able to better capture the temporal and spatial characteristics of droughts over Pakistan compared to those by ANN and KNN-based models. KNN which was used in developing drought models for the first time displayed limited performance in comparison to that by SVM and ANN-based drought models, in validation. It was found that in the Rabi season SPEI is positively correlated with relative humidity over the Mediterranean Sea and the region north of the Caspian Sea. In the Kharif season, SPEI is positively correlated with the humid region over the south-eastern part of the Bay of Bengal and the regions north of the Mediterranean and Caspian Seas. "", ""In developing a drought prediction model for Pakistan, relative humidity, temperature and wind speed should be considered with a domain which encompasses the Mediterranean Sea, the region north of the Caspian Sea, the Indian Ocean and the Arabian Sea.""]}","https://doi.org/10.1016/j.advwatres.2020.103562",NA,NA,NA
"rayyan-100677280","Deep-Learning-Based Earthquake Detection for Fiber-Optic Distributed Acoustic Sensing",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"In this paper, deep learning models trained with real seismic data are proposed and proven to detect earthquakes in fiber-optic distributed acoustic sensor (DAS) measurements. The proposed neural network architectures cover the three classical deep learning paradigms: fully connected artificial neural networks (FC-ANNs), convolutional neural networks (CNNs) and recurrent neural networks (RNNs). Results demonstrate that training these networks with seismic waveforms measured by traditional broadband seismometers can extract and learn relevant features of earthquakes, enabling the reliable detection of seismic waves in DAS measurements. The intrinsic differences between DAS and seismograph waveforms, and eventual errors in the labelling of the DAS data, slightly reduce the performance of the models when tested with the distributed acoustic measurements. Despites of that, trained models can still reach up to 96.94% accuracy in the case of CNN and 93.86% in the case of CNN+RNN. The method and results here reported could represent an important contribution to the development of an early warning earthquake system based on DAS technology.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1109/jlt.2021.3138724",NA,NA,NA
"rayyan-100677283","Analog Forecasting of Extreme‐Causing Weather Patterns Using Deep Learning",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract Numerical weather prediction models require ever‐growing computing time and resources but, still, have sometimes difficulties with predicting weather extremes. We introduce a data‐driven framework that is based on analog forecasting (prediction using past similar patterns) and employs a novel deep learning pattern‐recognition technique (capsule neural networks, CapsNets) and an impact‐based autolabeling strategy. Using data from a large‐ensemble fully coupled Earth system model, CapsNets are trained on midtropospheric large‐scale circulation patterns (Z500) labeled 0–4 depending on the existence and geographical region of surface temperature extremes over North America several days ahead. The trained networks predict the occurrence/region of cold or heat waves, only using Z500, with accuracies (recalls) of 69–45% (77–48%) or 62–41% (73–47%) 1–5 days ahead. Using both surface temperature and Z500, accuracies (recalls) with CapsNets increase to 80% (88%). In both cases, CapsNets outperform simpler techniques such as convolutional neural networks and logistic regression, and their accuracy is least affected as the size of the training set is reduced. The results show the promises of multivariate data‐driven frameworks for accurate and fast extreme weather predictions, which can potentially augment numerical weather prediction efforts in providing early warnings.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1029/2019ms001958",NA,NA,NA
"rayyan-100677284","The promise of implementing machine learning in earthquake engineering: A state-of-the-art review",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Machine learning (ML) has evolved rapidly over recent years with the promise to substantially alter and enhance the role of data science in a variety of disciplines. Compared with traditional approaches, ML offers advantages to handle complex problems, provide computational efficiency, propagate and treat uncertainties, and facilitate decision making. Also, the maturing of ML has led to significant advances in not only the main-stream artificial intelligence (AI) research but also other science and engineering fields, such as material science, bioengineering, construction management, and transportation engineering. This study conducts a comprehensive review of the progress and challenges of implementing ML in the earthquake engineering domain. A hierarchical attribute matrix is adopted to categorize the existing literature based on four traits identified in the field, such as ML method, topic area, data resource, and scale of analysis. The state-of-the-art review indicates to what extent ML has been applied in four topic areas of earthquake engineering, including seismic hazard analysis, system identification and damage detection, seismic fragility assessment, and structural control for earthquake mitigation. Moreover, research challenges and the associated future research needs are discussed, which include embracing the next generation of data sharing and sensor technologies, implementing more advanced ML techniques, and developing physics-guided ML models.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1177/8755293020919419",NA,NA,NA
"rayyan-100677292","Building damage assessment for rapid disaster response with a deep object-based semantic change detection framework: From natural disasters to man-made disasters",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate | USER-NOTES: {""Maria""=>[""Sudden-onset natural and man-made disasters represent a threat to the safety of human life and property. Rapid and accurate building damage assessment using bitemporal high spatial resolution (HSR) remote sensing images can quickly and safely provide us with spatial distribution information and statistics of the damage degree to assist with humanitarian assistance and disaster response. For building damage assessment, strong feature representation and semantic consistency are the keys to obtaining a high accuracy. However, the conventional object-based image analysis (OBIA) framework using a patch-based convolutional neural network (CNN) can guarantee semantic consistency, but with weak feature representation, while the Siamese fully convolutional network approach has strong feature representation capabilities but is semantically inconsistent. In this paper, we propose a deep object-based semantic change detection framework, called ChangeOS, for building damage assessment. "", ""To seamlessly integrate OBIA and deep learning, we adopt a deep object localization network to generate accurate building objects, in place of the superpixel segmentation commonly used in the conventional OBIA framework. Furthermore, the deep object localization network and deep damage classification network are integrated into a unified semantic change detection network for end-to-end building damage assessment. This also provides deep object features that can supply an object prior to the deep damage classification network for more consistent semantic feature representation. Object-based post-processing is adopted to further guarantee the semantic consistency of each object."", ""The experimental results obtained on a global scale dataset including 19 natural disaster events and two local scale datasets including the Beirut port explosion event and the Bata military barracks explosion event show that ChangeOS is superior to the currently published methods in speed and accuracy, and has a superior generalization ability for man-made disasters.""]}","https://doi.org/10.1016/j.rse.2021.112636",NA,NA,NA
"rayyan-100677297","Early forecasting of tsunami inundation from tsunami and geodetic observation data with convolutional neural networks",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract Rapid and accurate hazard forecasting is important for prompt evacuations and reducing casualties during natural disasters. In the decade since the 2011 Tohoku tsunami, various tsunami forecasting methods using real-time data have been proposed. However, rapid and accurate tsunami inundation forecasting in coastal areas remains challenging. Here, we propose a tsunami forecasting approach using convolutional neural networks (CNNs) for early warning. Numerical tsunami forecasting experiments for Tohoku demonstrated excellent performance with average maximum tsunami amplitude and tsunami arrival time forecasting errors of ~0.4 m and ~48 s, respectively, for 1,000 unknown synthetic tsunami scenarios. Our forecasting approach required only 0.004 s on average using a single CPU node. Moreover, the CNN trained on only synthetic tsunami scenarios provided reasonable inundation forecasts using actual observation data from the 2011 event, even with noisy inputs. These results verify the feasibility of AI-enabled tsunami forecasting for providing rapid and accurate early warnings.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1038/s41467-021-22348-0",NA,NA,NA
"rayyan-100677299","Forest Fire Prediction Based on Long- and Short-Term Time-Series Network",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Modeling and prediction of forest fire occurrence play a key role in guiding forest fire prevention. From the perspective of the whole world, forest fires are a natural disaster with a great degree of hazard, and many countries have taken mountain fire prediction as an important measure for fire prevention and control, and have conducted corresponding research. In this study, a forest fire prediction model based on LSTNet is proposed to improve the accuracy of forest fire forecasts. The factors that influence forest fires are obtained through remote sensing satellites and GIS, and their correlation is estimated using Pearson correlation analysis and testing for multicollinearity. To account for the spatial aggregation of forest fires, the data set was constructed using oversampling methods and proportional stratified sampling, and the LSTNet forest fire prediction model was established based on eight influential factors. Finally, the predicted data were incorporated into the model and the predicted risk map of forest fires in Chongli, China was drawn. This paper uses metrics such as RMSE to compare with traditional machine learning methods, and the results show that the LSTNet model proposed in this paper has high accuracy (ACC 0.941). This study illustrates that the model can effectively use spatial background information and the periodicity of forest fire factors, and is a novel method for spatial prediction of forest fire susceptibility.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.3390/f14040778",NA,NA,NA
"rayyan-100677315","Modern technologies and solutions to enhance surveillance and response systems for emerging zoonotic diseases",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Zoonotic diseases originating from animals pose a significant threat to global public health. Recent outbreaks, such as COVID-19, have caused widespread illness, death, and socioeconomic disruptions worldwide. To effectively combat these diseases, it is crucial to strengthen surveillance capabilities and establish rapid response systems. This review examines modern technologies and solutions that have the potential to enhance zoonotic disease surveillance and outbreak response. The review discusses advanced tools including big data analytics, artificial intelligence, Internet of Things, geographic information systems, remote sensing, molecular diagnostics, point-of-care testing, telemedicine, digital contact tracing, and early warning systems. These technologies enable real-time monitoring, prediction of outbreak risks, early anomaly detection, rapid diagnosis, and targeted interventions during outbreaks. When integrated thoughtfully through collaborative partnerships, they have the potential to significantly improve the speed and effectiveness of zoonotic disease control. However, several challenges persist, particularly in resource-limited settings, including infrastructure limitations, costs, data integration, training requirements, and ethical implementation. With strategic planning and coordinated efforts, modern technologies and solutions offer immense potential to bolster surveillance and outbreak response, serving as a critical arsenal against emerging zoonotic disease threats worldwide.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.1016/j.soh.2023.100061",NA,NA,NA
"rayyan-100677316","Deep Machine Learning Based Possible Atmospheric and Ionospheric Precursors of the 2021 Mw 7.1 Japan Earthquake",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Global Navigation Satellite System (GNSS)- and Remote Sensing (RS)-based Earth observations have a significant approach on the monitoring of natural disasters. Since the evolution and appearance of earthquake precursors exhibit complex behavior, the need for different methods on multiple satellite data for earthquake precursors is vital for prior and after the impending main shock. This study provided a new approach of deep machine learning (ML)-based detection of ionosphere and atmosphere precursors. In this study, we investigate multi-parameter precursors of different physical nature defining the states of ionosphere and atmosphere associated with the event in Japan on 13 February 2021 (Mw 7.1). We analyzed possible precursors from surface to ionosphere, including Sea Surface Temperature (SST), Air Temperature (AT), Relative Humidity (RH), Outgoing Longwave Radiation (OLR), and Total Electron Content (TEC). Furthermore, the aim is to find a possible pre-and post-seismic anomaly by implementing standard deviation (STDEV), wavelet transformation, the Nonlinear Autoregressive Network with Exogenous Inputs (NARX) model, and the Long Short-Term Memory Inputs (LSTM) network. Interestingly, every method shows anomalous variations in both atmospheric and ionospheric precursors before and after the earthquake. Moreover, the geomagnetic irregularities are also observed seven days after the main shock during active storm days (Kp &gt; 3.7; Dst &lt; −30 nT). This study demonstrates the significance of ML techniques for detecting earthquake anomalies to support the Lithosphere-Atmosphere-Ionosphere Coupling (LAIC) mechanism for future studies.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.3390/rs15071904",NA,NA,NA
"rayyan-100677320","Warming and greening exacerbate the propagation risk from meteorological to soil moisture drought",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate | USER-NOTES: {""Maria""=>[""Quantifying the propagation time (PT) and trigger threshold (TR) from meteorological to soil moisture drought is critically important for drought early warning and precise defense. Nevertheless, existing propagation characteristics usually have a low temporal-spatial resolution, and their dynamics and related driving mechanisms (such as global warming and greening) are still incompletely understood. To fill the knowledge gap, this study proposes a drought propagation model based on the Bayesian causal analysis framework for quantifying the PT and TR with a high resolution. Taking Northeast China (NEC) as a case study, we further explore the dynamics of drought propagation characteristics in recent decades and possible driving mechanisms using the sliding window and Random Forest model."", ""Results showed that: (1) the drought PT varies spatially and temporally in the study area, with long PT in the central plain and western high-altitude areas in the early growing season (typically over 200 days), while short in the middle and late growing season in most regions (less than one month in July and August), The TR is generally lower than 80 mm in the western regions and do not change significantly with time; (2) the PT and TR in the vast central and western regions exhibit a downward trend in the late growing season, resulting in a strikingly increased risk of drought propagation; (3) Increasing vapor pressure deficit (VPD) due to warming, along with decreasing aridity index (AI) due to precipitation shortage are the main drivers on the accelerated drought propagation."", ""Moreover, local greening has also played a critical role in accelerating propagation via transpiration that consumes soil water, which contributes more than 20% to propagation dynamics. Overall, this study sheds new insights into drought propagation dynamics and mechanisms in a changing environment, providing a promising avenue for drought early warning and mitigation.""]}","https://doi.org/10.1016/j.jhydrol.2023.129716",NA,NA,NA
"rayyan-100677325","Predicting land use effects on flood susceptibility using machine learning and remote sensing in coastal Vietnam",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract Flood damage is becoming increasingly severe in the context of climate change and changes in land use. Assessing the effects of these changes on floods is important, to help decision-makers and local authorities understand the causes of worsening floods and propose appropriate measures. The objective of this study was to evaluate the effects of climate and land use change on flood susceptibility in Thua Thien Hue province, Vietnam, using machine learning techniques (support vector machine (SVM) and random forest (RF)) and remote sensing. The machine learning models used a flood inventory including 1,864 flood locations and 11 conditional factors in 2017 and 2021, as the input data. The predictive capacity of the proposed models was assessed using the area under the curve (AUC), the root mean square error (RMSE), and the mean absolute error (MAE). Both proposed models were successful, with AUC values exceeding 0.95 in predicting the effects of climate and land use change on flood susceptibility. The RF model, with AUC = 0.98, outperformed the SVM model (AUC = 0.97). The areas most susceptible to flooding increased between 2017 and 2021 due to increased built-up area.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.2166/wpt.2023.088",NA,NA,NA
"rayyan-100677341","Future changes in water resources, floods and droughts under the joint impact of climate and land-use changes in the Chao Phraya basin, Thailand",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Global Climate change and local human activities have profoundly affected the regional hydrological cycle and water resources. It is imperative to explore the potential changes in future water resources and water-related hazards at the regional scale under global warming and local socioeconomic development, while a scientific assessment of future hydrological risks requires reasonable projection of future climate, land use and vegetation changes. In order to improve the traditional statistical downscaling method, this study combines the machine learning and quantile mapping methods to project future climate under four shared socio-economic pathway-representative concentration pathways (SSP-RCP) of the CMIP6. Future land use is projected jointly with the future climate by the CA-Markov model, and the vegetation dynamics are simulated by the Biome-BGC model. Then we employ a physically-based distributed hydrological model to simulate the future hydrological changes in the Upper Chao Phraya basin under the interaction among climate and land use changes and the vegetation dynamics. The results show that under the joint impact of climate and land-use changes, the study area may face increasing water scarcity and more frequent floods and droughts in the future. Water scarcity will reach the worst in the mid-21st century (water resources per capita decrease 34.2% compared to the 2010 s). By the end-21st century, the 100-year historical flood and drought in the study basin will increase by 1.63 times and 0.59 times, respectively, under the SSP126 scenario (the most sustainable pathway), and by 4.55 times and 1.56 times under the SSP370 scenario (the most pessimistic rocky-road pathway). Results demonstrate that climate change is the major cause for more frequent floods and droughts in the future, while afforestation or more sustainable land use management will mitigate the adverse effects of climate change to some extent. This finding is helpful to the local government in managing future water resources, floods, and droughts in the study basin.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1016/j.jhydrol.2023.129454",NA,NA,NA
"rayyan-100677342","Enhancing FAIR Data Services in Agricultural Disaster: A Review",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"The agriculture sector is highly vulnerable to natural disasters and climate change, leading to severe impacts on food security, economic stability, and rural livelihoods. The use of geospatial information and technology has been recognized as a valuable tool to help farmers reduce the adverse impacts of natural disasters on agriculture. Remote sensing and GIS are gaining traction as ways to improve agricultural disaster response due to recent advancements in spatial resolution, accessibility, and affordability. This paper presents a comprehensive overview of the FAIR agricultural disaster services. It holistically introduces the current status, case studies, technologies, and challenges, and it provides a big picture of exploring geospatial applications for agricultural disaster “from farm to space”. The review begins with an overview of the governments and organizations worldwide. We present the major international and national initiatives relevant to the agricultural disaster context. The second part of this review illustrates recent research on remote sensing-based agricultural disaster monitoring, with a special focus on drought and flood events. Traditional, integrative, and machine learning-based methods are highlighted in this section. We then examine the role of spatial data infrastructure and research on agricultural disaster services and systems. The generic lifecycle of agricultural disasters is briefly introduced. Eventually, we discuss the grand challenges and emerging opportunities that range from analysis-ready data to decision-ready services, providing guidance on the foreseeable future.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.3390/rs15082024",NA,NA,NA
"rayyan-100677352","Forest fire and smoke detection using deep learning-based learning without forgetting",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract Background Forests are an essential natural resource to humankind, providing a myriad of direct and indirect benefits. Natural disasters like forest fires have a major impact on global warming and the continued existence of life on Earth. Automatic identification of forest fires is thus an important field to research in order to minimize disasters. Early fire detection can also help decision-makers plan mitigation methods and extinguishing tactics. This research looks at fire/smoke detection from images using AI-based computer vision techniques. Convolutional Neural Networks (CNN) are a type of Artificial Intelligence (AI) approach that have been shown to outperform state-of-the-art methods in image classification and other computer vision tasks, but their training time can be prohibitive. Further, a pretrained CNN may underperform when there is no sufficient dataset available. To address this issue, transfer learning is exercised on pre-trained models. However, the models may lose their classification abilities on the original datasets when transfer learning is applied. To solve this problem, we use learning without forgetting (LwF), which trains the network with a new task but keeps the network’s preexisting abilities intact. Results In this study, we implement transfer learning on pre-trained models such as VGG16, InceptionV3, and Xception, which allow us to work with a smaller dataset and lessen the computational complexity without degrading accuracy. Of all the models, Xception excelled with 98.72% accuracy. We tested the performance of the proposed models with and without LwF. Without LwF, among all the proposed models, Xception gave an accuracy of 79.23% on a new task (BowFire dataset). While using LwF, Xception gave an accuracy of 91.41% for the BowFire dataset and 96.89% for the original dataset. We find that fine-tuning the new task with LwF performed comparatively well on the original dataset. Conclusion Based on the experimental findings, it is found that the proposed models outperform the current state-of-the-art methods. We also show that LwF can successfully categorize novel and unseen datasets.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1186/s42408-022-00165-0",NA,NA,NA
"rayyan-100677354","Landslide detection by deep learning of non-nadiral and crowdsourced optical images",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract The recent development of mobile surveying platforms and crowdsourced geoinformation has produced a huge amount of non-validated data that are now available for research and application. In the field of risk analysis, with particular reference to landslide hazard, images generated by autonomous platforms (such as UAVs, ground-based acquisition systems, satellite sensors) and pictures obtained from web data mining are easily gathered and contribute to the fast surge in the amount of non-organized information that may engulf data storage facilities. Therefore, the high potential impact of such methods is severely reduced by the need of a massive amount of human intelligence tasks (HITs), which is necessary to filter and classify the data, whatever the final purpose. In this work, we present a new set of convolutional neural networks (CNNs) specifically designed for the automated recognition of landslides and mass movements in non-standard pictures that can be used in automated image classification, in supporting UAV autonomous guidance and in the filtering of data-mined information. Computer vision can be of great help in fostering the autonomous capability of intelligent systems to complement, or completely substitute, HITs. Image and object recognition are at the forefront of this research field. The deep learning procedure has been accomplished by applying transfer learning to some of the top-performer CNNs available in the literature. Results show that the deep learning machines, calibrated on a relevant dataset of validated images of landforms, may supply reliable predictions with computational time and resource requirements compatible with most of the UAV platforms and web data mining applications in landslide hazard studies. Average accuracy achieved by the proposed methods ranges between 87 and 90% and is consistently higher than that obtained by general-purpose state-of-the-art image recognition convolutional neural networks. The method can be applied to early warning, vulnerability assessment, residual risk estimation, model parameterisation and landslide mapping. Specific advantages will be the reduction of the present limitations in the intelligent guidance of landslide mapping drones, the classification of fake news, the validation of post-disaster information and the correct interpretation of an impending change in the environment.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1007/s10346-020-01513-4",NA,NA,NA
"rayyan-100677355","Machine Learning in Disaster Management: Recent Developments in Methods and Applications",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Recent years include the world’s hottest year, while they have been marked mainly, besides the COVID-19 pandemic, by climate-related disasters, based on data collected by the Emergency Events Database (EM-DAT). Besides the human losses, disasters cause significant and often catastrophic socioeconomic impacts, including economic losses. Recent developments in artificial intelligence (AI) and especially in machine learning (ML) and deep learning (DL) have been used to better cope with the severe and often catastrophic impacts of disasters. This paper aims to provide an overview of the research studies, presented since 2017, focusing on ML and DL developed methods for disaster management. In particular, focus has been given on studies in the areas of disaster and hazard prediction, risk and vulnerability assessment, disaster detection, early warning systems, disaster monitoring, damage assessment and post-disaster response as well as cases studies. Furthermore, some recently developed ML and DL applications for disaster management have been analyzed. A discussion of the findings is provided as well as directions for further research.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.3390/make4020020",NA,NA,NA
"rayyan-100677364","Machine Learning in Tropical Cyclone Forecast Modeling: A Review",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Tropical cyclones have always been a concern of meteorologists, and there are many studies regarding the axisymmetric structures, dynamic mechanisms, and forecasting techniques from the past 100 years. This research demonstrates the ongoing progress as well as the many remaining problems. Machine learning, as a means of artificial intelligence, has been certified by many researchers as being able to provide a new way to solve the bottlenecks of tropical cyclone forecasts, whether using a pure data-driven model or improving numerical models by incorporating machine learning. Through summarizing and analyzing the challenges of tropical cyclone forecasts in recent years and successful cases of machine learning methods in these aspects, this review introduces progress based on machine learning in genesis forecasts, track forecasts, intensity forecasts, extreme weather forecasts associated with tropical cyclones (such as strong winds and rainstorms, and their disastrous impacts), and storm surge forecasts, as well as in improving numerical forecast models. All of these can be regarded as both an opportunity and a challenge. The opportunity is that at present, the potential of machine learning has not been completely exploited, and a large amount of multi-source data have also not been fully utilized to improve the accuracy of tropical cyclone forecasting. The challenge is that the predictable period and stability of tropical cyclone prediction can be difficult to guarantee, because tropical cyclones are different from normal weather phenomena and oceanographic processes and they have complex dynamic mechanisms and are easily influenced by many factors.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.3390/atmos11070676",NA,NA,NA
"rayyan-100677368","Monitoring Forest Change in the Amazon Using Multi-Temporal Remote Sensing Data and Machine Learning Classification on Google Earth Engine",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Deforestation causes diverse and profound consequences for the environment and species. Direct or indirect effects can be related to climate change, biodiversity loss, soil erosion, floods, landslides, etc. As such a significant process, timely and continuous monitoring of forest dynamics is important, to constantly follow existing policies and develop new mitigation measures. The present work had the aim of mapping and monitoring the forest change from 2000 to 2019 and of simulating the future forest development of a rainforest region located in the Pará state, Brazil. The land cover dynamics were mapped at five-year intervals based on a supervised classification model deployed on the cloud processing platform Google Earth Engine. Besides the benefits of reduced computational time, the service is coupled with a vast data catalogue providing useful access to global products, such as multispectral images of the missions Landsat five, seven, eight and Sentinel-2. The validation procedures were done through photointerpretation of high-resolution panchromatic images obtained from CBERS (China–Brazil Earth Resources Satellite). The more than satisfactory results allowed an estimation of peak deforestation rates for the period 2000–2006; for the period 2006–2015, a significant decrease and stabilization, followed by a slight increase till 2019. Based on the derived trends a forest dynamics was simulated for the period 2019–2028, estimating a decrease in the deforestation rate. These results demonstrate that such a fusion of satellite observations, machine learning, and cloud processing, benefits the analysis of the forest dynamics and can provide useful information for the development of forest policies.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.3390/ijgi9100580",NA,NA,NA
"rayyan-100677369","Unprecedented atmospheric conditions (1948–2019) drive the 2019 exceptional melting season over the Greenland ice sheet",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract. Understanding the role of atmospheric circulation anomalies on the surface mass balance of the Greenland ice sheet (GrIS) is fundamental for improving estimates of its current and future contributions to sea level rise. Here, we show, using a combination of remote sensing observations, regional climate model outputs, reanalysis data, and artificial neural networks, that unprecedented atmospheric conditions (1948–2019) occurring in the summer of 2019 over Greenland promoted new record or close-to-record values of surface mass balance (SMB), runoff, and snowfall. Specifically, runoff in 2019 ranked second within the 1948–2019 period (after 2012) and first in terms of surface mass balance negative anomaly for the hydrological year 1 September 2018–31 August 2019. The summer of 2019 was characterized by an exceptional persistence of anticyclonic conditions that, in conjunction with low albedo associated with reduced snowfall in summer, enhanced the melt–albedo feedback by promoting the absorption of solar radiation and favored advection of warm, moist air along the western portion of the ice sheet towards the north, where the surface melt has been the highest since 1948. The analysis of the frequency of daily 500 hPa geopotential heights obtained from artificial neural networks shows that the total number of days with the five most frequent atmospheric patterns that characterized the summer of 2019 was 5 standard deviations above the 1981–2010 mean, confirming the exceptional nature of the 2019 season over Greenland.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.5194/tc-14-1209-2020",NA,NA,NA
"rayyan-100677374","Data-driven predictions of the time remaining until critical global warming thresholds are reached",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Leveraging artificial neural networks (ANNs) trained on climate model output, we use the spatial pattern of historical temperature observations to predict the time until critical global warming thresholds are reached. Although no observations are used during the training, validation, or testing, the ANNs accurately predict the timing of historical global warming from maps of historical annual temperature. The central estimate for the 1.5 °C global warming threshold is between 2033 and 2035, including a ±1σ range of 2028 to 2039 in the Intermediate (SSP2-4.5) climate forcing scenario, consistent with previous assessments. However, our data-driven approach also suggests a substantial probability of exceeding the 2 °C threshold even in the Low (SSP1-2.6) climate forcing scenario. While there are limitations to our approach, our results suggest a higher likelihood of reaching 2 °C in the Low scenario than indicated in some previous assessments—though the possibility that 2 °C could be avoided is not ruled out. Explainable AI methods reveal that the ANNs focus on particular geographic regions to predict the time until the global threshold is reached. Our framework provides a unique, data-driven approach for quantifying the signal of climate change in historical observations and for constraining the uncertainty in climate model projections. Given the substantial existing evidence of accelerating risks to natural and human systems at 1.5 °C and 2 °C, our results provide further evidence for high-impact climate change over the next three decades.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1073/pnas.2207183120",NA,NA,NA
"rayyan-100677375","Deep Learning-Based Extreme Heatwave Forecast",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Because of the impact of extreme heat waves and heat domes on society and biodiversity, their study is a key challenge. We specifically study long-lasting extreme heat waves, which are among the most important for climate impacts. Physics driven weather forecast systems or climate models can be used to forecast their occurrence or predict their probability. The present work explores the use of deep learning architectures, trained using outputs of a climate model, as an alternative strategy to forecast the occurrence of extreme long-lasting heatwaves. This new approach will be useful for several key scientific goals which include the study of climate model statistics, building a quantitative proxy for resampling rare events in climate models, study the impact of climate change, and should eventually be useful for forecasting. Fulfilling these important goals implies addressing issues such as class-size imbalance that is intrinsically associated with rare event prediction, assessing the potential benefits of transfer learning to address the nested nature of extreme events (naturally included in less extreme ones). We train a Convolutional Neural Network, using 1000 years of climate model outputs, with large-class undersampling and transfer learning. From the observed snapshots of the surface temperature and the 500 hPa geopotential height fields, the trained network achieves significant performance in forecasting the occurrence of long-lasting extreme heatwaves. We are able to predict them at three different levels of intensity, and as early as 15 days ahead of the start of the event (30 days ahead of the end of the event).","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.3389/fclim.2022.789641",NA,NA,NA
"rayyan-100677380","Rapid prediction of earthquake ground shaking intensity using raw waveform data and a convolutional neural network",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"This study describes a deep convolutional neural network (CNN) based technique for the prediction of intensity measurements (IMs) of ground shaking. The input data to the CNN model consists of multistation 3C broadband and accelerometric waveforms recorded during the 2016 Central Italy earthquake sequence for M $\ge$ 3.0. We find that the CNN is capable of predicting accurately the IMs at stations far from the epicenter and that have not yet recorded the maximum ground shaking when using a 10 s window starting at the earthquake origin time. The CNN IM predictions do not require previous knowledge of the earthquake source (location and magnitude). Comparison between the CNN model predictions and the predictions obtained with Bindi et al. (2011) GMPE (which require location and magnitude) has shown that the CNN model features similar error variance but smaller bias. Although the technique is not strictly designed for earthquake early warning, we found that it can provide useful estimates of ground motions within 15-20 sec after earthquake origin time depending on various setup elements (e.g., times for data transmission, computation, latencies). The technique has been tested on raw data without any initial data pre-selection in order to closely replicate real-time data streaming. When noise examples were included with the earthquake data, the CNN was found to be stable predicting accurately the ground shaking intensity corresponding to the noise amplitude.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1093/gji/ggaa233",NA,NA,NA
"rayyan-100677394","Toward an Integrated Disaster Management Approach: How Artificial Intelligence Can Boost Disaster Management",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Technical and methodological enhancement of hazards and disaster research is identified as a critical question in disaster management. Artificial intelligence (AI) applications, such as tracking and mapping, geospatial analysis, remote sensing techniques, robotics, drone technology, machine learning, telecom and network services, accident and hot spot analysis, smart city urban planning, transportation planning, and environmental impact analysis, are the technological components of societal change, having significant implications for research on the societal response to hazards and disasters. Social science researchers have used various technologies and methods to examine hazards and disasters through disciplinary, multidisciplinary, and interdisciplinary lenses. They have employed both quantitative and qualitative data collection and data analysis strategies. This study provides an overview of the current applications of AI in disaster management during its four phases and how AI is vital to all disaster management phases, leading to a faster, more concise, equipped response. Integrating a geographic information system (GIS) and remote sensing (RS) into disaster management enables higher planning, analysis, situational awareness, and recovery operations. GIS and RS are commonly recognized as key support tools for disaster management. Visualization capabilities, satellite images, and artificial intelligence analysis can assist governments in making quick decisions after natural disasters.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.3390/su132212560",NA,NA,NA
"rayyan-100677396","Autonomous Satellite Wildfire Detection Using Hyperspectral Imagery and Neural Networks: A Case Study on Australian Wildfire",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"One of the United Nations (UN) Sustainable Development Goals is climate action (SDG-13), and wildfire is among the catastrophic events that both impact climate change and are aggravated by it. In Australia and other countries, large-scale wildfires have dramatically grown in frequency and size in recent years. These fires threaten the world’s forests and urban woods, cause enormous environmental and property damage, and quite often result in fatalities. As a result of their increasing frequency, there is an ongoing debate over how to handle catastrophic wildfires and mitigate their social, economic, and environmental repercussions. Effective prevention, early warning, and response strategies must be well-planned and carefully coordinated to minimise harmful consequences to people and the environment. Rapid advancements in remote sensing technologies such as ground-based, aerial surveillance vehicle-based, and satellite-based systems have been used for efficient wildfire surveillance. This study focuses on the application of space-borne technology for very accurate fire detection under challenging conditions. Due to the significant advances in artificial intelligence (AI) techniques in recent years, numerous studies have previously been conducted to examine how AI might be applied in various situations. As a result of its special physical and operational requirements, spaceflight has emerged as one of the most challenging application fields. This work contains a feasibility study as well as a model and scenario prototype for a satellite AI system. With the intention of swiftly generating alerts and enabling immediate actions, the detection of wildfires has been studied with reference to the Australian events that occurred in December 2019. Convolutional neural networks (CNNs) were developed, trained, and used from the ground up to detect wildfires while also adjusting their complexity to meet onboard implementation requirements for trusted autonomous satellite operations (TASO). The capability of a 1-dimensional convolution neural network (1-DCNN) to classify wildfires is demonstrated in this research and the results are assessed against those reported in the literature. In order to enable autonomous onboard data processing, various hardware accelerators were considered and evaluated for onboard implementation. The trained model was then implemented in the following: Intel Movidius NCS-2 and Nvidia Jetson Nano and Nvidia Jetson TX2. Using the selected onboard hardware, the developed model was then put into practice and analysis was carried out. The results were positive and in favour of using the technology that has been proposed for onboard data processing to enable TASO on future missions. The findings indicate that data processing onboard can be very beneficial in disaster management and climate change mitigation by facilitating the generation of timely alerts for users and by enabling rapid and appropriate responses.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.3390/rs15030720",NA,NA,NA
"rayyan-100677402","A Contemporary Systematic Review of Cyberinfrastructure Systems and Applications for Flood and Drought Data Analytics and Communication",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Hydrometeorological disasters, including floods and droughts, have intensified in both frequency and severity in recent years. This trend underscores the critical role of timely monitoring, accurate forecasting, and effective warning systems in facilitating proactive responses. Today's information systems offer a vast and intricate mesh of data, encompassing satellite imagery, meteorological metrics, and predictive modeling. Easily accessible to the general public, these cyberinfrastructures simulate potential disaster scenarios, serving as invaluable aids to decision-making processes. This review collates key literature on water-related disaster information systems, underscoring the transformative impact of emerging information and Internet technologies. These advancements promise enhanced flood and drought warning timeliness and greater preparedness through improved management, analysis, visualization, and data sharing. Moreover, these systems aid in hydrometeorological predictions, foster the development of web-based educational platforms, and support decision-making frameworks, digital twins, and metaverse applications in disaster contexts. They further bolster scientific research and development, enrich climate change vulnerability frameworks, and strengthen associated cyberinfrastructures. This article delves into prospective developments in the realm of natural disasters, pinpointing primary challenges and gaps in current water-related disaster information systems, and highlighting the potential intersections with future artificial intelligence solutions.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.31223/x5937w",NA,NA,NA
"rayyan-100677409","Receptor-based detection of microplastics and nanoplastics: Current and future",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Plastic pollution is an emerging environmental concern, gaining significant attention worldwide. They are classified into microplastics (MP; defined from 1 μm to 5 mm) and smaller nanoplastics (NP; <1 μm). NPs may pose higher ecological risks than MPs. Various microscopic and spectroscopic techniques have been used to detect MPs, and the same methods have occasionally been used for NPs. However, they are not based on receptors, which provide high specificity in most biosensing applications. Receptor-based micro/nanoplastics (MNP) detection can provide high specificity, distinguishing MNPs from the environmental samples and, more importantly, identifying the plastic types. It can also offer a low limit of detection (LOD) required for environmental screening. Such receptors are expected to detect NPs specifically at the molecular level. This review categorizes the receptors into cells, proteins, peptides, fluorescent dyes, polymers, and micro/nanostructures. Detection techniques used with these receptors are also summarized and categorized. There is plenty of room for future research to test for broader classes of environmental samples and many plastic types, to lower the LOD, and to apply the current techniques for NPs. Portable and handheld MNP detection should also be demonstrated for field use since the current demonstrations primarily utilized laboratory instruments. Detection on microfluidic platforms will also be crucial in miniaturizing and automating the assay and, eventually, collecting an extensive database to support machine learning-based classification of MNP types.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1016/j.bios.2023.115361",NA,NA,NA
"rayyan-100677410","An Effective Approach for Automatic River Features Extraction Using High-Resolution UAV Imagery",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"The effects of climate change are causing an increase in the frequency and extent of natural disasters. Because of their morphological characteristics, rivers can cause major flooding events. Indeed, they can be subjected to variations in discharge in response to heavy rainfall and riverbank failures. Among the emerging methodologies that address the monitoring of river flooding, those that include the combination of Unmanned Aerial Vehicle (UAV) and photogrammetric techniques (i.e., Structure from Motion-SfM) ensure the high-frequency acquisition of high-resolution spatial data over wide areas and so the generation of orthomosaics, useful for automatic feature extraction. Trainable Weka Segmentation (TWS) is an automatic feature extraction open-source tool. It was developed to primarily fulfill supervised classification purposes of biological microscope images, but its usefulness has been demonstrated in several image pipelines. At the same time, there is a significant lack of published studies on the applicability of TWS with the identification of a universal and efficient combination of machine learning classifiers and segmentation approach, in particular with respect to classifying UAV images of riverine environments. In this perspective, we present a study comparing the accuracy of nine combinations, classifier plus image segmentation filter, using TWS, also with respect to human photo-interpretation, in order to identify an effective supervised approach for automatic river features extraction from UAV multi-temporal orthomosaics. The results, which are very close to human interpretation, indicate that the proposed approach could prove to be a valuable tool to support and improve the hydro-geomorphological and flooding hazard assessments in riverine environments.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.3390/drones7020070",NA,NA,NA
"rayyan-100677411","Advances and gaps in the science and practice of impact‐based forecasting of droughts",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract Advances in impact modeling and numerical weather forecasting have allowed accurate drought monitoring and skilful forecasts that can drive decisions at the regional scale. State‐of‐the‐art drought early‐warning systems are currently based on statistical drought indicators, which do not account for dynamic regional vulnerabilities, and hence neglect the socio‐economic impact for initiating actions. The transition from conventional physical forecasts of droughts toward impact‐based forecasting (IbF) is a recent paradigm shift in early warning services, to ultimately bridge the gap between science and action. The demand to generate predictions of “what the weather will do” underpins the rising interest in drought IbF across all weather‐sensitive sectors. Despite the large expected socio‐economic benefits, migrating to this new paradigm presents myriad challenges. In this article, we provide a comprehensive overview of drought IbF, outlining the progress made in the field. Additionally, we present a road map highlighting current challenges and limitations in the science and practice of drought IbF and possible ways forward. We identify seven scientific and practical challenges/limitations: the contextual challenge (inadequate accounting for the spatio‐sectoral dynamics of vulnerability and exposure), the human‐water feedbacks challenge (neglecting how human activities influence the propagation of drought), the typology challenge (oversimplifying drought typology to meteorological), the model challenge (reliance on mainstream machine learning models), and the data challenge (mainly textual) with the linked sectoral and geographical limitations. Our vision is to facilitate the progress of drought IbF and its use in making informed and timely decisions on mitigation measures, thus minimizing the drought impacts globally. This article is categorized under: Science of Water &gt; Water Extremes Science of Water &gt; Methods Science of Water &gt; Water and Environmental Change","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1002/wat2.1698",NA,NA,NA
"rayyan-100677418","A Machine Learning Framework for Predicting and Understanding the Canadian Drought Monitor",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract Drought is a costly natural disaster that impacts economies and ecosystems worldwide, so monitoring drought and communicating its impacts to individuals, communities, industry, and governments is important for mitigation, adaptation, and decision‐making. This research describes a novel machine learning framework to predict and understand the Canadian Drought Monitor (CDM). This fully automated approach is trained on nearly two decades of expert analysis and would assist the comprehensive monitoring of drought impacts without the continued requirement of ground support, a benefit in many data‐limited areas across the country. The framework also integrates the Shapley Additive Explanation (SHAP) variable importance metric to provide insight into drought dynamics in near real‐time, demonstrating its usefulness for understanding the value of different data sets for drought assessments and dispelling the commonly held misconception that machine learning models are not useful for inference. The results demonstrate that the model can effectively predict the CDM maps and realistically capture the evolution of drought events over time. A SHAP analysis found that the Prairie drought of 2015 was related to a strong El Niño event that reduced water supply to a region already facing long‐term water deficits, and the subsequent reduction in groundwater availability was detected by the Gravity Recovery and Climate Experiment satellite. Overall, this research shows strong potential to streamline the CDM methodology, integrate scientific insight into operations in near real‐time using SHAP values, and provide an avenue to retrospectively extend the CDM for evaluating current and future drought events in a historical context.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1029/2022wr033847",NA,NA,NA
"rayyan-100677421","Developing risk assessment framework for wildfire in the United States – A deep learning approach to safety and sustainability",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"The frequency and intensity of wildfires have significantly increased in the United States over recent decades, posing profound threats to community safety and ecological sustainability. The escalating losses of human life, property, and biodiversity demand a proactive approach to wildfire prediction and management. This study proposes a highly efficient deep learning framework, utilizing a geospatial database of wildfire incidents in the United States from 1992 to 2018, aimed at bolstering our collective resilience against such disasters. The framework comprises two components: firstly, leveraging Deep Neural Networks (DNN), the cause and size of potential wildfires are predicted, achieving accuracy rates of 76.9% and 76.4% for 5-class classification respectively. Secondly, a forecast model using Long Short Term Memory Networks (LSTM) and an autoencoder is used to anticipate the likelihood of imminent wildfires, focusing on highly at-risk regions such as California. A specific model created to perform one-week forecasts for California achieved a coefficient of determination (R2) and root-mean-square error (RMSE) of 0.90 and 49.5076, respectively. These predictive models offer a significant step towards advancing community safety and environmental sustainability by facilitating timely and effective responses, thereby mitigating the catastrophic effects of wildfires on human life, properties, and delicate ecosystems.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1016/j.jsasus.2023.09.002",NA,NA,NA
"rayyan-100677439","Explainable machine learning for the prediction and assessment of complex drought impacts",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate | USER-NOTES: {""Maria""=>[""Drought is a common and costly natural disaster with broad social, economic, and environmental impacts. Machine learning (ML) has been widely applied in scientific research because of its outstanding performance on predictive tasks. However, for practical applications like disaster monitoring and assessment, the cost of the models failure, especially false negative predictions, might significantly affect society. Stakeholders are not satisfied with or do not “trust” the predictions from a so-called black box. The explainability of ML models becomes progressively crucial in studying drought and its impacts. In this work, we propose an explainable ML pipeline using the XGBoost model and SHAP model based on a comprehensive database of drought impacts in the U.S. "", ""The XGBoost models significantly outperformed the baseline models in predicting the occurrence of multi-dimensional drought impacts derived from the text-based Drought Impact Reporter, attaining an average   score of 0.883 at the national level and 0.942 at the state level. The interpretation of the models at the state scale indicates that the Standardized Precipitation Index (SPI) and Standardized Temperature Index (STI) contribute significantly to predicting multi-dimensional drought impacts. The time scalar, importance, and relationships of the SPI and STI vary depending on the types of drought impacts and locations. "", ""The patterns between the SPI variables and drought impacts indicated by the SHAP values reveal an expected relationship in which negative SPI values positively contribute to complex drought impacts. The explainability based on the SPI variables improves the trustworthiness of the XGBoost models. Overall, this study reveals promising results in accurately predicting complex drought impacts and rendering the relationships between the impacts and indicators more interpretable. This study also reveals the potential of utilizing explainable ML for the general social good to help stakeholders better understand the multi-dimensional drought impacts at the regional level and motivate appropriate responses.""]}","https://doi.org/10.1016/j.scitotenv.2023.165509",NA,NA,NA
"rayyan-100677442","Implementation and Evaluation of a Machine Learned Mesoscale Eddy Parameterization Into a Numerical Ocean Circulation Model",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract We address the question of how to use a machine learned (ML) parameterization in a general circulation model (GCM), and assess its performance both computationally and physically. We take one particular ML parameterization (Guillaumin &amp; Zanna, 2021, https://doi.org/10.1002/essoar.10506419.1 ) and evaluate the online performance in a different model from which it was previously tested. This parameterization is a deep convolutional network that predicts parameters for a stochastic model of subgrid momentum forcing by mesoscale eddies. We treat the parameterization as we would a conventional parameterization once implemented in the numerical model. This includes trying the parameterization in a different flow regime from that in which it was trained, at different spatial resolutions, and with other differences, all to test generalization. We assess whether tuning is possible, which is a common practice in GCM development. We find the parameterization, without modification or special treatment, to be stable and that the action of the parameterization to be diminishing as spatial resolution is refined. We also find some limitations of the machine learning model in implementation: (a) tuning of the outputs from the parameterization at various depths is necessary; (b) the forcing near boundaries is not predicted as well as in the open ocean; (c) the cost of the parameterization is prohibitively high on central processing units. We discuss these limitations, present some solutions to problems, and conclude that this particular ML parameterization does inject energy, and improve backscatter, as intended but it might need further refinement before we can use it in production mode in contemporary climate models.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1029/2023ms003697",NA,NA,NA
"rayyan-100677448","Stochastic‐Deep Learning Parameterization of Ocean Momentum Forcing",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract Coupled climate simulations that span several hundred years cannot be run at a high‐enough spatial resolution to resolve mesoscale ocean dynamics. Recently, several studies have considered Deep Learning to parameterize subgrid forcing within macroscale ocean equations using data from ocean‐only simulations with idealized geometry. We present a stochastic Deep Learning parameterization that is trained on data generated by CM2.6, a high‐resolution state‐of‐the‐art coupled climate model. We train a Convolutional Neural Network for the subgrid momentum forcing using macroscale surface velocities from a few selected subdomains with different dynamical regimes. At each location of the coarse grid, rather than predicting a single number for the subgrid momentum forcing, we predict both the mean and standard deviation of a Gaussian probability distribution. This approach requires training our neural network to minimize a negative log‐likelihood loss function rather than the Mean Square Error, which has been the standard in applications of Deep Learning to the problem of parameterizations. Each estimate of the conditional mean subgrid forcing is thus associated with an uncertainty estimate–the standard deviation—which will form the basis for a stochastic subgrid parameterization. Offline tests show that our parameterization generalizes well to the global oceans and a climate with increased levels without further training. We then implement our learned stochastic parameterization in an eddy‐permitting idealized shallow water model. The implementation is stable and improves some statistics of the flow. Our work demonstrates the potential of combining Deep Learning tools with a probabilistic approach in parameterizing unresolved ocean dynamics.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1029/2021ms002534",NA,NA,NA
"rayyan-100677452","High-resolution downscaling with interpretable deep learning: Rainfall extremes over New Zealand",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"The gap in resolution between existing global climate model output and that sought by decision-makers drives an ongoing need for climate downscaling. Here we test the extent to which developments in deep learning can out-perform existing statistical approaches for downscaling historical rainfall in the highly complex terrain setting of New Zealand. While deep learning removes the need for manual feature selection when extracting spatial-temporal information from predictor fields, several key considerations need to be addressed. These include: the chosen complexity of the network architecture, suitable loss functions tailored to the problem, as well as input data considerations of domain size and amount of training data required to provide adequate out-of-sample generalization. Sensitivity testing to these considerations reveals that a relatively simple convolutional neural network (CNN) architecture with carefully selected loss functions can considerably outperform existing statistical downscaling models based on multiple linear regression with manual feature selection. When aggregated across the entire region, the fraction of explained variance on wet days increased from 0.35 to 0.52, the root-mean-squared error reduced by over 20% and percentage biases for the 90th percentile of rainfall improved by over 25%. Using interpretable machine learning methods, we demonstrate that the CNN has been capable of self-learning physically plausible relationships between the large-scale atmospheric environment and extreme localized rainfall events. The historical performance and physical interpretability documented here lends support for wider development and application of deep learning in climate downscaling.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1016/j.wace.2022.100525",NA,NA,NA
"rayyan-100677458","Hydrological Drought Forecasting Using Machine Learning—Gidra River Case Study",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Drought is one of many critical problems that could arise as a result of climate change as it has an impact on many aspects of the world, including water resources and water scarcity. In this study, an assessment of hydrological drought in the Gidra River is carried out to characterize dry, normal, and wet hydrological situations by using the Slovak Hydrometeorological Institute (SHMI) methodology. The water bearing coefficient is used as the index of the hydrological drought. As machine and deep learning are increasingly being used in many areas of hydroinformatics, this study is utilized artificial neural networks (ANNs) and support vector machine (SVM) models to predict the hydrological drought in the Gidra River based on daily average discharges in January, February, March, and April of the corresponding year. The study utilized in total 58 years of daily average discharge values containing 35 normal and wet years and 23 dry years. The results of the study show high accuracy of 100% in predicting hydrological drought in the Gidra River. The early classification of the hydrological situation in the Gidra River shows the potential of integrating water management with the deep and machine learning models in terms of irrigation planning and mitigation of drought effects.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.3390/w14030387",NA,NA,NA
"rayyan-100677493","Shared Blocks-Based Ensemble Deep Learning for Shallow Landslide Susceptibility Mapping",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Natural disaster impact assessment is of the utmost significance for post-disaster recovery, environmental protection, and hazard mitigation plans. With their recent usage in landslide susceptibility mapping, deep learning (DL) architectures have proven their efficiency in many scientific studies. However, some restrictions, including insufficient model variance and limited generalization capabilities, have been reported in the literature. To overcome these restrictions, ensembling DL models has often been preferred as a practical solution. In this study, an ensemble DL architecture, based on shared blocks, was proposed to improve the prediction capability of individual DL models. For this purpose, three DL models, namely Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), and Long Short-Term Memory (LSTM), together with their ensemble form (CNN–RNN–LSTM) were utilized to model landslide susceptibility in Trabzon province, Turkey. The proposed DL architecture produced the highest modeling performance of 0.93, followed by CNN (0.92), RNN (0.91), and LSTM (0.86). Findings proved that the proposed model excelled the performance of the DL models by up to 7% in terms of overall accuracy, which was also confirmed by the Wilcoxon signed-rank test. The area under curve analysis also showed a significant improvement (~4%) in susceptibility map accuracy by the proposed strategy.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.3390/rs13234776",NA,NA,NA
"rayyan-100677513","FLOOD RISK MAPPING USING RANDOM FOREST AND SUPPORT VECTOR MACHINE",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract. Floods are among the natural disasters that cause financial and human losses all over the world every year. By production of a flood risk map and determination of potential flood risk areas, the possible damages of this phenomenon can be reduced. To map the flood extend in Calcasieu Parish, Louisiana, US, conditioning factors affecting the flood occurrence including elevation, slope, plan curvature, land use, distance from rivers, density of rivers, rainfall, normalized difference vegetation index (NDVI), modified normalized difference water index (MNDWI), and normalized difference built-up index (NDBI) were identified and their information layers produced using the Google Earth Engine (GEE) cloud platform. Then, for flood risk mapping, Random Forest (RF) and support vector machine (SVM) as two machine learning models have been implemented and their results compared. RF and SVM models have been validated based on the maximum absolute error (MAE) index with an accuracy of 0.043 and 0.097, respectively. Visualization of the predicted values in QGIS software confirms that the RF model has provided better outputs than that of the SVM model. By analysing the features importance of the layers in the RF model, it was verified that the elevation, slope, and plan curvature layers have the highest degree of influence on the flood risk with degrees of importance of 0.197, 0.135, and 0.123.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.5194/isprs-annals-x-4-w1-2022-201-2023",NA,NA,NA
"rayyan-100677514","Tropical Cyclone Detection from the Thermal Infrared Sensor IASI Data Using the Deep Learning Model YOLOv3",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Tropical cyclone (TC) detection is essential to mitigate natural disasters, as TCs can cause significant damage to life, infrastructure and economy. In this study, we applied the deep learning object detection model YOLOv3 to detect TCs in the North Atlantic Basin, using data from the Thermal InfraRed (TIR) Atmospheric Sounding Interferometer (IASI) onboard the Metop satellites. IASI measures the outgoing TIR radiation of the Earth-Atmosphere. For the first time, we provide a proof of concept of the possibility of constructing images required by YOLOv3 from a TIR remote sensor that is not an imager. We constructed a dataset by selecting 50 IASI radiance channels and using them to create images, which we labeled by constructing bounding boxes around TCs using the hurricane database HURDAT2. We trained the YOLOv3 on two settings, first with three “best” selected channels, then using an autoencoder to exploit all 50 channels. We assessed its performance with the Average Precision (AP) metric at two different intersection over union (IoU) thresholds (0.1 and 0.5). The model achieved promising results with AP at IoU threshold 0.1 of 78.31%. Lower performance was achieved with IoU threshold 0.5 (31.05%), showing the model lacks precision regarding the size and position of the predicted boxes. Despite that, we show YOLOv3 demonstrates great potential for TC detection using TIR instruments data.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.3390/atmos14020215",NA,NA,NA
"rayyan-100677515","Towards understanding the environmental and climatic changes and its contribution to the spread of wildfires in Ghana using remote sensing tools and machine learning (Google Earth Engine)",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Data processing and climate characterisation to study its impact is becoming difficult due to insufficient and unavailable data, especially in developing countries. Understanding climate's impact on burnt areas in Ghana (Guinea-savannah (GSZ) and Forest-savannah Mosaic zones (FSZ)) leads us to opt for machine learning. Through Google Earth Engine (GEE), rainfall (PR), maximum temperature (Tmax), minimum temperature (Tmin), average temperature (Tmean), Palmer Drought Severity Index (PDSI), relative humidity (RH), wind speed (WS), soil moisture (SM), actual evapotranspiration (ETA) and reference evapotranspiration (ETR) have been acquired through CHIRPS (Climate Hazards group Infrared Precipitation with Stations), FLDAS dataset (Famine Early Warning Systems Network (FEWS NET) Land Data Assimilation System) and TerraClimate platform from 1991 to 2021. The objective is to analyse the link and the contribution of climatic and environmental parameters on wildfire spread in GSZ and FSZ in Ghana. Variables were analysed (area burnt and the number of active fires) through Spearman correlation and the cross-correlation function (CCF) (2001 to 2021). The tests (Mann-Kendall and Sens's slope trend test, Pettitt test and the Lee and Heghinian test) showed the overall decrease in rainfall and increase in temperature respectively (−0.1 mm; + 0.8°C) in GSZ and (−0.9 mm; + 0.3°C) in FSZ. In terms of impact, PR, ETR, FDI, Tmean, Tmax, Tmin, RH, ETA and SM contribute to fire spread. Through the codes developed, researchers and decision-makers could update them at different times easily to monitor climate variability and its impact on fires.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.1080/17538947.2023.2197263",NA,NA,NA
"rayyan-100677525","Search-and-rescue in the Central Mediterranean Route does not induce migration: Predictive modeling to answer causal queries in migration research",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract State- and private-led search-and-rescue are hypothesized to foster irregular migration (and thereby migrant fatalities) by altering the decision calculus associated with the journey. We here investigate this ‘pull factor’ claim by focusing on the Central Mediterranean route, the most frequented and deadly irregular migration route towards Europe during the past decade. Based on three intervention periods—(1) state-led Mare Nostrum , (2) private-led search-and-rescue, and (3) coordinated pushbacks by the Libyan Coast Guard—which correspond to substantial changes in laws, policies, and practices of search-and-rescue in the Mediterranean, we are able to test the ‘pull factor’ claim by employing an innovative machine learning method in combination with causal inference. We employ a Bayesian structural time-series model to estimate the effects of these three intervention periods on the migration flow as measured by crossing attempts (i.e., time-series aggregate counts of arrivals, pushbacks, and deaths), adjusting for various known drivers of irregular migration. We combine multiple sources of traditional and non-traditional data to build a synthetic, predicted counterfactual flow. Results show that our predictive modeling approach accurately captures the behavior of the target time-series during the various pre-intervention periods of interest. A comparison of the observed and predicted counterfactual time-series in the post-intervention periods suggest that pushback policies did affect the migration flow, but that the search-and-rescue periods did not yield a discernible difference between the observed and the predicted counterfactual number of crossing attempts. Hence we do not find support for search-and-rescue as a driver of irregular migration. In general, this modeling approach lends itself to forecasting migration flows with the goal of answering causal queries in migration research.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.1038/s41598-023-38119-4",NA,NA,NA
"rayyan-100677547","Using Machine Learning to Cut the Cost of Dynamical Downscaling",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract Global climate models (GCMs) are commonly downscaled to understand future local climate change. The high computational cost of regional climate models (RCMs) limits how many GCMs can be dynamically downscaled, restricting uncertainty assessment. While statistical downscaling is cheaper, its validity in a changing climate is unclear. We combine these approaches to build an emulator leveraging the merits of dynamical and statistical downscaling. A machine learning model is developed for each coarse grid cell to predict fine grid variables, using coarse‐scale climate predictors with fine grid land characteristics. Two RCM emulators, one Multilayer Perceptron (MLP) and one Multiple Linear Regression error‐reduced with Random Forest (MLR‐RF), are developed to downscale daily evapotranspiration from 12.5 km (coarse‐scale) to 1.5 km (fine‐scale). Out‐of‐sample tests for the MLP and MLR‐RF achieve Kling‐Gupta‐Efficiency of 0.86 and 0.83, correlation of 0.89 and 0.86, and coefficient of determination ( R 2 ) of 0.78 and 0.75, with a relative bias of −6% to 5% and −5% to 4%, respectively. Using histogram match for spatial efficiency, both emulators achieve a median score of ∼0.77. This is generally better than a common statistical downscaling method in a range of metrics. Additionally, through “spatial transitivity,” we can downscale GCMs for new regions at negligible cost and only minor performance loss. The framework offers a cheap and quick way to downscale large ensembles of GCMs. This could enable high‐resolution climate projections from a larger number of global models, enabling uncertainty quantification, and so better support for resilience and adaptation planning.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1029/2022ef003291",NA,NA,NA
"rayyan-100677554","Is artificial intelligence greening global supply chains? Exposing the political economy of environmental costs",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene | USER-NOTES: {""Maria""=>[""Artificial intelligence (AI) is set to greatly enhance the productivity and efficiency of global supply chains over the next decade. Transnational corporations are hailing these gains as a ‘game changer’ for advancing environmental sustainability. Yet, looking through a political economy lens, it is clear that AI is not advancing sustainability nearly as much as industry leaders are claiming. As this article argues, the metrics and rhetoric of corporate social responsibility are exaggerating the benefits and obscuring the costs of AI. Productivity and efficiency gains in the middle sections of supply chains are rebounding into more production and consumption, doing far more to enhance the profitability of big business than the sustainability of the earth. At the same time, AI is accelerating natural resource extraction and the distancing of waste, casting dark shadows of harm across marginalized communities, fragile ecosystems, and future generations."", ""The micro-level gains from AI, as this article exposes, are not going to add up to macro-level solutions for the negative environmental consequences of global supply chains, while portraying AI as a force of sustainability is legitimizing business as usual, reinforcing a narrative of corporate responsibility, obfuscating the need for greater state regulation, and empowering transnational corporations as global governors. These findings extend the theoretical understanding in the field of international political economy of the hidden dangers of relying on technology and corporate governance to resolve the deep unsustainability of the contemporary world order.""]}","https://doi.org/10.1080/09692290.2020.1814381",NA,NA,NA
"rayyan-100677560","Deep learning models for forecasting dengue fever based on climate data in Vietnam",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Background Dengue fever (DF) represents a significant health burden in Vietnam, which is forecast to worsen under climate change. The development of an early-warning system for DF has been selected as a prioritised health adaptation measure to climate change in Vietnam. Objective This study aimed to develop an accurate DF prediction model in Vietnam using a wide range of meteorological factors as inputs to inform public health responses for outbreak prevention in the context of future climate change. Methods Convolutional neural network (CNN), Transformer, long short-term memory (LSTM), and attention-enhanced LSTM (LSTM-ATT) models were compared with traditional machine learning models on weather-based DF forecasting. Models were developed using lagged DF incidence and meteorological variables (measures of temperature, humidity, rainfall, evaporation, and sunshine hours) as inputs for 20 provinces throughout Vietnam. Data from 1997–2013 were used to train models, which were then evaluated using data from 2014–2016 by Root Mean Square Error (RMSE) and Mean Absolute Error (MAE). Results and discussion LSTM-ATT displayed the highest performance, scoring average places of 1.60 for RMSE-based ranking and 1.95 for MAE-based ranking. Notably, it was able to forecast DF incidence better than LSTM in 13 or 14 out of 20 provinces for MAE or RMSE, respectively. Moreover, LSTM-ATT was able to accurately predict DF incidence and outbreak months up to 3 months ahead, though performance dropped slightly compared to short-term forecasts. To the best of our knowledge, this is the first time deep learning methods have been employed for the prediction of both long- and short-term DF incidence and outbreaks in Vietnam using unique, rich meteorological features. Conclusion This study demonstrates the usefulness of deep learning models for meteorological factor-based DF forecasting. LSTM-ATT should be further explored for mitigation strategies against DF and other climate-sensitive diseases in the coming years.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.1371/journal.pntd.0010509",NA,NA,NA
"rayyan-100677563","Towards a global understanding of the drivers of marine and terrestrial biodiversity",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Understanding the distribution of life's variety has driven naturalists and scientists for centuries, yet this has been constrained both by the available data and the models needed for their analysis. Here we compiled data for over 67,000 marine and terrestrial species and used artificial neural networks to model species richness with the state and variability of climate, productivity, and multiple other environmental variables. We find terrestrial diversity is better predicted by the available environmental drivers than is marine diversity, and that marine diversity can be predicted with a smaller set of variables. Ecological mechanisms such as geographic isolation and structural complexity appear to explain model residuals and also identify regions and processes that deserve further attention at the global scale. Improving estimates of the relationships between the patterns of global biodiversity, and the environmental mechanisms that support them, should help in efforts to mitigate the impacts of climate change and provide guidance for adapting to life in the Anthropocene.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.1371/journal.pone.0228065",NA,NA,NA
"rayyan-100677571","Landslide Susceptibility Mapping Using Machine Learning: A Literature Survey",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Landslide is a devastating natural disaster, causing loss of life and property. It is likely to occur more frequently due to increasing urbanization, deforestation, and climate change. Landslide susceptibility mapping is vital to safeguard life and property. This article surveys machine learning (ML) models used for landslide susceptibility mapping to understand the current trend by analyzing published articles based on the ML models, landslide causative factors (LCFs), study location, datasets, evaluation methods, and model performance. Existing literature considered in this comprehensive survey is systematically selected using the ROSES protocol. The trend indicates a growing interest in the field. The choice of LCFs depends on data availability and case study location; China is the most studied location, and area under the receiver operating characteristic curve (AUC) is considered the best evaluation metric. Many ML models have achieved an AUC value &gt; 0.90, indicating high reliability of the susceptibility map generated. This paper also discusses the recently developed hybrid, ensemble, and deep learning (DL) models in landslide susceptibility mapping. Generally, hybrid, ensemble, and DL models outperform conventional ML models. Based on the survey, a few recommendations and future works which may help the new researchers in the field are also presented.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.3390/rs14133029",NA,NA,NA
"rayyan-100677577","Wildfire and Smoke Detection Using Staged YOLO Model and Ensemble CNN",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"One of the most expensive and fatal natural disasters in the world is forest fires. For this reason, early discovery of forest fires helps minimize mortality and harm to ecosystems and forest life. The present research enriches the body of knowledge by evaluating the effectiveness of an efficient wildfire and smoke detection solution implementing ensembles of multiple convolutional neural network architectures tackling two different computer vision tasks in a stage format. The proposed architecture combines the YOLO architecture with two weights with a voting ensemble CNN architecture. The pipeline works in two stages. If the CNN detects the existence of abnormality in the frame, then the YOLO architecture localizes the smoke or fire. The addressed tasks are classification and detection in the presented method. The obtained model’s weights achieve very decent results during training and testing. The classification model achieves a 0.95 F1-score, 0.99 accuracy, and 0.98e sensitivity. The model uses a transfer learning strategy for the classification task. The evaluation of the detector model reveals strong results by achieving a 0.85 mean average precision with 0.5 threshold (mAP@0.5) score for the smoke detection model and 0.76 mAP for the combined model. The smoke detection model also achieves a 0.93 F1-score. Overall, the presented deep learning pipeline shows some important experimental results with potential implementation capabilities despite some issues encountered during training, such as the lack of good-quality real-world unmanned aerial vehicle (UAV)-captured fire and smoke images.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.3390/electronics12010228",NA,NA,NA
"rayyan-100677590","On the suitability of deep convolutional neural networks for continental-wide downscaling of climate change projections",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract In a recent paper, Baño-Medina et al. (Configuration and Intercomparison of deep learning neural models for statistical downscaling. preprint, 2019) assessed the suitability of deep convolutional neural networks (CNNs) for downscaling of temperature and precipitation over Europe using large-scale ‘perfect’ reanalysis predictors. They compared the results provided by CNNs with those obtained from a set of standard methods which have been traditionally used for downscaling purposes (linear and generalized linear models), concluding that CNNs are well suited for continental-wide applications. That analysis is extended here by assessing the suitability of CNNs for downscaling future climate change projections using Global Climate Model (GCM) outputs as predictors. This is particularly relevant for this type of “black-box” models, whose results cannot be easily explained based on physical reasons and could potentially lead to implausible downscaled projections due to uncontrolled extrapolation artifacts. Based on this premise, we analyze in this work the two key assumptions that are made in perfect prognosis downscaling: (1) the predictors chosen to build the statistical model should be well reproduced by GCMs and (2) the statistical model should be able to reliably extrapolate out of sample (climate change) conditions. As a first step to test the suitability of these models, the latter assumption is assessed here by analyzing how the CNNs affect the raw GCM climate change signal (defined as the difference, or delta, between future and historical climate). Our results show that, as compared to well-established generalized linear models (GLMs), CNNs yield smaller departures from the raw GCM outputs for the end of century, resulting in more plausible downscaling results for climate change applications. Moreover, as a consequence of the automatic treatment of spatial features, CNNs are also found to provide more spatially homogeneous downscaled patterns than GLMs.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1007/s00382-021-05847-0",NA,NA,NA
"rayyan-100677591","Systematic mapping of global research on climate and health: a machine learning review",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"BackgroundThe global literature on the links between climate change and human health is large, increasing exponentially, and it is no longer feasible to collate and synthesise using traditional systematic evidence mapping approaches. We aimed to use machine learning methods to systematically synthesise an evidence base on climate change and human health.MethodsWe used supervised machine learning and other natural language processing methods (topic modelling and geoparsing) to systematically identify and map the scientific literature on climate change and health published between Jan 1, 2013, and April 9, 2020. Only literature indexed in English were included. We searched Web of Science Core Collection, Scopus, and PubMed using title, abstract, and keywords only. We searched for papers including both a health component and an explicit mention of either climate change, climate variability, or climate change-relevant weather phenomena. We classified relevant publications according to the fields of climate research, climate drivers, health impact, date, and geography. We used supervised and unsupervised machine learning to identify and classify relevant articles in the field of climate and health, with outputs including evidence heat maps, geographical maps, and narrative synthesis of trends in climate health-related publications. We included empirical literature of any study design that reported on health pathways associated with climate impacts, mitigation, or adaptation.FindingsWe predict that there are 15 963 studies in the field of climate and health published between 2013 and 2019. Climate health literature is dominated by impact studies, with mitigation and adaptation responses and their co-benefits and co-risks remaining niche topics. Air quality and heat stress are the most frequently studied exposures, with all-cause mortality and infectious disease incidence being the most frequently studied health outcomes. Seasonality, extreme weather events, heat, and weather variability are the most frequently studied climate-related hazards. We found major gaps in evidence on climate health research for mental health, undernutrition, and maternal and child health. Geographically, the evidence base is dominated by studies from high-income countries and China, with scant evidence from low-income counties, which often suffer most from the health consequences of climate change.InterpretationOur findings show the importance and feasibility of using automated machine learning to comprehensively map the science on climate change and human health in the age of big literature. These can provide key inputs into global climate and health assessments. The scant evidence on climate change response options is concerning and could significantly hamper the design of evidence-based pathways to reduce the effects on health of climate change. In the post-2015 Paris Agreement era of climate solutions, we believe much more attention should be given to climate adaptation and mitigation options and their effects on human health.FundingForeign, Commonwealth & Development Office.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.1016/s2542-5196(21)00179-0",NA,NA,NA
"rayyan-100677599","Deep Learning for Downscaling Tropical Cyclone Rainfall to Hazard‐Relevant Spatial Scales",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract Flooding, driven in part by intense rainfall, is the leading cause of mortality and damages from the most intense tropical cyclones (TCs). With rainfall from TCs set to increase under anthropogenic climate change, it is critical to accurately estimate extreme rainfall to better support short‐term and long‐term resilience efforts. While high‐resolution climate models capture TC statistics better than low‐resolution models, they are computationally expensive. This leads to a trade‐off between capturing TC features accurately, and generating large enough simulation data sets to sufficiently sample high‐impact, low‐probability events. Downscaling can assist by predicting high‐resolution features from relatively cheap, low‐resolution models. Here, we develop and evaluate a set of three deep learning models for downscaling TC rainfall to hazard‐relevant spatial scales. We use rainfall from the Multi‐Source Weighted‐Ensemble Precipitation observational product at a coarsened resolution of ∼100 km, and apply our downscaling model to reproduce the original resolution of ∼10 km. We find that the Wasserstein Generative Adversarial Network is able to capture realistic spatial structures and power spectra and performs the best overall, with mean biases within 5% of observations. We also show that the model can perform well at extrapolating to the most extreme storms, which were not used in training.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1029/2022jd038163",NA,NA,NA
"rayyan-100677600","Observation‐Constrained Projection of Flood Risks and Socioeconomic Exposure in China",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract As the planet warms, the atmosphere's water vapor holding capacity rises, leading to more intense precipitation extremes. River floods with high peak discharge or long duration can increase the likelihood of infrastructure failure and enhance ecosystem vulnerability. However, changes in the peak and duration of floods and corresponding socioeconomic exposure under climate change are still poorly understood. This study employs a bivariate framework to quantify changes in flood risks and their socioeconomic impacts in China between the past (1985–2014) and future (2071–2100) in 204 catchments. Future daily river streamflow is projected by using a cascade modeling chain based on the outputs of five bias‐corrected global climate models (GCMs) under three shared socioeconomic CMIP6 pathways (SSP1‐26, SSP3‐70, and SSP5‐85), a machine learning model and four hydrological models. We also utilize the copula function to build the joint distribution of flood peak and duration, and calculate the joint return periods of the bivariate flood hazard. Finally, the exposure of population and regional gross domestic product to floods are investigated at the national scale. Our results indicate that flood peak and duration are likely to increase in the majority of catchments by 25%–100% by the late 21st century depending on the shared socioeconomic pathway. China is projected to experience a significant increase in bivariate flood risks even under the lowest emission pathway, with 24.0 million dollars/km 2 and 608 people/km 2 exposed under a moderate emissions scenario (SSP3‐70). These findings have direct implications for hazard mitigation and climate adaptation policies in China.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1029/2022ef003308",NA,NA,NA
"rayyan-100677601","Land Data Assimilation: Harmonizing Theory and Data in Land Surface Process Studies",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract Data assimilation plays a dual role in advancing the “scientific” understanding and serving as an “engineering tool” for the Earth system sciences. Land data assimilation (LDA) has evolved into a distinct discipline within geophysics, facilitating the harmonization of theory and data and allowing land models and observations to complement and constrain each other. Over recent decades, substantial progress has been made in the theory, methodology, and application of LDA, necessitating a holistic and in‐depth exploration of its full spectrum. Here, we present a thorough review elucidating the theoretical and methodological developments in LDA and its distinctive features. This encompasses breakthroughs in addressing strong nonlinearities in land surface processes, exploring the potential of machine learning approaches in data assimilation, quantifying uncertainties arising from multiscale spatial correlation, and simultaneously estimating model states and parameters. LDA has proven successful in enhancing the understanding and prediction of various land surface processes (including soil moisture, snow, evapotranspiration, streamflow, groundwater, irrigation and land surface temperature), particularly within the realms of water and energy cycles. This review outlines the development of global, regional, and catchment‐scale LDA systems and software platforms, proposing grand challenges of generating land reanalysis and advancing coupled land‒atmosphere DA. We lastly highlight the opportunities to expand the applications of LDA from pure geophysical systems to coupled natural and human systems by ingesting a deluge of Earth observation and social sensing data. The paper synthesizes current LDA knowledge and provides a steppingstone for its future development, particularly in promoting dual driven theory‐data land processes studies.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1029/2022rg000801",NA,NA,NA
"rayyan-100677625","Monitoring vegetation sensitivity to drought events in China",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate | USER-NOTES: {""Maria""=>[""The frequency and severity of drought events have increased over the decades under the influence of global warming. Continued drought increases the risk of vegetation degradation. Many studies have investigated the responses of vegetation to drought but rarely from the perspective of drought events. Moreover, the spatial distributions of vegetation sensitivity to drought events are not well understood in China. Thus, the spatiotemporal patterns of drought events were quantified based on the run theory at different time-scales in this study. The relative importance of drought characteristics for vegetation anomalies during drought events were calculated by using the BRT model. Then, the sensitivity of vegetation anomalies and vegetation phenology was quantified by dividing standardized anomalies of vegetation parameters (NDVI and phenological metrics) and SPEI during drought events for different regions in China. "", ""The results show that Southern Xinjiang and Southeast China experienced relatively higher values of drought severity, especially at the 3-month and 6-month scales. Most arid areas experienced more drought events but of low severity, while some humid zones underwent few drought events but of high severity. Notable negative NDVI anomalies appeared in the Northeast China and Southwest China, while positive NDVI anomalies were observed in Southeast China and Northern central region. Drought interval, intensity and severity contributed approximately 80 % of the model's explained vegetation variance in most regions. The sensitivity of vegetation anomalies to drought events (VASD) varied regionally in China. The Qinghai-Tibet Plateau and Northeast China tended to exhibit higher sensitivity to drought events. "", ""Vegetation in these regions with high sensitivity faced a high risk of degradation and could function as warning signals of vegetation degradation. Drought events at high timescales had a greater impact on vegetation sensitivity in dry zones, while they had a smaller impact on humid areas. With the increase in drought degree of climate zones and the decrease in vegetation coverage, VASD showed a gradual increase. Furthermore, a strong negative correlation between VASD and the aridity index (AI) was observed in all vegetation types. The change in VASD for sparse vegetation was the largest with the change in AI. For vegetation phenology, drought events in most regions delayed the end of the growing season and extended the length of growing season, especially for sparse vegetation. "", ""The start of the growing season was advanced in most humid areas, while being delayed in most dry areas during drought events. Knowledge of vegetation sensitivity to drought events will be beneficial to provide decision-making references for the prevention and control of vegetation degradation, especially in the ecological fragile regions.""]}","https://doi.org/10.1016/j.scitotenv.2023.164917",NA,NA,NA
"rayyan-100677650","Successful implementations of a real-time and intelligent early warning system for loess landslides on the Heifangtai terrace, China",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate | USER-NOTES: {""Maria""=>[""Real-time monitoring and intelligent early warning system are crucial and significant to take mitigation measures and reduce casualties and property losses related to landslides. It is difficult to obtain entire monitoring data in the accelerated deformation phase in a landslide event, and hard to issue early warning information using a traditional monitoring approach with fixed and low sampling frequency. Displacement increments of loess landslides induced by agriculture irrigation on the Heifangtai terrace could be sudden and extremely rapid. Typical landslide types include loess flowslides and loess falls. It is of practical significance to develop a self-adaptive data acquisition monitoring technique and establish a real-time landslide early warning system (LEWS) to meet the needs for risk mitigation of rapid sliding slopes on the Heifangtai terrace."", ""The monitoring technique can wirelessly transmit displacement data and the LEWS was devised using the new artificial intelligence. The LEWS could automatically release the warning information in advance of the event once the early warning parameters exceed default thresholds. In this study, the early warning procedures, real-time monitoring approach, intelligent LEWS, a multiple criteria warning model, warning release and emergency mitigation measures, and performance are introduced in detail. Six loess landslides at Heifangtai and eight landslides in other regions of China have been successfully warned since its implementation in 2012. "", ""This study proposed an effective and practical solution for the early warning of loess landslides at Heifangtai. Two typical loess landslides that had successful early warnings at Heifangtai were presented. The successful implementation could serve as a reference for global rapid slope failure cases, considering the complex nature of landslide behaviors and failure mechanisms.""]}","https://doi.org/10.1016/j.enggeo.2020.105817",NA,NA,NA
"rayyan-100677658","Machine-learning modelling of fire susceptibility in a forest-agriculture mosaic landscape of southern India",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate | USER-NOTES: {""Maria""=>[""The recurrent forest fires have been a serious management concern in southern Western Ghats, India. This study investigates the applicability of various geospatial data, machine learning techniques (MLTs) and spatial statistical tools to demarcate the forest fire susceptible regions of the forested landscape of the Wayanad district in the southern Western Ghats (Kerala, India). The inventory map of 279 forest fire locations (period = 2001–2018) was developed via Sentinel 2A satellite images, NASA fire archives, and field visits. The forest fire susceptibility modelling involves twelve influencing factors, such as ambient air temperature, wind speed, rainfall, relative humidity, atmospheric water vapor pressure (WVP), elevation, slope angle, topographical wetness index (TWI), slope aspect, land use/land cover (LU/LC), distance from the road and distance from the villages. "", ""Considering the varying level of performances (i.e., receiver operating characteristics-area under curve (ROC-AUC) values ranging from 0.869 to 0.924 in the testing phase) of the MLTs, viz., artificial neural network (ANN), generalized linear model (GLM), multivariate adaptive regression splines (MARS), Naïve Bayesian classifier (NBC), K-nearest neighbour (KNN), support vector machine (SVM), random forest (RF), gradient boosting machine (GBM), adaptive boosting (AdaBoost) and maximum entropy (MaxEnt), we propose a weighted approach to characterize the forest fire susceptibility of the region using the outputs of the different MLTs. The proposed method demonstrates improvement in accuracy (AUC = 0.890) for mapping the forest fire susceptibility of the region compared to the individual MLTs (AUC = 0.715 to 0.869) while validating with the recent forest fire data (i.e., 2019–2021). "", ""This study suggests that roughly one-third of the study area is highly susceptible to the occurrence of forest fires, implying the severity of the disturbance regime. The analysis also indicates the role of anthropogenic factors in the occurrence of forest fires in the region. It is expected that the demarcation and prioritization of the forest fire susceptibility zones in the region, which is a part of one of the global biodiversity hotspots, have significant implications on biodiversity conservation at a regional scale.""]}","https://doi.org/10.1016/j.ecoinf.2021.101348",NA,NA,NA
"rayyan-100677659","A review of drought monitoring with big data: Issues, methods, challenges and research directions",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Over recent years, the frequency and intensity of droughts have increased and there has been a large drying trend over many parts of the world. Consequently, drought monitoring using big data analytic has gained an explosive interest. Droughts stand among the most damaging natural disasters. It threatens agricultural production, ecological environment, and socio-economic development. For this reason, early warning, accurate evaluation, and efficient prediction are an emergency especially for the nations that are the most menaced by this danger. There are numerous emerging studies addressing big data and its applications in drought monitoring. In fact, big data handle data heterogeneity which is an additive value for the prediction of drought, it offers a view of the different dimensions such as the spatial distribution, the temporal distribution and the severity detection of this phenomenon. Big data analytic and drought are introduced and reviewed in this paper. Besides, this review includes different studies, researches and applications of big data to drought monitoring. Challenges related to data life cycle such as data challenges, data processing challenges and data infrastructure management challenges are also discussed. Finally, we conclude that big data analytic can be beneficial in drought monitoring but there is a need for statistical and artificial intelligence-based approaches.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1016/j.ecoinf.2020.101136",NA,NA,NA
"rayyan-100677666","Active Fire Detection from Landsat-8 Imagery Using Deep Multiple Kernel Learning",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Active fires are devastating natural disasters that cause socio-economical damage across the globe. The detection and mapping of these disasters require efficient tools, scientific methods, and reliable observations. Satellite images have been widely used for active fire detection (AFD) during the past years due to their nearly global coverage. However, accurate AFD and mapping in satellite imagery is still a challenging task in the remote sensing community, which mainly uses traditional methods. Deep learning (DL) methods have recently yielded outstanding results in remote sensing applications. Nevertheless, less attention has been given to them for AFD in satellite imagery. This study presented a deep convolutional neural network (CNN) “MultiScale-Net” for AFD in Landsat-8 datasets at the pixel level. The proposed network had two main characteristics: (1) several convolution kernels with multiple sizes, and (2) dilated convolution layers (DCLs) with various dilation rates. Moreover, this paper suggested an innovative Active Fire Index (AFI) for AFD. AFI was added to the network inputs consisting of the SWIR2, SWIR1, and Blue bands to improve the performance of the MultiScale-Net. In an ablation analysis, three different scenarios were designed for multi-size kernels, dilation rates, and input variables individually, resulting in 27 distinct models. The quantitative results indicated that the model with AFI-SWIR2-SWIR1-Blue as the input variables, using multiple kernels of sizes 3 × 3, 5 × 5, and 7 × 7 simultaneously, and a dilation rate of 2, achieved the highest F1-score and IoU of 91.62% and 84.54%, respectively. Stacking AFI with the three Landsat-8 bands led to fewer false negative (FN) pixels. Furthermore, our qualitative assessment revealed that these models could detect single fire pixels detached from the large fire zones by taking advantage of multi-size kernels. Overall, the MultiScale-Net met expectations in detecting fires of varying sizes and shapes over challenging test samples.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.3390/rs14040992",NA,NA,NA
"rayyan-100677669","A neural network surrogate model for the performance assessment of a vertical structure subjected to non-stationary, tornadic wind loads",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Despite significant advancements in computational technologies and methods, the comprehensive assessment of the performance capacities and risk of structures built in environments prone to severe natural hazards is still a daunting task under standard Monte Carlo-based simulation schemes. This issue is particularly relevant for the consideration of wind actions from loads generated by non-stationary phenomena (e.g. tornadoes) because of extreme complexities in the simulated flow field and the fluid-structure interaction. To mitigate such computational burdens, this study proposes a surrogate modeling approach that utilizes predicted fragilities from artificial neural networks (ANNs) to facilitate the performance-based assessment of a vertical structure subjected to tornadic wind loads. Calibration data for the feedforward ANNs are extracted from numerically generated responses based on a derived wind loading model that capitalizes on the developments of various analytical formulations of a tornado’s wind field. Uncertainties in the structural behavior and in the overall modeling procedure are incorporated in the process, culminating in a life-cycle cost assessment that incorporates a practical, economic value to the simulation framework. The novel application of ANNs in this study, therefore, empowers a more robust performance-based framework for the risk evaluation of structures subjected to tornado wind loads.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1016/j.compstruc.2020.106208",NA,NA,NA
"rayyan-100677671","Monitoring meteorological drought in a semiarid region using two long-term satellite-estimated rainfall datasets: A case study of the Piranhas River basin, northeastern Brazil",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate | USER-NOTES: {""Maria""=>[""Extreme weather events have frequently caused serious damage to the quality of life of the population and the economy of the Brazilian semiarid region, where droughts are the main natural disaster. Therefore, a robust and skilled data network is needed for precise monitoring of droughts, and satellite precipitation products have stood out as useful alternative methods for providing estimated precipitation data. Thus, the objective of this study was to evaluate the efficiency of satellite-estimated long-term precipitation for monitoring meteorological drought in a semiarid region. This study was carried out in the Piranhas River basin, northeastern Brazil, using the following data (1994–2017): (a) observations at 38 rain gauges, (b) the Precipitation Estimation from Remotely Sensed Information using Artificial Neural Network – Climate Data Record (PERSIANN-CDR), and (c) Climate Hazards Group InfraRed Precipitation with Station (CHIRPS) data."", ""To assess the short-, medium- and long-term meteorological droughts, standardized precipitation indices (SPI-6, SPI-12 and SPI-24) were used in semiannual, annual and biannual time scale analyses. The results showed that the CHIRPS and PERSIANN-CDR data presented acceptable performance in the identification of meteorological droughts in the study area. The results also showed that the time scales of the SPI-6, SPI-12 and SPI-24 datasets were adequate for identifying the main drought events that have affected the Piranhas River basin in recent years. The PERSIANN-CDR data performed better than the CHIRPS data, although the two datasets described the occurrence of droughts in the basin well. In summary, the study showed that CHIRPS and PERSIANN-CDR are valuable complements to rain gauge-measured rainfall data and that these datasets could be additional sources for hydrometeorological applications in the Piranhas River basin.""]}","https://doi.org/10.1016/j.atmosres.2020.105380",NA,NA,NA
"rayyan-100677672","Automated River Plastic Monitoring Using Deep Learning and Cameras",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene | USER-NOTES: {""Maria""=>[""Quantifying plastic pollution on surface water is essential to understand and mitigate the impact of plastic pollution to the environment. Current monitoring methods such as visual counting are labor intensive. This limits the feasibility of scaling to long-term monitoring at multiple locations. We present an automated method for monitoring plastic pollution that overcomes this limitation. Floating macroplastics are detected from images of the water surface using deep learning. We perform an experimental evaluation of our method using images from bridge-mounted cameras at five different river locations across Jakarta, Indonesia. The four main results of the experimental evaluation are as follows. First, we realize a method that obtains a reliable estimate of plastic density (68.7% precision). "", ""Our monitoring method successfully distinguishes plastics from environmental elements, such as water surface reflection and organic waste. Second, when trained on one location, the method generalizes well to new locations with relatively similar conditions without retraining (≈50% average precision). Third, generalization to new locations with considerably different conditions can be boosted by retraining on only 50 objects of the new location (improving precision from ≈20% to ≈42%). "", ""Fourth, our method matches visual counting methods and detects ≈35% more plastics, even more so during periods of plastic transport rates of above 10 items per meter per minute. Taken together, these results demonstrate that our method is a promising way of monitoring plastic pollution. By extending the variety of the data set the monitoring method can be readily applied at a larger scale.""]}","https://doi.org/10.1029/2019ea000960",NA,NA,NA
"rayyan-100677676","Machine learning methods trained on simple models can predict critical transitions in complex natural systems",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Forecasting sudden changes in complex systems is a critical but challenging task, with previously developed methods varying widely in their reliability. Here we develop a novel detection method, using simple theoretical models to train a deep neural network to detect critical transitions—the Early Warning Signal Network (EWSNet). We then demonstrate that this network, trained on simulated data, can reliably predict observed real-world transitions in systems ranging from rapid climatic change to the collapse of ecological populations. Importantly, our model appears to capture latent properties in time series missed by previous warning signals approaches, allowing us to not only detect if a transition is approaching, but critically whether the collapse will be catastrophic or non-catastrophic. These novel properties mean EWSNet has the potential to serve as an indicator of transitions across a broad spectrum of complex systems, without requiring information on the structure of the system being monitored. Our work highlights the practicality of deep learning for addressing further questions pertaining to ecosystem collapse and has much broader management implications.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1098/rsos.211475",NA,NA,NA
"rayyan-100677677","BDANet: Multiscale Convolutional Neural Network With Cross-Directional Attention for Building Damage Assessment From Satellite Images",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Fast and effective responses are required when a natural disaster (e.g., earthquake, hurricane, etc.) strikes. Building damage assessment from satellite imagery is critical before relief effort is deployed. With a pair of pre- and post-disaster satellite images, building damage assessment aims at predicting the extent of damage to buildings. With the powerful ability of feature representation, deep neural networks have been successfully applied to building damage assessment. Most existing works simply concatenate pre- and post-disaster images as input of a deep neural network without considering their correlations. In this paper, we propose a novel two-stage convolutional neural network for Building Damage Assessment, called BDANet. In the first stage, a U-Net is used to extract the locations of buildings. Then the network weights from the first stage are shared in the second stage for building damage assessment. In the second stage, a two-branch multi-scale U-Net is employed as backbone, where pre- and post-disaster images are fed into the network separately. A cross-directional attention module is proposed to explore the correlations between pre- and post-disaster images. Moreover, CutMix data augmentation is exploited to tackle the challenge of difficult classes. The proposed method achieves state-of-the-art performance on a large-scale dataset -- xBD. The code is available at https://github.com/ShaneShen/BDANet-Building-Damage-Assessment.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1109/tgrs.2021.3080580",NA,NA,NA
"rayyan-100677687","Machine Learning Methods for Postprocessing Ensemble Forecasts of Wind Gusts: A Systematic Comparison",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Postprocessing ensemble weather predictions to correct systematic errors has become a standard practice in research and operations. However, only few recent studies have focused on ensemble postprocessing of wind gust forecasts, despite its importance for severe weather warnings. Here, we provide a comprehensive review and systematic comparison of eight statistical and machine learning methods for probabilistic wind gust forecasting via ensemble postprocessing, that can be divided in three groups: State of the art postprocessing techniques from statistics (ensemble model output statistics (EMOS), member-by-member postprocessing, isotonic distributional regression), established machine learning methods (gradient-boosting extended EMOS, quantile regression forests) and neural network-based approaches (distributional regression network, Bernstein quantile network, histogram estimation network). The methods are systematically compared using six years of data from a high-resolution, convection-permitting ensemble prediction system that was run operationally at the German weather service, and hourly observations at 175 surface weather stations in Germany. While all postprocessing methods yield calibrated forecasts and are able to correct the systematic errors of the raw ensemble predictions, incorporating information from additional meteorological predictor variables beyond wind gusts leads to significant improvements in forecast skill. In particular, we propose a flexible framework of locally adaptive neural networks with different probabilistic forecast types as output, which not only significantly outperform all benchmark postprocessing methods but also learn physically consistent relations associated with the diurnal cycle, especially the evening transition of the planetary boundary layer.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1175/mwr-d-21-0150.1",NA,NA,NA
"rayyan-100677691","ClimaX: A foundation model for weather and climate",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Most state-of-the-art approaches for weather and climate modeling are based on physics-informed numerical models of the atmosphere. These approaches aim to model the non-linear dynamics and complex interactions between multiple variables, which are challenging to approximate. Additionally, many such numerical models are computationally intensive, especially when modeling the atmospheric phenomenon at a fine-grained spatial and temporal resolution. Recent data-driven approaches based on machine learning instead aim to directly solve a downstream forecasting or projection task by learning a data-driven functional mapping using deep neural networks. However, these networks are trained using curated and homogeneous climate datasets for specific spatiotemporal tasks, and thus lack the generality of numerical models. We develop and demonstrate ClimaX, a flexible and generalizable deep learning model for weather and climate science that can be trained using heterogeneous datasets spanning different variables, spatio-temporal coverage, and physical groundings. ClimaX extends the Transformer architecture with novel encoding and aggregation blocks that allow effective use of available compute while maintaining general utility. ClimaX is pre-trained with a self-supervised learning objective on climate datasets derived from CMIP6. The pre-trained ClimaX can then be fine-tuned to address a breadth of climate and weather tasks, including those that involve atmospheric variables and spatio-temporal scales unseen during pretraining. Compared to existing data-driven baselines, we show that this generality in ClimaX results in superior performance on benchmarks for weather forecasting and climate projections, even when pretrained at lower resolutions and compute budgets. The source code is available at https://github.com/microsoft/ClimaX.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.48550/arxiv.2301.10343",NA,NA,NA
"rayyan-100677693","Transfer learning: improving neural network based prediction of earthquake ground shaking for an area with insufficient training data",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"SUMMARY In a recent study, we showed that convolutional neural networks (CNNs) applied to network seismic traces can be used for rapid prediction of earthquake peak ground motion intensity measures (IMs) at distant stations using only recordings from stations near the epicentre. The predictions are made without any previous knowledge concerning the earthquake location and magnitude. This approach differs significantly from the standard procedure adopted by earthquake early warning systems that rely on location and magnitude information. In the previous study, we used 10 s, raw, multistation (39 stations) waveforms for the 2016 earthquake sequence in central Italy for 915 M ≥ 3.0 events (CI data set). The CI data set has a large number of spatially concentrated earthquakes and a dense network of stations. In this work, we applied the same CNN model to an area of central western Italy. In our initial application of the technique, we used a data set consisting of 266 M ≥ 3.0 earthquakes recorded by 39 stations. We found that the CNN model trained using this smaller-sized data set performed worse compared to the results presented in the previously published study. To counter the lack of data, we explored the adoption of ‘transfer learning’ (TL) methodologies using two approaches: first, by using a pre-trained model built on the CI data set and, next, by using a pre-trained model built on a different (seismological) problem that has a larger data set available for training. We show that the use of TL improves the results in terms of outliers, bias and variability of the residuals between predicted and true IM values. We also demonstrate that adding knowledge of station relative positions as an additional layer in the neural network improves the results. The improvements achieved through the experiments were demonstrated by the reduction of the number of outliers by 5 per cent, the residuals R median by 39 per cent and their standard deviation by 11 per cent.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1093/gji/ggab488",NA,NA,NA
"rayyan-100677699","Building a Resilient, Sustainable, and Healthier Food Supply Through Innovation and Technology",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"The modern food supply faces many challenges. The global population continues to grow and people are becoming wealthier, so the food production system must respond by creating enough high-quality food to feed everyone with minimal damage to our environment. The number of people suffering or dying from diet-related chronic diseases, such as obesity, diabetes, heart disease, stroke, and cancer, continues to rise, which is partly linked to overconsumption of highly processed foods, especially high-calorie or rapidly digestible foods. After falling for many years, the number of people suffering from starvation or malnutrition is rising, and thishas been exacerbated by the global COVID-19 pandemic. The highly integrated food supply chains that spread around the world are susceptible to disruptions due to policy changes, economic stresses, and natural disasters, as highlighted by the recent pandemic. In this perspective article, written by members of the Editorial Committee of the Annual Review of Food Science and Technology, we highlight some of the major challenges confronting the modern food supply chain as well as how innovations in policy and technology can be used to address them. Pertinent technological innovations include robotics, machine learning, artificial intelligence, advanced diagnostics, nanotechnology, biotechnology, gene editing, vertical farming, and soft matter physics. Many of these technologies are already being employed across the food chain by farmers, distributors, manufacturers, and consumers to improve the quality, nutrition, safety, and sustainability of the food supply. These innovations are required to stimulate the development and implementation of new technologies to ensure a more equitable, resilient, and efficient food production system. Where appropriate, these technologies should be carefully tested before widespread implementation so that proper risk–benefit analyses can be carried out. They can then be employed without causing unforeseen adverse consequences. Finally, it is important to actively engage all stakeholders involved in the food supply chain throughout the development and testing of these new technologies to support their adoption if proven safe and effective.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.1146/annurev-food-092220-030824",NA,NA,NA
"rayyan-100677700","Status and prospects for drought forecasting: opportunities in artificial intelligence and hybrid physical–statistical forecasting",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate | USER-NOTES: {""Maria""=>[""Despite major improvements in weather and climate modelling and substantial increases in remotely sensed observations, drought prediction remains a major challenge. After a review of the existing methods, we discuss major research gaps and opportunities to improve drought prediction. We argue that current approaches are top-down, assuming that the process(es) and/or driver(s) are known—i.e. starting with a model and then imposing it on the observed events (reality). With the help of an experiment, we show that there are opportunities to develop bottom-up drought prediction models—i.e. starting from the reality (here, observed events) and searching for model(s) and driver(s) that work. "", ""Recent advances in artificial intelligence and machine learning provide significant opportunities for developing bottom-up drought forecasting models. Regardless of the type of drought forecasting model (e.g. machine learning, dynamical simulations, analogue based), we need to shift our attention to robustness of theories and outputs rather than event-based verification. A shift in our focus towards quantifying the stability of uncertainty in drought prediction models, rather than the goodness of fit or reproducing the past, could be the first step towards this goal. Finally, we highlight the advantages of hybrid dynamical and statistical models for improving current drought prediction models.""]}","https://doi.org/10.1098/rsta.2021.0288",NA,NA,NA
"rayyan-100677706","Climate Downscaling Using YNet",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate | USER-NOTES: {""Maria""=>[""Climate change is one of the major challenges to human beings in our time. It brings many unexpected disasters which cause drastic losses including lives and properties. To better understand climate change, scientists developed various Global Climate Models (GCMs) to simulate the global climate and make projections for future climate values. These global climate models have coarse grids (i.e., low resolutions both in space and time) due to limitations of computing power and simulation time. Although they are helpful in predicting large scale long term trend in climate, they are too coarse for impact analysis in smaller scales such as in regional or local scale. However, climate conditions in regional or local scale are very important in making decisions related to climate conditions such as infrastructure, transportation and evacuation, as they highly depend on small scale climate conditions. "", ""In this paper, we proposed YNet, a novel deep convolutional neural network (CNN) with skip connections and fusion capabilities to perform downscaling for climate variables, on multiple GCMs directly rather than on reanalysis data. We analyzed and compared our proposed method with four other methods on datasets of three climate variables: mean precipitation, and extreme values (maximum temperature and minimum temperature). The results show the effectiveness of the proposed method.""]}","https://doi.org/10.1145/3394486.3403366",NA,NA,NA
"rayyan-100677709","Streamflow modelling and forecasting for Canadian watersheds using LSTM networks with attention mechanism",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract This study investigates the capability of sequence-to-sequence machine learning (ML) architectures in an effort to develop streamflow forecasting tools for Canadian watersheds. Such tools are useful to inform local and region-specific water management and flood forecasting related activities. Two powerful deep-learning variants of the Recurrent Neural Network were investigated, namely the standard and attention-based encoder-decoder long short-term memory (LSTM) models. Both models were forced with past hydro-meteorological states and daily meteorological data with a look-back time window of several days. These models were tested for 10 different watersheds from the Ottawa River watershed, located within the Great Lakes Saint-Lawrence region of Canada, an economic powerhouse of the country. The results of training and testing phases suggest that both models are able to simulate overall hydrograph patterns well when compared to observational records. Between the two models, the attention model significantly outperforms the standard model in all watersheds, suggesting the importance and usefulness of the attention mechanism in ML architectures, not well explored for hydrological applications. The mean performance accuracy of the attention model on unseen data, when assessed in terms of mean Nash–Sutcliffe Efficiency and Kling-Gupta Efficiency is, respectively, found to be 0.985 and 0.954 for these watersheds. Streamflow forecasts with lead times of up to 5 days with the attention model demonstrate overall skillful performance with well above the benchmark accuracy of 70%. The results of the study suggest that the encoder–decoder LSTM, with attention mechanism, is a powerful modelling choice for developing streamflow forecasting systems for Canadian watersheds.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1007/s00521-022-07523-8",NA,NA,NA
"rayyan-100677723","Adversarial super-resolution of climatological wind and solar data",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Significance Global climate simulations are typically unable to resolve wind and solar data at a resolution sufficient for renewable energy resource assessment in different climate scenarios. We introduce an adversarial deep learning approach to super resolve wind and solar outputs from global climate models by up to 50×. The inferred high-resolution fields are robust, physically consistent with the properties of atmospheric turbulence and solar irradiation, and can be adapted to domains from regional to global scales. This resolution enhancement enables critical localized assessments of the potential long-term economic viability of renewable energy resources.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1073/pnas.1918964117",NA,NA,NA
"rayyan-100677724","The potential of data driven approaches for quantifying hydrological extremes",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Recent droughts in Europe have shown that national water systems are facing increasing challenges when dealing with drought impacts. Especially the Netherlands has seen an increasing need to adapt their water management to improve preparedness for future drought events. Ideally, the necessary information needed for operational water management decisions should be readily available ahead of time and/or computed flexibly and efficiently to ensure sufficient time to evaluate the various management actions. In this study, we show that in addition to physically based hydrological models, the upcoming and promising trend of incorporating machine learning (ML) in hydrology can provide the basis for future efforts in supporting national operational water management by providing the needed information efficiently and with the required accuracy. As a precursor for their use in a forecasting system, we assessed the ability of five different data driven methods to simulate hydrological variables at a national-scale. We developed a unified workflow where we use limited information on hydro-meteorological variables and general water management policies to simulate historic timeseries of discharge, groundwater levels, surface water levels and surface water temperatures. We find that all ML methods, ranging from very simple to more complex ones, showed a generally good performance for stations and target levels which are closely linked to the input data and location (e.g. stations along main river network). For downstream stations and small rivers, the Random Forest method outperforms the other methods both for discharge and surface water levels. For surface water temperature no location dependency was observed and for groundwater levels, all methods were performing comparable with most stations ranging in nRMSE 0.2-0.3. Generally, the best performances were reached by the more advanced Random Forest and LSTM methods, which was also seen when simulating high and low flow events. High flow events were slightly better captured than low flow events but overall simulating extreme events based on a simple input data set remains challenging. Specific training sets, including event related information and additional input variables, could like improve future assessments. Including the feature importance of the methods allowed us to detect how and where water management influence played an important role. The addition of information on water management in the ML routines increases overall performance, although limited. We conclude that ML and other data driven approaches have potential in predicting different hydrological variables. We were able to capture and incorporate water management aspects in our analysis, creating a base for future experiments where scenario analysis might reveal ML based mitigation strategies. The combination of limited input data requirement and short computation times makes this new framework suitable for forecasting purposes.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1016/j.advwatres.2021.104017",NA,NA,NA
"rayyan-100677725","FWENet: a deep convolutional neural network for flood water body extraction based on SAR images",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"As one of the most severe natural disasters in the world, floods caused substantial economic losses and casualties every year. Timely and accurate acquisition of flood inundation extent could provide technical support for relevant departments in the field of flood emergency response and disaster relief. Given the accuracy of existing research works extracting flood inundation extent based on Synthetic Aperture Radar (SAR) images and deep learning methods is relatively low, this study utilized Sentinel-1 SAR images as the data source and proposed a novel model named flood water body extraction convolutional neural network (FWENet) for flood information extraction. Then three classical semantic segmentation models (UNet, Deeplab v3 and UNet++) and two traditional water body extraction methods (Otsu global thresholding method and Object-Oriented method) were compared with the FWENet model. Furthermore, this paper analyzed the water body area change situations of Poyang Lake. The main results of this paper were as follows: Compared with other five water body extraction methods, the FWENet model achieved the highest water body extraction accuracy, its F1 score and mean intersection over union (mIoU) were 0.9871 and 0.9808, respectively. This study could guarantee the subsequent research on flood extraction based on SAR images.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1080/17538947.2021.1995513",NA,NA,NA
"rayyan-100677727","Machine learning for weather and climate are worlds apart",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Modern weather and climate models share a common heritage and often even components; however, they are used in different ways to answer fundamentally different questions. As such, attempts to emulate them using machine learning should reflect this. While the use of machine learning to emulate weather forecast models is a relatively new endeavour, there is a rich history of climate model emulation. This is primarily because while weather modelling is an initial condition problem, which intimately depends on the current state of the atmosphere, climate modelling is predominantly a boundary condition problem. To emulate the response of the climate to different drivers therefore, representation of the full dynamical evolution of the atmosphere is neither necessary, or in many cases, desirable. Climate scientists are typically interested in different questions also. Indeed emulating the steady-state climate response has been possible for many years and provides significant speed increases that allow solving inverse problems for e.g. parameter estimation. Nevertheless, the large datasets, non-linear relationships and limited training data make climate a domain which is rich in interesting machine learning challenges. Here, I seek to set out the current state of climate model emulation and demonstrate how, despite some challenges, recent advances in machine learning provide new opportunities for creating useful statistical models of the climate. This article is part of the theme issue ‘Machine learning for weather and climate modelling’.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1098/rsta.2020.0098",NA,NA,NA
"rayyan-100677732","MEDIC: a multi-task learning dataset for disaster image classification",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract Recent research in disaster informatics demonstrates a practical and important use case of artificial intelligence to save human lives and suffering during natural disasters based on social media contents (text and images). While notable progress has been made using texts, research on exploiting the images remains relatively under-explored. To advance image-based approaches, we propose MEDIC ( https://crisisnlp.qcri.org/medic/index.html ), which is the largest social media image classification dataset for humanitarian response consisting of 71,198 images to address four different tasks in a multi-task learning setup. This is the first dataset of its kind: social media images, disaster response, and multi-task learning research. An important property of this dataset is its high potential to facilitate research on multi-task learning , which recently receives much interest from the machine learning community and has shown remarkable results in terms of memory, inference speed, performance, and generalization capability. Therefore, the proposed dataset is an important resource for advancing image-based disaster management and multi-task machine learning research. We experiment with different deep learning architectures and report promising results, which are above the majority baselines for all tasks. Along with the dataset, we also release all relevant scripts ( https://github.com/firojalam/medic ).","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1007/s00521-022-07717-0",NA,NA,NA
"rayyan-100677735","Artificial Intelligence-Driven Circular Economy as a Key Enabler for Sustainable Energy Management",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Nearly a billion new energy consumers join the society in 13–15 years; and the growing demand for higher standards of living makes the worldwide energy consumption continuously growing. Strategic solutions are therefore required not only for addressing energy gaps but also for eliminating the undesirable environmental effects of energy supply chain to ensure quality and sustainable living. This article advocates leveraging artificial intelligence associated digital technologies to increase energy efficiency, to facilitate carbon trading, and to realize the circular economy vision of countries to mitigate extreme weather conditions and climate change.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.1007/s42824-020-00009-9",NA,NA,NA
"rayyan-100677736","Significant increase in natural disturbance impacts on European forests since 1950",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract Over the last decades, the natural disturbance is increasingly putting pressure on European forests. Shifts in disturbance regimes may compromise forest functioning and the continuous provisioning of ecosystem services to society, including their climate change mitigation potential. Although forests are central to many European policies, we lack the long‐term empirical data needed for thoroughly understanding disturbance dynamics, modeling them, and developing adaptive management strategies. Here, we present a unique database of &gt;170,000 records of ground‐based natural disturbance observations in European forests from 1950 to 2019. Reported data confirm a significant increase in forest disturbance in 34 European countries, causing on an average of 43.8 million m 3 of disturbed timber volume per year over the 70‐year study period. This value is likely a conservative estimate due to under‐reporting, especially of small‐scale disturbances. We used machine learning techniques for assessing the magnitude of unreported disturbances, which are estimated to be between 8.6 and 18.3 million m 3 /year. In the last 20 years, disturbances on average accounted for 16% of the mean annual harvest in Europe. Wind was the most important disturbance agent over the study period (46% of total damage), followed by fire (24%) and bark beetles (17%). Bark beetle disturbance doubled its share of the total damage in the last 20 years. Forest disturbances can profoundly impact ecosystem services (e.g., climate change mitigation), affect regional forest resource provisioning and consequently disrupt long‐term management planning objectives and timber markets. We conclude that adaptation to changing disturbance regimes must be placed at the core of the European forest management and policy debate. Furthermore, a coherent and homogeneous monitoring system of natural disturbances is urgently needed in Europe, to better observe and respond to the ongoing changes in forest disturbance regimes.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1111/gcb.16531",NA,NA,NA
"rayyan-100677750","Using Support Vector Machine (SVM) with GPS Ionospheric TEC Estimations to Potentially Predict Earthquake Events",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"There are significant controversies surrounding the detection of precursors that may precede earthquakes. Natural hazard signatures associated with strong earthquakes can appear in the lithosphere, troposphere, and ionosphere, where current remote sensing technologies have become valuable tools for detecting and measuring early warning signals of stress build-up deep in the Earth’s crust (presumably associated with earthquake events). Here, we propose implementing a machine learning support vector machine (SVM) technique, applied with GPS ionospheric total electron content (TEC) pre-processed time series estimations, to evaluate potential precursors caused by earthquakes and manifested as disturbances in the TEC data. After filtering and screening our data for solar or geomagnetic influences at different time scales, our results indicate that for large earthquakes (&gt;Mw 6), true negative predictions can be achieved with 85.7% accuracy, and true positive predictions with an accuracy of 80%. We tested our method with different skill scores, such as accuracy (0.83), precision (0.85), recall (0.8), the Heidke skill score (0.66), and true skill statistics (0.66).","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.3390/rs14122822",NA,NA,NA
"rayyan-100677758","Deep learning rainfall–runoff predictions of extreme events",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract. The most accurate rainfall–runoff predictions are currently based on deep learning. There is a concern among hydrologists that the predictive accuracy of data-driven models based on deep learning may not be reliable in extrapolation or for predicting extreme events. This study tests that hypothesis using long short-term memory (LSTM) networks and an LSTM variant that is architecturally constrained to conserve mass. The LSTM network (and the mass-conserving LSTM variant) remained relatively accurate in predicting extreme (high-return-period) events compared with both a conceptual model (the Sacramento Model) and a process-based model (the US National Water Model), even when extreme events were not included in the training period. Adding mass balance constraints to the data-driven model (LSTM) reduced model skill during extreme events.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.5194/hess-26-3377-2022",NA,NA,NA
"rayyan-100677762","Global prediction of extreme floods in ungauged watersheds",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract Floods are one of the most common natural disasters, with a disproportionate impact in developing countries that often lack dense streamflow gauge networks 1 . Accurate and timely warnings are critical for mitigating flood risks 2 , but hydrological simulation models typically must be calibrated to long data records in each watershed. Here we show that artificial intelligence-based forecasting achieves reliability in predicting extreme riverine events in ungauged watersheds at up to a five-day lead time that is similar to or better than the reliability of nowcasts (zero-day lead time) from a current state-of-the-art global modelling system (the Copernicus Emergency Management Service Global Flood Awareness System). In addition, we achieve accuracies over five-year return period events that are similar to or better than current accuracies over one-year return period events. This means that artificial intelligence can provide flood warnings earlier and over larger and more impactful events in ungauged basins. The model developed here was incorporated into an operational early warning system that produces publicly available (free and open) forecasts in real time in over 80 countries. This work highlights a need for increasing the availability of hydrological data to continue to improve global access to reliable flood warnings.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1038/s41586-024-07145-1",NA,NA,NA
"rayyan-100677769","The transformer earthquake alerting model: a new versatile approach to earthquake early warning",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Earthquakes are major hazards to humans, buildings and infrastructure. Early warning methods aim to provide advance notice of incoming strong shaking to enable preventive action and mitigate seismic risk. Their usefulness depends on accuracy, the relation between true, missed and false alerts, and timeliness, the time between a warning and the arrival of strong shaking. Current approaches suffer from apparent aleatoric uncertainties due to simplified modelling or short warning times. Here we propose a novel early warning method, the deep-learning based transformer earthquake alerting model (TEAM), to mitigate these limitations. TEAM analyzes raw, strong motion waveforms of an arbitrary number of stations at arbitrary locations in real-time, making it easily adaptable to changing seismic networks and warning targets. We evaluate TEAM on two regions with high seismic hazard, Japan and Italy, that are complementary in their seismicity. On both datasets TEAM outperforms existing early warning methods considerably, offering accurate and timely warnings. Using domain adaptation, TEAM even provides reliable alerts for events larger than any in the training data, a property of highest importance as records from very large events are rare in many regions.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1093/gji/ggaa609",NA,NA,NA
"rayyan-100677773","Using Information Technology to Manage the COVID-19 Pandemic: Development of a Technical Framework Based on Practical Experience in China",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Background The coronavirus disease (COVID-19) epidemic poses an enormous challenge to the global health system, and governments have taken active preventive and control measures. The health informatics community in China has actively taken action to leverage health information technologies for epidemic monitoring, detection, early warning, prevention and control, and other tasks. Objective The aim of this study was to develop a technical framework to respond to the COVID-19 epidemic from a health informatics perspective. Methods In this study, we collected health information technology–related information to understand the actions taken by the health informatics community in China during the COVID-19 outbreak and developed a health information technology framework for epidemic response based on health information technology–related measures and methods. Results Based on the framework, we review specific health information technology practices for managing the outbreak in China, describe the highlights of their application in detail, and discuss critical issues to consider when using health information technology. Technologies employed include mobile and web-based services such as Internet hospitals and Wechat, big data analyses (including digital contact tracing through QR codes or epidemic prediction), cloud computing, Internet of things, Artificial Intelligence (including the use of drones, robots, and intelligent diagnoses), 5G telemedicine, and clinical information systems to facilitate clinical management for COVID-19. Conclusions Practical experience in China shows that health information technologies play a pivotal role in responding to the COVID-19 epidemic.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.2196/19515",NA,NA,NA
"rayyan-100677781","Fermented food products in the era of globalization: tradition meets biotechnology innovations",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene | USER-NOTES: {""Maria""=>[""Omics tools offer the opportunity to characterize and trace traditional and industrial fermented foods. Bioinformatics, through machine learning, and other advanced statistical approaches, are able to disentangle fermentation processes and to predict the evolution and metabolic outcomes of a food microbial ecosystem. By assembling microbial artificial consortia, the biotechnological advances will also be able to enhance the nutritional value and organoleptics characteristics of fermented food, preserving, at the same time, the potential of autochthonous microbial consortia and metabolic pathways, which are difficult to reproduce. Preserving the traditional methods contributes to protecting the hidden value of local biodiversity, and exploits its potential in industrial processes with the final aim of guaranteeing food security and safety, even in developing countries.""]}","https://doi.org/10.1016/j.copbio.2020.10.006",NA,NA,NA
"rayyan-100677783","Predicting and Understanding Landslide Events With Explainable AI",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Rainfall induced landslide is one of the main geological hazard in Italy and in the world. Each year it causes fatalities, casualties and economic and social losses on large populated areas. Accurate short-term predictions of landslides can be extremely important and useful, in order to both provide local authorities with efficient prediction/early warning and increase the resilience to manage emergencies. There is an extensive literature addressing the problem of computing landslide susceptibility maps (which is a classification problem exploiting a large range of static features) and only few on actual short terms predictions (spatial and temporal). The short-term prediction models are still empirical and obtain unsatisfactory results, also in the identification of the predictors. The new aspects addressed in this paper are: (i) a short-term prediction model (1 day in advance) of landslide based on machine learning, (ii) real time features as good predictors. The introduction of explainable artificial intelligence techniques allowed to understand global and local feature relevance. In order to find the best prediction model, some machine learning solutions have been implemented and assessed. The obtained models overcome the ones available in literature. The validation has been performed in the context of the Metropolitan City of Florence, data from 2013 to 2019. The method based on XGBoost achieved best results, demonstrating that it is the most reliable and robust against false alarms. Finally, we applied explainable artificial intelligence techniques locally and globally to derive a deep understanding of the predictive model’s outputs and features’ relevance, and relationships. The analysis allowed us to identify the best feature for short term predictions and their impact in local cases and global prediction model. Solutions have been implemented on Snap4City.org infrastructure.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1109/access.2022.3158328",NA,NA,NA
"rayyan-100677793","floodGAN: Using Deep Adversarial Learning to Predict Pluvial Flooding in Real Time",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Using machine learning for pluvial flood prediction tasks has gained growing attention in the past years. In particular, data-driven models using artificial neuronal networks show promising results, shortening the computation times of physically based simulations. However, recent approaches have used mainly conventional fully connected neural networks which were (a) restricted to spatially uniform precipitation events and (b) limited to a small amount of input data. In this work, a deep convolutional generative adversarial network has been developed to predict pluvial flooding caused by nonlinear spatial heterogeny rainfall events. The model developed, floodGAN, is based on an image-to-image translation approach whereby the model learns to generate 2D inundation predictions conditioned by heterogenous rainfall distributions—through the minimax game of two adversarial networks. The training data for the floodGAN model was generated using a physically based hydrodynamic model. To evaluate the performance and accuracy of the floodGAN, model multiple tests were conducted using both synthetic events and a historic rainfall event. The results demonstrate that the proposed floodGAN model is up to 106 times faster than the hydrodynamic model and promising in terms of accuracy and generalizability. Therefore, it bridges the gap between detailed flood modelling and real-time applications such as end-to-end early warning systems.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.3390/w13162255",NA,NA,NA
"rayyan-100677794","Deploying digitalisation and artificial intelligence in sustainable development research",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Many industrialised countries have benefited from the advent of twenty-first century technologies, especially automation, that have fundamentally changed manufacturing and industrial production processes. The next step in the evolution of automation is the development of artificial intelligence (AI), i.e. intelligence which is demonstrated by machines and systems, which cannot only perform tasks but also work synergistically with humans and nature. Intelligent systems that can see, analyse situations and respond sensitively to real-time cues, from human gestures and facial expressions to pedestrians crossing a busy street, will reshape transportation, precision agriculture, biodiversity conservation, environmental modelling, public health, construction and manufacturing, as well as initiatives designed to promote prosperity on Earth. This paper explores the connections between AI systems and sustainable development (SD) research. By means of a literature review, world survey, and case studies, ways in which AI can support research on SD and, inter alia, contribute to a more sustainable and equitable world, are identified.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.1007/s10668-022-02252-3",NA,NA,NA
"rayyan-100677795","Comparison of Machine Learning Algorithms for Flood Susceptibility Mapping",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Floods are one of the most destructive natural disasters, causing financial and human losses every year. As a result, reliable Flood Susceptibility Mapping (FSM) is required for effective flood management and reducing its harmful effects. In this study, a new machine learning model based on the Cascade Forest Model (CFM) was developed for FSM. Satellite imagery, historical reports, and field data were used to determine flood-inundated areas. The database included 21 flood-conditioning factors obtained from different sources. The performance of the proposed CFM was evaluated over two study areas, and the results were compared with those of other six machine learning methods, including Support Vector Machine (SVM), Decision Tree (DT), Random Forest (RF), Deep Neural Network (DNN), Light Gradient Boosting Machine (LightGBM), Extreme Gradient Boosting (XGBoost), and Categorical Boosting (CatBoost). The result showed CFM produced the highest accuracy compared to other models over both study areas. The Overall Accuracy (AC), Kappa Coefficient (KC), and Area Under the Receiver Operating Characteristic Curve (AUC) of the proposed model were more than 95%, 0.8, 0.95, respectively. Most of these models recognized the southwestern part of the Karun basin, northern and northwestern regions of the Gorganrud basin as susceptible areas.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.3390/rs15010192",NA,NA,NA
"rayyan-100677802","A guided latent Dirichlet allocation approach to investigate real-time latent topics of Twitter data during Hurricane Laura",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate | USER-NOTES: {""Maria""=>[""Natural disasters cause significant damage, casualties and economical losses. Twitter has been used to support prompt disaster response and management because people tend to communicate and spread information on public social media platforms during disaster events. To retrieve real-time situational awareness (SA) information from tweets, the most effective way to mine text is using natural language processing (NLP). Among the advanced NLP models, the supervised approach can classify tweets into different categories to gain insight and leverage useful SA information from social media data. However, high-performing supervised models require domain knowledge to specify categories and involve costly labelling tasks."", ""This research proposes a guided latent Dirichlet allocation (LDA) workflow to investigate temporal latent topics from tweets during a recent disaster event, the 2020 Hurricane Laura. With integration of prior knowledge, a coherence model, LDA topics visualisation and validation from official reports, our guided approach reveals that most tweets contain several latent topics during the 10-day period of Hurricane Laura. This result indicates that state-of-the-art supervised models have not fully utilised tweet information because they only assign each tweet a single label. In contrast, our model can not only identify emerging topics during different disaster events but also provides multilabel references to the classification schema. "", ""In addition, our results can help to quickly identify and extract SA information to responders, stakeholders and the general public so that they can adopt timely responsive strategies and wisely allocate resource during Hurricane events.""]}","https://doi.org/10.1177/01655515211007724",NA,NA,NA
"rayyan-100677803","RCL-Learning: ResNet and convolutional long short-term memory-based spatiotemporal air pollutant concentration prediction model",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate | USER-NOTES: {""Maria""=>[""Predicting the concentration of air pollutants is an effective method for preventing pollution incidents by providing an early warning of harmful substances in the air. Accurate prediction of air pollutant concentration can more effectively control and prevent air pollution. In this study, a big data correlation principle and deep learning technology are used for a proposed model of predicting PM2.5 concentration. The model comprises a deep learning network model based on a residual neural network (ResNet) and a convolutional long short-term memory (LSTM) network (ConvLSTM). ResNet is used to deeply extract the spatial distribution features of pollutant concentration and meteorological data from multiple cities."", ""The output is used as input to ConvLSTM, which further extracts the preliminary spatial distribution features extracted from the ResNet, while extracting the spatiotemporal features of the pollutant concentration and meteorological data. The model combines the two features to achieve a spatiotemporal correlation of feature sequences, thereby accurately predicting the future PM2.5 concentration of the target city for a period of time. Compared with other neural network models and traditional models, the proposed pollutant concentration prediction model improves the accuracy of predicting pollutant concentration."", ""For 1- to 3-hours prediction tasks, the proposed pollutant concentration prediction model performed well and exhibited root mean square error (RMSE) between 5.478 and 13.622. In addition, we conducted multiscale predictions in the target city and achieved satisfactory performance, with the average RMSE value able to reach 22.927 even for 1- to 15-hours prediction tasks.""]}","https://doi.org/10.1016/j.eswa.2022.118017",NA,NA,NA
"rayyan-100677813","DiTing: A large-scale Chinese seismic benchmark dataset for artificial intelligence in seismology",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"In recent years, artificial intelligence technology has exhibited great potential in seismic signal recognition, setting off a new wave of research. Vast amounts of high-quality labeled data are required to develop and apply artificial intelligence in seismology research. In this study, based on the 2013–2020 seismic cataloging reports of the China Earthquake Networks Center, we constructed an artificial intelligence seismological training dataset (""DiTing"") with the largest known total time length. Data were recorded using broadband and short-period seismometers. The obtained dataset included 2,734,748 three-component waveform traces from 787,010 regional seismic events, the corresponding P- and S-phase arrival time labels, and 641,025 P-wave first-motion polarity labels. All waveforms were sampled at 50 Hz and cut to a time length of 180 s starting from a random number of seconds before the occurrence of an earthquake. Each three-component waveform contained a considerable amount of descriptive information, such as the epicentral distance, back azimuth, and signal-to-noise ratios. The magnitudes of seismic events, epicentral distance, signal-to-noise ratio of P-wave data, and signal-to-noise ratio of S-wave data ranged from 0 to 7.7, 0 to 330 km, –0.05 to 5.31 dB, and –0.05 to 4.73 dB, respectively. The dataset compiled in this study can serve as a high-quality benchmark for machine learning model development and data-driven seismological research on earthquake detection, seismic phase picking, first-motion polarity determination, earthquake magnitude prediction, early warning systems, and strong ground-motion prediction. Such research will further promote the development and application of artificial intelligence in seismology.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1016/j.eqs.2022.01.022",NA,NA,NA
"rayyan-100677814","Self-organizing maps of typhoon tracks allow for flood forecasts up to two days in advance",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract Typhoons are among the greatest natural hazards along East Asian coasts. Typhoon-related precipitation can produce flooding that is often only predictable a few hours in advance. Here, we present a machine-learning method comparing projected typhoon tracks with past trajectories, then using the information to predict flood hydrographs for a watershed on Taiwan. The hydrographs provide early warning of possible flooding prior to typhoon landfall, and then real-time updates of expected flooding along the typhoon’s path. The method associates different types of typhoon tracks with landscape topography and runoff data to estimate the water inflow into a reservoir, allowing prediction of flood hydrographs up to two days in advance with continual updates. Modelling involves identifying typhoon track vectors, clustering vectors using a self-organizing map, extracting flow characteristic curves, and predicting flood hydrographs. This machine learning approach can significantly improve existing flood warning systems and provide early warnings to reservoir management.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1038/s41467-020-15734-7",NA,NA,NA
"rayyan-100677827","Short-term flood probability density forecasting using a conceptual hydrological model with machine learning techniques",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Making accurate and reliable probability density forecasts of flood processes is fundamentally challenging for machine learning techniques, especially when prediction targets are outside the range of training data. Conceptual hydrological models can reduce rainfall-runoff modelling errors with efficient quasi-physical mechanisms. The Monotone Composite Quantile Regression Neural Network (MCQRNN) is used for the first time to make probability density forecasts of flood processes and serves as a benchmark model, whereas it confronts the drawbacks of overfitting and biased-prediction. Here we propose an integrated model (i.e. XAJ-MCQRNN) that incorporates Xinanjiang conceptual model (XAJ) and MCQRNN to overcome the phenomena of error propagation and accumulation encountered in multi-step-ahead flood probability density forecasts. We consider flood forecasts as a function of rainfall factors and runoff data. The models are evaluated by long-term (2009–2015) 3-hour streamflow series of the Jianxi River catchment in China and rainfall products of the European Centre for Medium-Range Weather Forecasts. Results demonstrated that the proposed XAJ-MCQRNN model can not only outperform the MCQRNN model but also prominently enhance the accuracy and reliability of multi-step-ahead probability density forecasts of flood process. Regarding short-term forecasts in testing stages at four horizons, the XAJ-MCQRNN model achieved higher Nash-Sutcliffe Efficiency but lower Root Mean Square Error values, while improving Coverage Ratio and Relative Bandwidth values in comparison to the MCQRNN model. Consequently, the improvement can benefit the mitigation of the impacts associated with uncertainties of extreme flood and rainfall events as well as promote the accuracy and reliability of flood forecasting and early warning.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1016/j.jhydrol.2021.127255",NA,NA,NA
"rayyan-100677837","OmbriaNet—Supervised Flood Mapping via Convolutional Neural Networks Using Multitemporal Sentinel-1 and Sentinel-2 Data Fusion",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Regions around the world experience adverse climate-change-induced conditions that pose severe risks to the normal and sustainable operations of modern societies. Extreme weather events, such as floods, rising sea levels, and storms, stand as characteristic examples that impair the core services of the global ecosystem. Especially floods have a severe impact on human activities, hence, early and accurate delineation of the disaster is of top priority since it provides environmental, economic, and societal benefits and eases relief efforts. In this article, we introduce OmbriaNet, a deep neural network architecture, based on convolutional neural networks, that detects changes between permanent and flooded water areas by exploiting the temporal differences among flood events extracted by different sensors. To demonstrate the potential of the proposed approach, we generated OMBRIA, a bitemporal and multimodal satellite imagery dataset for image segmentation through supervised binary classification. It consists of a total number of 3.376 images, synthetic aperture radar imagery from Sentinel-1, and multispectral imagery from Sentinel-2, accompanied with ground-truth binary images produced from data derived by experts and provided from the Emergency Management Service of the European Space Agency Copernicus Program. The dataset covers 23 flood events around the globe, from 2017 to 2021. We collected, co-registrated and preprocessed the data in Google Earth Engine. To validate the performance of our method, we performed different benchmarking experiments on the OMBRIA dataset and we compared with several competitive state-of-the-art techniques. The experimental analysis demonstrated that the proposed formulation is able to produce high-quality flood maps, achieving a superior performance over the state-of-the-art. We provide OMBRIA dataset, as well as OmbriaNet code at: <uri xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">https://github.com/geodrak/OMBRIA</uri> .","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1109/jstars.2022.3155559",NA,NA,NA
"rayyan-100677841","Analysing trade-offs and synergies between SDGs for urban development, food security and poverty alleviation in rapidly changing peri-urban areas: a tool to support inclusive urban planning",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract Transitional peri-urban contexts are frontiers for sustainable development where land-use change involves negotiation and contestation between diverse interest groups. Multiple, complex trade-offs between outcomes emerge which have both negative and positive impacts on progress towards achieving Sustainable Development Goals (SDGs). These trade-offs are often overlooked in policy and planning processes which depend on top-down expert perspectives and rely on course grain aggregate data which does not reflect complex peri-urban dynamics or the rapid pace of change. Tools are required to address this gap, integrate data from diverse perspectives and inform more inclusive planning processes. In this paper, we draw on a reinterpretation of empirical data concerned with land-use change and multiple dimensions of food security from the city of Wuhan in China to illustrate some of the complex trade-offs between SDG goals that tend to be overlooked with current planning approaches. We then describe the development of an interactive web-based tool that implements deep learning methods for fine-grained land-use classification of high-resolution remote sensing imagery and integrates this with a flexible method for rapid trade-off analysis of land-use change scenarios. The development and potential use of the tool are illustrated using data from the Wuhan case study example. This tool has the potential to support participatory planning processes by providing a platform for multiple stakeholders to explore the implications of planning decisions and land-use policies. Used alongside other planning, engagement and ecosystem service mapping tools it can help to reveal invisible trade-offs and foreground the perspectives of diverse stakeholders. This is vital for building approaches which recognise how trade-offs between the achievement of SDGs can be influenced by development interventions.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.1007/s11625-020-00802-0",NA,NA,NA
"rayyan-100677849","Forecasting global climate drivers using Gaussian processes and convolutional autoencoders",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Machine learning (ML) methods have become an important tool for modelling and forecasting complex high-dimensional spatiotemporal datasets such as those found in environmental and climate modelling applications. ML approaches can offer a fast, low-cost alternative to short-term forecasting than expensive numerical simulation while addressing a significant outstanding limitation of numerical modelling by being able to robustly and dynamically quantify predictive uncertainty. Low-cost and near-instantaneous forecasting of high-level climate variables has clear applications in early warning systems, nowcasting, and parameterising small-scale locally relevant simulations. This paper presents a novel approach for multi-task spatiotemporal regression by combining data-driven autoencoders with Gaussian Processes (GP) to produce a probabilistic tensor-based regression model. The proposed method is demonstrated for forecasting one-step-ahead temperature and pressure on a global scale simultaneously. By conducting probabilistic regression in the learned latent space, samples can be propagated back to the original feature space to produce uncertainty estimates at a vastly reduced computational cost. The composite GP-autoencoder model was able to simultaneously forecast global temperature and pressure values with average errors of 3.82 °C and 638 hPa, respectively. Further, on average the true values were within the proposed posterior distribution 95.6% of the time illustrating that the model produces a well-calibrated predictive posterior distribution.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1016/j.engappai.2023.107536",NA,NA,NA
"rayyan-100677863","Deepti: Deep-Learning-Based Tropical Cyclone Intensity Estimation System",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Tropical cyclones are one of the costliest natural disasters globally because of the wide range of associated hazards. Thus, an accurate diagnostic model for tropical cyclone intensity can save lives and property. There are a number of existing techniques and approaches that diagnose tropical cyclone wind speed using satellite data at a given time with varying success. This article presents a deep-learning-based objective, diagnostic estimate of tropical cyclone intensity from infrared satellite imagery with 13.24-kn root mean squared error. In addition, a visualization portal in a production system is presented that displays deep learning output and contextual information for end users, one of the first of its kind.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1109/jstars.2020.3011907",NA,NA,NA
"rayyan-100677870","ClimateBench v1.0: A Benchmark for Data‐Driven Climate Projections",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract Many different emission pathways exist that are compatible with the Paris climate agreement, and many more are possible that miss that target. While some of the most complex Earth System Models have simulated a small selection of Shared Socioeconomic Pathways, it is impractical to use these expensive models to fully explore the space of possibilities. Such explorations therefore mostly rely on one‐dimensional impulse response models, or simple pattern scaling approaches to approximate the physical climate response to a given scenario. Here we present ClimateBench—the first benchmarking framework based on a suite of Coupled Model Intercomparison Project, AerChemMIP and Detection‐Attribution Model Intercomparison Project simulations performed by a full complexity Earth System Model, and a set of baseline machine learning models that emulate its response to a variety of forcers. These emulators can predict annual mean global distributions of temperature, diurnal temperature range and precipitation (including extreme precipitation) given a wide range of emissions and concentrations of carbon dioxide, methane and aerosols, allowing them to efficiently probe previously unexplored scenarios. We discuss the accuracy and interpretability of these emulators and consider their robustness to physical constraints such as total energy conservation. Future opportunities incorporating such physical constraints directly in the machine learning models and using the emulators for detection and attribution studies are also discussed. This opens a wide range of opportunities to improve prediction, robustness and mathematical tractability. We hope that by laying out the principles of climate model emulation with clear examples and metrics we encourage engagement from statisticians and machine learning specialists keen to tackle this important and demanding challenge.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1029/2021ms002954",NA,NA,NA
"rayyan-100677878","Long-term time-series pollution forecast using statistical and deep learning methods",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Tackling air pollution has become of utmost importance since the last few decades. Different statistical as well as deep learning methods have been proposed till now, but seldom those have been used to forecast future long-term pollution trends. Forecasting long-term pollution trends into the future is highly important for government bodies around the globe as they help in the framing of efficient environmental policies. This paper presents a comparative study of various statistical and deep learning methods to forecast long-term pollution trends for the two most important categories of particulate matter (PM) which are PM2.5 and PM10. The study is based on Kolkata, a major city on the eastern side of India. The historical pollution data collected from government set-up monitoring stations in Kolkata are used to analyse the underlying patterns with the help of various time-series analysis techniques, which is then used to produce a forecast for the next two years using different statistical and deep learning methods. The findings reflect that statistical methods such as auto-regressive (AR), seasonal auto-regressive integrated moving average (SARIMA) and Holt-Winters outperform deep learning methods such as stacked, bi-directional, auto-encoder and convolution long short-term memory networks based on the limited data available.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1007/s00521-021-05901-2",NA,NA,NA
"rayyan-100677882","Hybrid forecasting: blending climate predictions with AI models",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract. Hybrid hydroclimatic forecasting systems employ data-driven (statistical or machine learning) methods to harness and integrate a broad variety of predictions from dynamical, physics-based models – such as numerical weather prediction, climate, land, hydrology, and Earth system models – into a final prediction product. They are recognized as a promising way of enhancing the prediction skill of meteorological and hydroclimatic variables and events, including rainfall, temperature, streamflow, floods, droughts, tropical cyclones, or atmospheric rivers. Hybrid forecasting methods are now receiving growing attention due to advances in weather and climate prediction systems at subseasonal to decadal scales, a better appreciation of the strengths of AI, and expanding access to computational resources and methods. Such systems are attractive because they may avoid the need to run a computationally expensive offline land model, can minimize the effect of biases that exist within dynamical outputs, benefit from the strengths of machine learning, and can learn from large datasets, while combining different sources of predictability with varying time horizons. Here we review recent developments in hybrid hydroclimatic forecasting and outline key challenges and opportunities for further research. These include obtaining physically explainable results, assimilating human influences from novel data sources, integrating new ensemble techniques to improve predictive skill, creating seamless prediction schemes that merge short to long lead times, incorporating initial land surface and ocean/ice conditions, acknowledging spatial variability in landscape and atmospheric forcing, and increasing the operational uptake of hybrid prediction schemes.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.5194/hess-27-1865-2023",NA,NA,NA
"rayyan-100677886","Predicting Lake Erie wave heights and periods using XGBoost and LSTM",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Waves in large lakes put coastal communities and vessels under threat, and accurate wave predictions are needed for early warnings. While physics-based numerical wave models such as WAVEWATCH III (WW3) are useful to provide spatial information to supplement in situ observations, they require intensive computational resources. An attractive alternative is machine learning (ML) methods, which can potentially improve the performance of numerical wave models, while only requiring a small fraction of the computational cost. In this study, we applied novel ML methods based on XGBoost and a Long Short-Term Memory (LSTM) recurrent neural network for predicting wave height and period under the near-idealized wave growth conditions of Lake Erie. Data sets of significant wave height (H), peak wave period (Tp) and surface wind from two offshore buoys from 1994 to 2017 were processed for model training and testing. We trained and validated the ML models with the data sets from 1994 to 2015, and then used the trained models to predict significant wave height and peak period for 2016 and 2017. The XGBoost model yielded the best overall performance, with Mean Absolute Percentage Error (MAPE) values of 15.6%–22.9% in H and 8.3%–13.4% in Tp. The LSTM model yielded MAPE values of 23.4%–30.8% in H and 9.1%–13.6% in Tp. An unstructured grid WW3 applied to Lake Erie yielded MAPE values of 15.3%–21.0% in H and 12.5%–19.3% in Tp. However, WW3 underestimated H and Tp during strong wind events, with relative biases of -11.76% to -14.15% in H and -15.59% to -19.68% in Tp. XGBoost and LSTM improve on these predictions with relative biases of -2.56% to -10.61% in H and -8.08% to -10.13% in Tp. An ensemble mean of these three models yielded lower scatter scores than the members, with MAPE values of 13.3%–17.3% in H and 8.0%–13.0% in Tp, although it did not improve the bias. The ML models ran significantly faster than WW3: For this 2-year run on the same computing environment, WW3 needed 24 h with 60 CPUs, whereas the trained LSTM needed 0.24 s on 1 CPU, and the trained XGBoost needed only 0.03 s on 1 CPU.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1016/j.ocemod.2021.101832",NA,NA,NA
"rayyan-100677892","DroughtCast: A Machine Learning Forecast of the United States Drought Monitor",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Drought is one of the most ecologically and economically devastating natural phenomena affecting the United States, causing the U.S. economy billions of dollars in damage, and driving widespread degradation of ecosystem health. Many drought indices are implemented to monitor the current extent and status of drought so stakeholders such as farmers and local governments can appropriately respond. Methods to forecast drought conditions weeks to months in advance are less common but would provide a more effective early warning system to enhance drought response, mitigation, and adaptation planning. To resolve this issue, we introduce DroughtCast, a machine learning framework for forecasting the United States Drought Monitor (USDM). DroughtCast operates on the knowledge that recent anomalies in hydrology and meteorology drive future changes in drought conditions. We use simulated meteorology and satellite observed soil moisture as inputs into a recurrent neural network to accurately forecast the USDM between 1 and 12 weeks into the future. Our analysis shows that precipitation, soil moisture, and temperature are the most important input variables when forecasting future drought conditions. Additionally, a case study of the 2017 Northern Plains Flash Drought shows that DroughtCast was able to forecast a very extreme drought event up to 12 weeks before its onset. Given the favorable forecasting skill of the model, DroughtCast may provide a promising tool for land managers and local governments in preparing for and mitigating the effects of drought.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.3389/fdata.2021.773478",NA,NA,NA
"rayyan-100677907","Learning to Correct Climate Projection Biases",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract The fidelity of climate projections is often undermined by biases in climate models due to their simplification or misrepresentation of unresolved climate processes. While various bias correction methods have been developed to post‐process model outputs to match observations, existing approaches usually focus on limited, low‐order statistics, or break either the spatiotemporal consistency of the target variable, or its dependency upon model resolved dynamics. We develop a Regularized Adversarial Domain Adaptation (RADA) methodology to overcome these deficiencies, and enhance efficient identification and correction of climate model biases. Instead of pre‐assuming the spatiotemporal characteristics of model biases, we apply discriminative neural networks to distinguish historical climate simulation samples and observation samples. The evidences based on which the discriminative neural networks make distinctions are applied to train the domain adaptation neural networks to bias correct climate simulations. We regularize the domain adaptation neural networks using cycle‐consistent statistical and dynamical constraints. An application to daily precipitation projection over the contiguous United States shows that our methodology can correct all the considered moments of daily precipitation at approximately resolution, ensures spatiotemporal consistency and inter‐field correlations, and can discriminate between different dynamical conditions. Our methodology offers a powerful tool for disentangling model parameterization biases from their interactions with the chaotic evolution of climate dynamics, opening a novel avenue toward big‐data enhanced climate predictions.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1029/2021ms002509",NA,NA,NA
"rayyan-100677916","Flash Drought: Review of Concept, Prediction and the Potential for Machine Learning, Deep Learning Methods",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract This paper reviews the Flash Drought concept, the uncertainties associated with FD prediction, and the potential of Machine Learning (ML) and Deep learning (DL) for future applications. For this, 121 relevant articles covering different aspects of FD ‐ definitions, key indicators, distinguishing characteristics, and the current methods for FD assessment (i.e., ‐ monitoring, prediction, and impact assessment) are examined. FD is typically a short‐term drought event ‐ characterized by the rapid progression of heat waves and precipitation deficits, causing cascading impacts on the land and surface hydrology. FD prediction is constrained by the lack of consistent FD definitions, key indicators, the limited predictability of FD at the subseasonal‐ to‐seasonal (S2S) timescale, and uncertainties associated with the current prediction methods. Some of the uncertainties in the current methods are associated with a lack of our understanding of the physical processes. They are also related to the error in the input datasets (imperfect representation of indicators), parameter uncertainty (parameterization scheme adopted by the prediction model), multicollinearity, nonlinear, and non‐stationary interactions among different indicators. Combining traditional methods and multisource fusion data with ML and DL methods shows promise to better understand FD evolution and improves prediction.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1029/2022ef002723",NA,NA,NA
"rayyan-100677921","Active Learning Based Federated Learning for Waste and Natural Disaster Image Classification",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"The feasibility of Federated Learning (FL) is highly dependent on the training and inference capabilities of local models, which are subject to the availability of meaningful and annotated data. The availability of such data is in turn contingent on the tedious and time-consuming annotation job that typically requires the manual analysis of training samples. Active Learning (AL) provides an alternative solution allowing a Machine Learning (ML) model to automatically choose and label the data from which it learns without involving manual inspection of each training sample. In this work, we explore how FL can benefit from unlabelled data available at each participating client using AL. To this aim, we propose an AL-based FL framework by employing and evaluating several AL methods in two different application domains. Through an extensive experimentation setup, we show that AL is equally useful in federated and centralized learning by achieving comparable results with manually labeled data using fewer samples without involving human annotators in collecting training data. We also demonstrated that the proposed method is dataset/application independent by evaluating the proposed method in two interesting applications, namely natural disaster analysis and waste classification, having different properties and challenges. Promising results are obtained on both applications resulting in comparable results against the best-case scenario where each sample is manually analyzed and annotated (Baseline 1), and improvement of 3.1% and 4% with best methods respectively over the training sets with irrelevant images on natural disaster and waste classification datasets (Baseline 2).","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1109/access.2020.3038676",NA,NA,NA
"rayyan-100677924","Deep learning-based aerial image segmentation with open data for disaster impact assessment",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Satellite images are an extremely valuable resource in the aftermath of natural disasters such as hurricanes and tsunamis where they can be used for risk assessment and disaster management. In order to provide timely and actionable information for disaster response, a framework utilising segmentation neural networks is proposed to identify impacted areas and accessible roads in post-disaster scenarios. The effectiveness of pretraining with ImageNet -for the task of aerial image segmentation has been analysed and performances of popular segmentation models compared. Experimental results show that pretraining on ImageNet usually improves the segmentation performance for a number of models. Open data available from OpenStreetMap (OSM) is used for training, forgoing the need for time-consuming manual annotation. The method also makes use of graph theory to update road network data available from OSM and to detect the changes caused by a natural disaster. Extensive experiments on data from the 2018 tsunami that struck Palu, Indonesia show the effectiveness of the proposed framework. ENetSeparable, with 30% fewer parameters compared to ENet, achieved comparable segmentation results to that of the state-of-the-art networks.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1016/j.neucom.2020.02.139",NA,NA,NA
"rayyan-100677927","Global Wildfire Susceptibility Mapping Based on Machine Learning Models",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Wildfires are a major natural hazard that lead to deforestation, carbon emissions, and loss of human and animal lives every year. Effective predictions of wildfire occurrence and burned areas are essential to forest management and firefighting. In this paper we apply various machine learning (ML) methods on a 0.25° monthly resolution global dataset of wildfires. We test the prediction accuracies of four different fire occurrence classifiers: random forest (RF), eXtreme Gradient Boosting (XGBoost), multilayer perceptron (MLP) neural network, and a logistic regression. Our best ML model predicts wildfire occurrence with over 90% accuracy, compared to approximately 70% using a logistic regression. We then train ML regression models to predict the size of burned areas and obtain an MAE score of 3.13 km2, compared to 7.48 km2 using a linear regression. To the best of our knowledge, this is the first study to be conducted in such resolution on a global dataset. We use the developed models to shed light on the influence of various factors on wildfire occurrence and burned areas. We suggest building upon these results to create ML-based fire weather indices.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.3390/f13071050",NA,NA,NA
"rayyan-100677935","IoT-Enabled Flood Severity Prediction via Ensemble Machine Learning Models",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"River flooding is a natural phenomenon that can have a devastating effect on human life and economic losses. There have been various approaches in studying river flooding; however, insufficient understanding and limited knowledge about flooding conditions hinder the development of prevention and control measures for this natural phenomenon. This paper entails a new approach for the prediction of water level in association with flood severity using the ensemble model. Our approach leverages the latest developments in the Internet of Things (IoT) and machine learning for the automated analysis of flood data that might be useful to prevent natural disasters. Research outcomes indicate that ensemble learning provides a more reliable tool to predict flood severity levels. The experimental results indicate that the ensemble learning using the Long-Short Term memory model and random forest outperformed individual models with a sensitivity, specificity and accuracy of 71.4%, 85.9%, 81.13%, respectively.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1109/access.2020.2986090",NA,NA,NA
"rayyan-100677945","Predicting global patterns of long-term climate change from short-term simulations using machine learning",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract Understanding and estimating regional climate change under different anthropogenic emission scenarios is pivotal for informing societal adaptation and mitigation measures. However, the high computational complexity of state-of-the-art climate models remains a central bottleneck in this endeavour. Here we introduce a machine learning approach, which utilises a unique dataset of existing climate model simulations to learn relationships between short-term and long-term temperature responses to different climate forcing scenarios. This approach not only has the potential to accelerate climate change projections by reducing the costs of scenario computations, but also helps uncover early indicators of modelled long-term climate responses, which is of relevance to climate change detection, predictability, and attribution. Our results highlight challenges and opportunities for data-driven climate modelling, especially concerning the incorporation of even larger model datasets in the future. We therefore encourage extensive data sharing among research institutes to build ever more powerful climate response emulators, and thus to enable faster climate change projections.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1038/s41612-020-00148-5",NA,NA,NA
"rayyan-100677948","The analysis of similarities between the European Union countries in terms of the level and structure of the emissions of selected gases and air pollutants into the atmosphere",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Based on the newly adopted strategy ""The European Green Deal"", by 2050, the European Union should become the first climate neutral region worldwide. This very ambitious goal will require many political, social and economic activities. Huge financial resources will also be needed to change the economy in order to reduce the emissions of harmful substances into the environment. The implementation of such an ambitious climate policy requires the development of a very reasonable economic plan, backed by many analyses, to ensure adequate financing of this idea. One of the basic objectives of such a plan should be to appropriately target aid funds to a group of countries with a similar structure of the emissions in question. The identification of the groups of similar countries in terms of the structure of harmful substance emissions requires the development of both appropriate methodology and applicable studies. Such methodology is presented in this paper, namely the Kohonen's artificial neural network model. The main objective of the developed methodology was to divide the European Union countries into groups similar in terms of the emissions of selected gases and dusts into the atmosphere. In addition to the division of the European Union countries into similar groups by the total volume of the emissions of studied substances, completely new division criteria were introduced. It was assumed that in order for the results of this study to be practically used, it is necessary to broaden the scope of the analysis. Therefore, an additional division of the European Union countries was made in relation to the volume of the emissions per capita, the value of gross domestic product and the area of a given country. This new approach was intended to show the diversity of the European Union countries in economic, demographic and geographical terms. The grouping results should be regarded as additional information to be utilized when preparing specific action plans to improve the state of the environment. Definitely, these plans need to be dedicated both to the groups of countries and the entire sectors in these groups. This will enable the efficient use of financial resources and can be a huge impetus for the European Union economic development. It will also allow smaller and less prosperous countries to achieve their goals. Undoubtedly, the developed methodology and conducted research allowed the authors to solve a significant research problem, and the results can be successfully used in practice.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.1016/j.jclepro.2020.123641",NA,NA,NA
"rayyan-100677950","AI-based risk assessment for construction site disaster preparedness through deep learning-based digital twinning",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Hurricanes are among the most devastating natural disasters in the United States, causing billions of dollars of property damage and insured losses. During extreme wind events, unsecured objects in jobsites can easily become airborne debris, which results in substantial loss to construction projects and neighboring communities. Towards a systematic disaster preparedness in construction jobsites, this paper presents a novel vision-based digital twinning and threat assessment framework. We encode the context of disaster risk into deep-learning architectures to identify and analyze the characteristics and impacts of potential wind-borne debris in construction site digital twin models. Case studies on nine piles of construction materials are presented to demonstrate and discuss the fidelity of the proposed computational modules. The proposed methods are expected to help provide heads up for practitioners to quickly recognize, localize, and assess potential wind-borne derbies in construction jobsites, and thereby implementing hurricane preparedness in an effective and timely manner.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1016/j.autcon.2021.104091",NA,NA,NA
"rayyan-100677954","Flood susceptibility prediction using four machine learning techniques and comparison of their performance at Wadi Qena Basin, Egypt",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Floods represent catastrophic environmental hazards that have a significant impact on the environment and human life and their activities. Environmental and water management in many countries require modeling of flood susceptibility to help in reducing the damages and impact of floods. The objective of the current work is to employ four data mining/machine learning models to generate flood susceptibility maps, namely boosted regression tree (BRT), functional data analysis (FDA), general linear model (GLM), and multivariate discriminant analysis (MDA). This study was done in Wadi Qena Basin in Egypt. Flood inundated locations were determined and extracted from the interpretation of different datasets, including high-resolution satellite images (sentinel-2 and Astro digital) (after flood events), historical records, and intensive field works. In total, 342 flood inundated locations were mapped using ArcGIS 10.5, which separated into two groups; training (has 239 flood locations represents 70%) and validating (has 103 flood locations represents 30%), respectively. Nine themes of flood-influencing factors were prepared, including slope-angle, slope length, altitude, distance from main wadis, landuse/landcover, lithological units, curvature, slope-aspect, and topographic wetness index. The relationships between the flood-influencing factors and the flood inventory map were evaluated using the mentioned models (BRT, FDA, GLM, and MDA). The results were compared with flood inundating locations (validating flood sites), which were not used in constructing the models. The accuracy of the models was calculated through the success (training data) and prediction (validation data) rate curves according to the receiver operating characteristics (ROC) and the area under the curve (AUC). The results showed that the AUC for success and prediction rates are 0.783, 0.958, 0.816, 0.821 and 0.812, 0.856, 0.862, 0.769 for BRT, FDA, GLM, and MDA models, respectively. Subsequently, flood susceptibility maps were divided into five classes, including very low, low, moderate, high, and very high susceptibility. The results revealed that the BRT, FDA, GLM, and MDA models provide reasonable accuracy in flood susceptibility mapping. The produced susceptibility maps might be vitally important for future development activities in the area, especially in choosing new urban areas, infrastructural activities, and flood mitigation areas.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1007/s11069-020-04296-y",NA,NA,NA
"rayyan-100677960","Failure Mechanism and Long Short-Term Memory Neural Network Model for Landslide Risk Prediction",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Rockslides along a stepped failure surface have characteristics of stepped deformation characteristic and it is difficult to predict the failure time. In this study, the deformation characteristics and disaster prediction model of the Fengning granite rockslide were analyzed based on field surveys and monitoring data. To evaluate the stability, the shear strength parameters of the sliding surface were determined based on the back-propagation neural network and three-dimensional discrete element numerical method. Through the correlation analysis of deformation monitoring results with rainfall and blasting, it is shown that the landslide was triggered by excavation, rainfall, and blasting vibrations. The landslide displacement prediction model was established by using long short-term memory neural network (LSTM) based on the monitoring data, and the prediction results are compared with those using the BP model, SVM model and ARMA model. Results show that the LSTM model has strong advantages and good reliability for the stepped landslide deformation with short-term influence, and the predicted LSTM values were very consistent with the measured values, with a correlation coefficient of 0.977. Combined with the distribution characteristics of joints, the damage influence scope of the landslide was simulated by three-dimensional discrete element, which provides decision-making basis for disaster warning after slope instability. The method proposed in this paper can provide references for early warning and treatment of geological disasters.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.3390/rs14010166",NA,NA,NA
"rayyan-100677963","The complex dynamics of earthquake fault systems: new approaches to forecasting and nowcasting of earthquakes",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate | USER-NOTES: {""Maria""=>[""Charles Richter's observation that 'only fools and charlatans predict earthquakes,' reflects the fact that despite more than 100 years of effort, seismologists remain unable to do so with reliable and accurate results. Meaningful prediction involves specifying the location, time, and size of an earthquake before it occurs to greater precision than expected purely by chance from the known statistics of earthquakes in an area. In this context, 'forecasting' implies a prediction with a specification of a probability of the time, location, and magnitude. Two general approaches have been used. In one, the rate of motion accumulating across faults and the amount of slip in past earthquakes is used to infer where and when future earthquakes will occur and the shaking that would be expected."", ""Because the intervals between earthquakes are highly variable, these long-term forecasts are accurate to no better than a hundred years. They are thus valuable for earthquake hazard mitigation, given the long lives of structures, but have clear limitations. The second approach is to identify potentially observable changes in the Earth that precede earthquakes. Various precursors have been suggested, and may have been real in certain cases, but none have yet proved to be a general feature preceding all earthquakes or to stand out convincingly from the normal variability of the Earth's behavior. However, new types of data, models, and computational power may provide avenues for progress using machine learning that were not previously available. At present, it is unclear whether deterministic earthquake prediction is possible."", ""The frustrations of this search have led to the observation that (echoing Yogi Berra) 'it is difficult to predict earthquakes, especially before they happen.' However, because success would be of enormous societal benefit, the search for methods of earthquake prediction and forecasting will likely continue. In this review, we note that the focus is on anticipating the earthquake rupture before it occurs, rather than characterizing it rapidly just after it occurs. The latter is the domain of earthquake early warning, which we do not treat in detail here, although we include a short discussion in the machine learning section at the end.""]}","https://doi.org/10.1088/1361-6633/abf893",NA,NA,NA
"rayyan-100677976","Constructing high-resolution groundwater drought at spatio-temporal scale using GRACE satellite data based on machine learning in the Indus Basin",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"The complicated phenomenon induced by inadequate precipitation is a drought that impacts water resources and human life. Traditional methods to assess groundwater drought events are hindered due to sparse groundwater observations on a spatio-temporal scale. These groundwater drought events are not well studied in the study area of the Indus Basin Irrigation System (IBIS) holistically. This study applied four machine learning models to the training datasets of Gravity Recovery and Climate Experiment (GRACE) Terrestrial Water Storage (TWS) and Groundwater Storage (GWS) data to improve resolution to 0.25° from 1°. The Extreme Gradient Boosting (XGBoost) model outperformed the four models and results showed Pearson correlation (R) (0.99), Nash Sutcliff Efficiency (NSE) (0.99), Root Mean Square Error (RMSE) (5.22 mm), and Mean Absolute Error (MAE) (2.75 mm). The GRACE Groundwater Drought Index (GGDI) was calculated by normalizing XGBoost-downscaled GWS. The trend characteristics, the temporal evolution, and spatial distribution of GGDI were analyzed across the IBIS from 2003 to 2016. The wavelet coherence approach was used to evaluate the relationship between teleconnection factors and GGDI. The XGBoost downscaling model can accurately reproduce local groundwater behavior, with the acceptable correlation of coefficient values for validation (ranging from 0.02 to 0.84). The accumulated Standardized Precipitation Evapotranspiration Index (SPEI) with the time of 1, 3, and 6 months, and self-calibrated Palmer Drought Severity Index (sc-PDSI) were used to validate GGDI. The findings have demonstrated that GGDI has comparable drought patterns to SPEI-3 and SPEI-6 and sc-PDSI. The teleconnection factors have a significant impact on the GGDI shown by the wavelet coherence technique. The impact of the sea surface temperature index (namely, NINO3.4) on GGDI was observed significantly high among other teleconnection factors in the IBIS. The proposed framework can serve as a useful tool for drought monitoring and a better understanding of extreme hydroclimatic conditions in the IBIS and other similar climatic regions.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1016/j.jhydrol.2022.128295",NA,NA,NA
"rayyan-100677978","Moving towards improved surveillance and earlier diagnosis of aquatic pathogens: From traditional methods to emerging technologies",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract Early and accurate diagnosis is key to mitigating the impact of infectious diseases, along with efficient surveillance. This however is particularly challenging in aquatic environments due to hidden biodiversity and physical constraints. Traditional diagnostics, such as visual diagnosis and histopathology, are still widely used, but increasingly technological advances such as portable next generation sequencing (NGS) and artificial intelligence (AI) are being tested for early diagnosis. The most straightforward methodologies, based on visual diagnosis, rely on specialist knowledge and experience but provide a foundation for surveillance. Future computational remote sensing methods, such as AI image diagnosis and drone surveillance, will ultimately reduce labour costs whilst not compromising on sensitivity, but they require capital and infrastructural investment. Molecular techniques have advanced rapidly in the last 30 years, from standard PCR through loop‐mediated isothermal amplification (LAMP) to NGS approaches, providing a range of technologies that support the currently popular eDNA diagnosis. There is now vast potential for transformative change driven by developments in human diagnostics. Here we compare current surveillance and diagnostic technologies with those that could be used or developed for use in the aquatic environment, against three gold standard ideals of high sensitivity, specificity, rapid diagnosis, and cost‐effectiveness.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.1111/raq.12674",NA,NA,NA
"rayyan-100677981","Self-Supervised Multisensor Change Detection",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Most change detection methods assume that pre-change and post-change images are acquired by the same sensor. However, in many real-life scenarios, e.g., natural disaster, it is more practical to use the latest available images before and after the occurrence of incidence, which may be acquired using different sensors. In particular, we are interested in the combination of the images acquired by optical and Synthetic Aperture Radar (SAR) sensors. SAR images appear vastly different from the optical images even when capturing the same scene. Adding to this, change detection methods are often constrained to use only target image-pair, no labeled data, and no additional unlabeled data. Such constraints limit the scope of traditional supervised machine learning and unsupervised generative approaches for multi-sensor change detection. Recent rapid development of self-supervised learning methods has shown that some of them can even work with only few images. Motivated by this, in this work we propose a method for multi-sensor change detection using only the unlabeled target bi-temporal images that are used for training a network in self-supervised fashion by using deep clustering and contrastive learning. The proposed method is evaluated on four multi-modal bi-temporal scenes showing change and the benefits of our self-supervised approach are demonstrated.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1109/tgrs.2021.3109957",NA,NA,NA
"rayyan-100677985","Nowcasting extreme rain and extreme wind speed with machine learning techniques applied to different input datasets",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Predicting extreme weather events in a short time period and their developing in localized areas is a challenge. The nowcasting of severe and extreme weather events is an issue for air traffic management and control because it affects aviation safety, and determines delays and diversions. This work is part of a larger study devoted to nowcasting rain and wind speed in the area of Malpensa airport by merging different datasets. We use as reference the weather station of Novara to develop a nowcasting machine learning model which could be reusable in other locations. In this location we have the availability of ground-based weather sensors, a Global Navigation Satellite System (GNSS) receiver, a C-band radar and lightning detectors. Our analysis shows that the Long Short-Term Memory Encoder Decoder (LSTM E/D) approach is well suited for the nowcasting of meteorological variables. The predictions are based on 4 different datasets configurations providing rain and wind speed nowcast for 1 h with a time step of 10 min. The results are very promising with the extreme wind speed probability of detection higher than 90%, the false alarms lower than 2%, and a good performance in extreme rain detection for the first 30 min. The configuration using just weather stations and GNSS data in input provides excellent performances and should be preferred to the other ones, since it refers to the pre-convective environment, and thus can be adaptable to any weather conditions.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1016/j.atmosres.2022.106548",NA,NA,NA
"rayyan-100677987","From Atmospheric Waves to Heatwaves: A Waveguide Perspective for Understanding and Predicting Concurrent, Persistent, and Extreme Extratropical Weather",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate | USER-NOTES: {""Maria""=>[""A notable number of high-impact weather extremes have occurred in recent years, often associated with persistent, strongly meandering atmospheric circulation patterns known as Rossby waves. Because of the high societal and ecosystem impacts, it is of great interest to be able to accurately project how such extreme events will change with climate change, and to predict these events on seasonal-to-subseasonal (S2S) time scales. There are multiple physical links connecting upper-atmosphere circulation patterns to surface weather extremes, and it is asking a lot of our dynamical models to accurately simulate all of these. Subsequently, our confidence in future projections and S2S forecasts of extreme events connected to Rossby waves remains relatively low."", ""We also lack full fundamental theories for the growth and propagation of Rossby waves on the spatial and temporal scales relevant to extreme events, particularly under strongly nonlinear conditions. By focusing on one of the first links in the chain from upper-atmospheric conditions to surface extremes—the Rossby waveguide—it may be possible to circumvent some model biases in later links. To further our understanding of the nature of waveguides, links to persistent surface weather events and their representation in models, we recommend exploring these links in model hierarchies of increasing complexity, developing fundamental theory, exploiting novel large ensemble datasets, harnessing deep learning, and increased community collaboration. "", ""This would help increase understanding and confidence in both S2S predictions of extremes and of projections of the impact of climate change on extreme weather events.""]}","https://doi.org/10.1175/bams-d-21-0170.1",NA,NA,NA
"rayyan-100677989","Typhoon Intensity Forecasting Based on LSTM Using the Rolling Forecast Method",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"A typhoon is an extreme weather event with strong destructive force, which can bring huge losses of life and economic damage to people. Thus, it is meaningful to reduce the prediction errors of typhoon intensity forecasting. Artificial and deep neural networks have recently become widely used for typhoon forecasting in order to ensure typhoon intensity forecasting is accurate and timely. Typhoon intensity forecasting models based on long short-term memory (LSTM) are proposed herein, which forecast typhoon intensity as a time series problem based on historical typhoon data. First, the typhoon intensity forecasting models are trained and tested with processed typhoon data from 2000 to 2014 to find the optimal prediction factors. Then, the models are validated using the optimal prediction factors compared to a feed-forward neural network (FNN). As per the results of the model applied for typhoons Chan-hom and Soudelor in 2015, the model based on LSTM using the optimal prediction factors shows the best performance and lowest prediction errors. Thus, the model based on LSTM is practical and meaningful for predicting typhoon intensity within 120 h.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.3390/a14030083",NA,NA,NA
"rayyan-100677996","Identification of Microplastics Based on the Fractal Properties of Their Holographic Fingerprint",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene | USER-NOTES: {""Maria""=>[""Water plastic pollution is a serious problem affecting sealife, marine habitats, and the food chain. Artificial intelligence-enabled coherent imaging has recently shown exciting advances in the field of environmental monitoring, and portable holographic microscopes are good candidates to map the microparticles content of marine waters. The “holographic fingerprint” due to coherent light diffraction is rich in information, fully encoded into the complex wavefront scattered by the sample. Hence, proper analysis of the wavefronts reconstructed from digital holograms can unlock new possibilities in the fields of diagnostics and environmental monitoring. "", ""Fractal geometry well describes natural objects and allows inferring added-value information on the way these fill 2D spaces and 3D volumes. The most abundant micron-scale class of objects that populate marine waters consists of microalgae named diatoms, which are of interest as bioindicators of water quality. Here we investigate the fractal properties of holographic patterns of diatoms and microplastics, considering a heterogeneous mixture of five types of plastic materials and 55 different species of microalgae. "", ""We show that, different from the case of weak scattering objects, a small set of fractal parameters is able to characterize these two large ensembles. As an applicative example, we carry out classification tests to show the possibility to identify the two classes with high accuracy. This new holographic fractal description of scattering micro-objects could be used in the near future for in situ automatic mapping of microplastic pollutants and for taxonomy of diatoms as water quality bioindicators, screened onboard holographic systems.""]}","https://doi.org/10.1021/acsphotonics.1c00591",NA,NA,NA
"rayyan-100678005","Machine Learning for Climate Precipitation Prediction Modeling over South America",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Many natural disasters in South America are linked to meteorological phenomena. Therefore, forecasting and monitoring climatic events are fundamental issues for society and various sectors of the economy. In the last decades, machine learning models have been developed to tackle different issues in society, but there is still a gap in applications to applied physics. Here, different machine learning models are evaluated for precipitation prediction over South America. Currently, numerical weather prediction models are unable to precisely reproduce the precipitation patterns in South America due to many factors such as the lack of region-specific parametrizations and data availability. The results are compared to the general circulation atmospheric model currently used operationally in the National Institute for Space Research (INPE: Instituto Nacional de Pesquisas Espaciais), Brazil. Machine learning models are able to produce predictions with errors under 2 mm in most of the continent in comparison to satellite-observed precipitation patterns for different climate seasons, and also outperform INPE’s model for some regions (e.g., reduction of errors from 8 to 2 mm in central South America in winter). Another advantage is the computational performance from machine learning models, running faster with much lower computer resources than models based on differential equations currently used in operational centers. Therefore, it is important to consider machine learning models for precipitation forecasts in operational centers as a way to improve forecast quality and to reduce computation costs.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.3390/rs13132468",NA,NA,NA
"rayyan-100678010","Robust Meteorological Drought Prediction Using Antecedent SST Fluctuations and Machine Learning",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract While reliable drought prediction is fundamental for drought mitigation and water resources management, it is still a challenge to develop robust drought prediction models due to complex local hydro‐climatic conditions and various predictors. Sea surface temperature (SST) is considered as the fundamental predictor to develop drought prediction models. However, traditional models usually extract SST signals from one or several specific sea zones within a given time span, which limits full use of SST signals for drought prediction. Here, we introduce a new meteorological drought prediction approach by using the antecedent SST fluctuation pattern (ASFP) and machine learning techniques (e.g., support vector regression (SVR), random forest (RF), and extreme learning machine (ELM)). Three models (i.e., ASFP‐SVR, ASFP‐ELM, and ASFP‐RF) are developed for ensemble, probability, and deterministic drought predictions. The Colorado, Danube, Orange, and Pearl River basins with frequent droughts over different continents are selected, as the cases, where standardized precipitation evapotranspiration index (SPEI) are predicted at the 1° × 1° resolution with 1‐ and 3‐month lead times. Results show that the ASFP‐ELM model can effectively predict space‐time evolutions of drought events with satisfactory skills, outperforming the ASFP‐SVR and ASFP‐RF models. Our study has potential to provide a reliable tool for drought prediction, which further supports the development of drought early warning systems.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1029/2020wr029413",NA,NA,NA
"rayyan-100678014","Climbing down Charney’s ladder: machine learning and the post-Dennard era of computational climate science",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"The advent of digital computing in the 1950s sparked a revolution in the science of weather and climate. Meteorology, long based on extrapolating patterns in space and time, gave way to computational methods in a decade of advances in numerical weather forecasting. Those same methods also gave rise to computational climate science, studying the behaviour of those same numerical equations over intervals much longer than weather events, and changes in external boundary conditions. Several subsequent decades of exponential growth in computational power have brought us to the present day, where models ever grow in resolution and complexity, capable of mastery of many small-scale phenomena with global repercussions, and ever more intricate feedbacks in the Earth system. The current juncture in computing, seven decades later, heralds an end to what is called Dennard scaling, the physics behind ever smaller computational units and ever faster arithmetic. This is prompting a fundamental change in our approach to the simulation of weather and climate, potentially as revolutionary as that wrought by John von Neumann in the 1950s. One approach could return us to an earlier era of pattern recognition and extrapolation, this time aided by computational power. Another approach could lead us to insights that continue to be expressed in mathematical equations. In either approach, or any synthesis of those, it is clearly no longer the steady march of the last few decades, continuing to add detail to ever more elaborate models. In this prospectus, we attempt to show the outlines of how this may unfold in the coming decades, a new harnessing of physical knowledge, computation and data. This article is part of the theme issue ‘Machine learning for weather and climate modelling’.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1098/rsta.2020.0085",NA,NA,NA
"rayyan-100678029","Observation‐Constrained Projection of Global Flood Magnitudes With Anthropogenic Warming",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate | USER-NOTES: {""Maria""=>[""River flooding is among the costliest natural disasters with severe economic, societal, and environmental consequences. However, substantial uncertainties remain in global and regional projections of future flood conditions simulated by global climate models (GCMs) and/or global hydrological models (GHMs). Using physical models coupled with machine learning (ML), for the first time, we project changes in flood magnitudes of 2062 global river basins by constraining physical-based streamflow simulations with observations under 1.5°C and 2°C warming scenarios identified for the Representative Concentration Pathway 8.5."", ""We found that, during the validation period, the GHMs-simulated flood magnitudes would improve with reduced uncertainty over the selected river basins after ML with a Long Short-Term Memory network. Our estimation suggested that flood magnitudes would increase in many Northern Hemisphere mid- and high-latitude rivers (e.g., Lena River, Amur River and Volga River) but decrease in some river basins in southern Finland and Eastern Europe in future periods (i.e., 1.5°C and 2°C warming levels). "", ""In 1.5°C and 2°C warmer worlds, the decreasing flood magnitudes in most South American rivers are associated with decreased soil moisture and increased evapotranspiration induced by warmer temperatures. Although the geographical pattern of changes in flood magnitudes for the +2°C experiment is close to that of the +1.5°C experiment, a 1.5°C warming target is more likely to reduce flood magnitudes of many river basins worldwide (e.g., in central and eastern Siberia, Alaska/Northwest Canada and South America).""]}","https://doi.org/10.1029/2020wr028830",NA,NA,NA
"rayyan-100678034","Rapid characterisation of landslide heterogeneity using unsupervised classification of electrical resistivity and seismic refraction surveys",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"The characterisation of the subsurface of a landslide is a critical step in developing ground models that inform planned mitigation measures, remediation works or future early-warning of instability. When a landslide failure may be imminent, the time pressures on producing such models may be great. Geoelectrical and seismic geophysical surveys are able to rapidly acquire volumetric data across large areas of the subsurface at the slope-scale. However, analysis of the individual model derived from each survey is typically undertaken in isolation, and a robust, accurate interpretation is highly dependent on the experience and skills of the operator. We demonstrate a machine learning process for constructing a rapid reconnaissance ground model, by integrating several sources of geophysical data in to a single ground model in a rapid and objective manner. Firstly, we use topographic data acquired by a UAV survey to co-locate three geophysical surveys of the Hollin Hill Landslide Observatory in the UK. The data are inverted using a joint 2D mesh, resulting in a set of co-located models of resistivity, P-wave velocity and S-wave velocity. Secondly, we analyse the relationships and trends present between the variables for each point in the mesh (resistivity, P-wave velocity, S-wave velocity, depth) to identify correlations. Thirdly, we use a Gaussian Mixture Model (GMM), a form of unsupervised machine learning, to classify the geophysical data into cluster groups with similar ranges and trends in measurements. The resulting model created from probabilistically assigning each subsurface point to a cluster group characterises the heterogeneity of landslide materials based on their geophysical properties, identifying the major subsurface discontinuities at the site. Finally, we compare the results of the cluster groups to intrusive borehole data, which show good agreement with the spatial variations in lithology. We demonstrate the applicability of integrated geophysical surveys coupled with simple unsupervised machine learning for producing rapid reconnaissance ground models in time-critical situations with minimal prior knowledge about the subsurface.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1016/j.enggeo.2021.106189",NA,NA,NA
"rayyan-100678035","Drought risk assessment: integrating meteorological, hydrological, agricultural and socio-economic factors using ensemble models and geospatial techniques",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate | USER-NOTES: {""Maria""=>[""Among natural disasters, drought hits almost half of the world every year, regardless of the climatic zones. Identifying drought vulnerability regions is fundamental to plan and adopt mitigation measures. Here we apply a multi-criteria-based machine learning technique that integrates spatial data for preparing drought vulnerability map of different categories. We adopted remote sensing tools with three machine learning models namely support vector machine (SVM), random forest (RF) and support vector regression (SVR) and their ensembles (i.e. Bagging, Boosting and Stacking), as applied to the northwestern part of Iran as a case study. Various types of geo-environmental factors were considered including meteorological, hydrological, agricultural and socio-economic. "", ""The result of the model was evaluated through arithmetic logic values (area under the curve [AUC]) under the receiver operating curve (ROC). Through multi-collinearity test, the prominent causative factors for the occurrences of drought are defined. The AUC value from ROC of SVR-Stacking, RF-Stacking and SVM-Stacking model for training datasets are 0.942, 0.918 and 0.896, respectively. The SVR-Stacking yielded the best result (AUC = 0.94) confirming that SVR serves as a robust model for the preparation of drought susceptibility maps that can be used by governmental and other administrative agencies.""]}","https://doi.org/10.1080/10106049.2021.1926558",NA,NA,NA
"rayyan-100678037","Using machine learning to predict fire‐ignition occurrences from lightning forecasts",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract Lightning‐caused wildfires are a significant contributor to burned areas, with lightning ignitions remaining one of the most unpredictable aspects of the fire environment. There is a clear connection between fuel moisture and the probability of ignition; however, the mechanisms are poorly understood and predictive methods are underdeveloped. Establishing a lightning–ignition relationship would be useful in developing a model that would complement early warning systems designed for fire control and prevention. A machine learning (ML) approach was used to define a predictive model for wildfire ignition based on lightning forecasts and environmental conditions. Three different binary classifiers were adopted: a decision tree, an AdaBoost and a Random Forest, showing promising results, with both ensemble methods (Random Forest and AdaBoost) exhibiting an out‐of‐sample accuracy of 78%. Data provided by a Western Australia wildfire database allowed a comprehensive verification on over 145 lightning‐ignited wildfires in regions of Australia during 2016. This highlighted that in a minimum of 71% of the cases the ML models correctly predicted the occurrence of an ignition when a fire was actually initiated. The super‐learner developed is planned to be used in an operational context to the enhance information connected to fire management.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1002/met.1973",NA,NA,NA
"rayyan-100678043","Real-Time Forest Fire Detection Framework Based on Artificial Intelligence Using Color Probability Model and Motion Feature Analysis",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"As part of the early warning system, forest fire detection has a critical role in detecting fire in a forest area to prevent damage to forest ecosystems. In this case, the speed of the detection process is the most critical factor to support a fast response by the authorities. Thus, this article proposes a new framework for fire detection based on combining color-motion-shape features with machine learning technology. The characteristics of the fire are not only red but also from their irregular shape and movement that tends to be constant at specific locations. These characteristics are represented by color probabilities in the segmentation stage, color histograms in the classification stage, and image moments in the verification stage. A frame-based evaluation and an intersection over union (IoU) ratio was applied to evaluate the proposed framework. Frame-based evaluation measures the performance in detecting fires. In contrast, the IoU ratio measures the performance in localizing the fires. The experiment found that the proposed framework produced 89.97% and 10.03% in the true-positive rate and the false-negative rate, respectively, using the VisiFire dataset. Meanwhile, the proposed method can obtain an average of 21.70 FPS in processing time. These results proved that the proposed method is fast in the detection process and can maintain performance accuracy. Thus, the proposed method is suitable and reliable for integrating into the early warning system.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.3390/fire5010023",NA,NA,NA
"rayyan-100678050","Deep Learning Based Cloud Cover Parameterization for ICON",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"A promising approach to improve cloud parameterizations within climate models and thus climate projections is to use deep learning in combination with training data from storm-resolving model (SRM) simulations. The ICOsahedral Non-hydrostatic (ICON) modeling framework permits simulations ranging from numerical weather prediction to climate projections, making it an ideal target to develop neural network (NN) based parameterizations for sub-grid scale processes. Within the ICON framework, we train NN based cloud cover parameterizations with coarse-grained data based on realistic regional and global ICON SRM simulations. We set up three different types of NNs that differ in the degree of vertical locality they assume for diagnosing cloud cover from coarse-grained atmospheric state variables. The NNs accurately estimate sub-grid scale cloud cover from coarse-grained data that has similar geographical characteristics as their training data. Additionally, globally trained NNs can reproduce sub-grid scale cloud cover of the regional SRM simulation. Using the game-theory based interpretability library SHapley Additive exPlanations, we identify an overemphasis on specific humidity and cloud ice as the reason why our column-based NN cannot perfectly generalize from the global to the regional coarse-grained SRM data. The interpretability tool also helps visualize similarities and differences in feature importance between regionally and globally trained column-based NNs, and reveals a local relationship between their cloud cover predictions and the thermodynamic environment. Our results show the potential of deep learning to derive accurate yet interpretable cloud cover parameterizations from global SRMs, and suggest that neighborhood-based models may be a good compromise between accuracy and generalizability.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1029/2021ms002959",NA,NA,NA
"rayyan-100678052","A deep learning approach using graph convolutional networks for slope deformation prediction based on time-series displacement data",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract Slope deformation prediction is crucial for early warning of slope failure, which can prevent property damage and save human life. Existing predictive models focus on predicting the displacement of a single monitoring point based on time series data, without considering spatial correlations among monitoring points, which makes it difficult to reveal the displacement changes in the entire monitoring system and ignores the potential threats from nonselected points. To address the above problem, this paper presents a novel deep learning method for predicting the slope deformation, by considering the spatial correlations between all points in the entire displacement monitoring system. The essential idea behind the proposed method is to predict the slope deformation based on the global information (i.e., the correlated displacements of all points in the entire monitoring system), rather than based on the local information (i.e., the displacements of a specified single point in the monitoring system). In the proposed method, (1) a weighted adjacency matrix is built to interpret the spatial correlations between all points, (2) a feature matrix is assembled to store the time-series displacements of all points, and (3) one of the state-of-the-art deep learning models, i.e., T-GCN, is developed to process the above graph-structured data consisting of two matrices. The effectiveness of the proposed method is verified by performing predictions based on a real dataset. The proposed method can be applied to predict time-dependency information in other similar geohazard scenarios, based on time-series data collected from multiple monitoring points.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1007/s00521-021-06084-6",NA,NA,NA
"rayyan-100678055","Stable climate simulations using a realistic general circulation model with neural network parameterizations for atmospheric moist physics and radiation processes",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract. In climate models, subgrid parameterizations of convection and clouds are one of the main causes of the biases in precipitation and atmospheric circulation simulations. In recent years, due to the rapid development of data science, machine learning (ML) parameterizations for convection and clouds have been demonstrated to have the potential to perform better than conventional parameterizations. Most previous studies were conducted on aqua-planet and idealized models, and the problems of simulation instability and climate drift still exist. Developing an ML parameterization scheme remains a challenging task in realistically configured models. In this paper, a set of residual deep neural networks (ResDNNs) with a strong nonlinear fitting ability is designed to emulate a super-parameterization (SP) with different outputs in a hybrid ML–physical general circulation model (GCM). It can sustain stable simulations for over 10 years under real-world geographical boundary conditions. We explore the relationship between the accuracy and stability by validating multiple deep neural network (DNN) and ResDNN sets in prognostic runs. In addition, there are significant differences in the prognostic results of the stable ResDNN sets. Therefore, trial and error is used to acquire the optimal ResDNN set for both high skill and long-term stability, which we name the neural network (NN) parameterization. In offline validation, the neural network parameterization can emulate the SP in mid- to high-latitude regions with a high accuracy. However, its prediction skill over tropical ocean areas still needs improvement. In the multi-year prognostic test, the hybrid ML–physical GCM simulates the tropical precipitation well over land and significantly improves the frequency of the precipitation extremes, which are vastly underestimated in the Community Atmospheric Model version 5 (CAM5), with a horizontal resolution of 1.9∘ × 2.5∘. Furthermore, the hybrid ML–physical GCM simulates the robust signal of the Madden–Julian oscillation with a more reasonable propagation speed than CAM5. However, there are still substantial biases with the hybrid ML–physical GCM in the mean states, including the temperature field in the tropopause and at high latitudes and the precipitation over tropical oceanic regions, which are larger than those in CAM5. This study is a pioneer in achieving multi-year stable climate simulations using a hybrid ML–physical GCM under actual land–ocean boundary conditions that become sustained over 30 times faster than the target SP. It demonstrates the emerging potential of using ML parameterizations in climate simulations.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.5194/gmd-15-3923-2022",NA,NA,NA
"rayyan-100678064","On Smart IoT Remote Sensing over Integrated Terrestrial-Aerial-Space Networks: An Asynchronous Federated Learning Approach",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate | USER-NOTES: {""Maria""=>[""While the Internet of Things (IoT), coupled with integrated terrestrial-aerial-space networks, has revolutionized the domain of ubiquitous remote sensing for natural resource management, several research challenges emerge due to the explosion of the collected IoT data. For instance, the edge nodes in these hybrid, next-generation networks are anticipated to carry out edge computing on the collected data to provide localized computing to enable early warning systems, including forest fire occurrence and spread, earth- quake wave detection, tsunami forecasting, and so forth. While edge computing can significantly reduce the high communication time required in the traditional cloud-based remote sensing analytics, it is important to develop a lightweight training framework to obtain smart remote sensing analytics at the edge devices in the integrated network while preserving the privacy of the collected data."", ""In this article, we propose an asynchronously updating federated learning model for the edge nodes to build local artificial intelligence models for smart remote sensing with a forest fire detection use case without the need for explicit data exchange with the cloud. This jointly preserves data privacy and also alleviates the network overhead. Extensive experimental results demonstrate the viability of our proposal in terms of significantly high remote sensing accuracy, low convergence time, and low bandwidth overhead compared to existing methods.""]}","https://doi.org/10.1109/mnet.101.2100125",NA,NA,NA
"rayyan-100678068","Towards an Ensemble Machine Learning Model of Random Subspace Based Functional Tree Classifier for Snow Avalanche Susceptibility Mapping",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Snow avalanche as a natural disaster severely affects socio-economic and geomorphic processes through damaging ecosystems, vegetation, landscape, infrastructures, transportation networks, and human life. Modeling the snow avalanche has been seen as an essential approach for understanding the mountainous landscape dynamics to assess hazard susceptibility leading to effective mitigation and resilience. Therefore, the main aim of this study is to introduce and implement an ensemble machine learning model of random subspace (RS) based on a classifier, functional tree (FT), named RSFT model for snow avalanche susceptibility mapping at Karaj Watershed, Iran. According to the best knowledge of literature, the proposed model, RSFT, has not earlier been introduced and implemented for snow avalanche modeling and mapping over the world. Four benchmark models, including logistic regression (LR), logistic model tree (LMT), alternating decision tree (ADT), and functional trees (FT) models were used to check the goodness-of-fit and prediction accuracy of the proposed model. To achieve this objective, the most important factors among many climatic, topographic, lithologic, and hydrologic factors, which affect the snow accumulation and snow avalanche occurrence, were determined by the information gain ratio (IGR) technique. The goodness-of-fit and prediction accuracy of the models were evaluated by some statistical-based indexes including, sensitivity, specificity, accuracy, kappa, and area under the ROC curve, Friedman and Wilcoxon sign rank tests. Results indicated that the ensemble proposed model (RSFT), had the highest performance (Sensitivity = 94.1%, Specificity = 92.4%, Accuracy = 93.3%, and Kappa = 0.782) rather than the other soft-computing benchmark models. The snow avalanche susceptibility maps indicated that the high and very high susceptibility avalanche areas are located in the north and northeast parts of the study area, which have a higher elevation with more precipitation and lower temperatures.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1109/access.2020.3014816",NA,NA,NA
"rayyan-100678087","Adoption of ICTs in Agri-Food Logistics: Potential and Limitations for Supply Chain Sustainability",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"A major challenge of Sustainable Development Goal 12 “Responsible Consumption and Production” is to reduce food losses along production and supply chains. This is particularly critical for fresh food products, due to their perishable and fragile nature, which makes the coordination of the actors all the more crucial to avoid wastes and losses. The rise of new technologies, referred to as “Industry 4.0” powered by the internet of things, big data analytics and artificial intelligence, could bring new solutions to meet these needs. Information and communication technologies (ICTs) allow for frequent exchanges of huge amounts of information between actors in the agrofood chains to coordinate their activities. The aim of the chapter is to provide a state-of-the-art analysis on ICTs used in agrofood supply chains, with a special focus on the case of fresh fruits and vegetables, to analyze the potential and weaknesses which exist in different forms of supply chains for ICTs becoming a “resource” (precious, rare, non-imitable, and nonsubstitutable) prospect and to suggest promising ICTs in this context.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.3390/su13126702",NA,NA,NA
"rayyan-100678089","The Cloud Is Material: On the Environmental Impacts of Computation and Data Storage",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"In the age of machine learning, cryptocurrency mining, and seemingly infinite data storage capacity enabled by cloud computing, the environmental costs of ubiquitous computing in modern life are obscured by the sheer complexity of infrastructures and supply chains involved in even the simplest of digital transactions. How does computation contribute to the warming of the planet? As information technology (IT) capacity demands continue to trend upward, what are some of the ecological obstacles that must be overcome to accommodate an ever-expanding, carbon-hungry Cloud? How do these material impacts play out in everyday life, behind the scenes, where servers, fiber optic cables, and technicians facilitate cloud services? This case study draws on firsthand ethnographic research in data centers—sprawling libraries of computer servers that facilitate everything from email to commerce—to identify some of the far-reaching and tangled environmental impacts of computation and data-storage infrastructures. It surveys a range of empirical accounts of server technicians to illustrate on-the-ground examples of material and ecological factors that permeate everyday life in the Cloud. These examples include air conditioning and thermal management, water cycling, and the disposal of e-waste. By attending to the culture of workplace practice and the behaviors and training of technicians in data centers, this case study reveals that the Cloud is not fully automated, nor is it hyperrational; emotion, instinct, and human judgment are enlisted to keep servers running. This case study closes with a speculative vignette that scales up from various local impacts to a planetary framework, sketching some of the particular ways that computation contributes to climate change and the Anthropocene.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.21428/2c646de5.031d4553",NA,NA,NA
"rayyan-100678097","Multivariate random forest prediction of poverty and malnutrition prevalence",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Advances in remote sensing and machine learning enable increasingly accurate, inexpensive, and timely estimation of poverty and malnutrition indicators to guide development and humanitarian agencies’ programming. However, state of the art models often rely on proprietary data and/or deep or transfer learning methods whose underlying mechanics may be challenging to interpret. We demonstrate how interpretable random forest models can produce estimates of a set of (potentially correlated) malnutrition and poverty prevalence measures using free, open access, regularly updated, georeferenced data. We demonstrate two use cases: contemporaneous prediction, which might be used for poverty mapping, geographic targeting, or monitoring and evaluation tasks, and a sequential nowcasting task that can inform early warning systems. Applied to data from 11 low and lower-middle income countries, we find predictive accuracy broadly comparable for both tasks to prior studies that use proprietary data and/or deep or transfer learning methods.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.1371/journal.pone.0255519",NA,NA,NA
"rayyan-100678103","Recent Advances and New Frontiers in Riverine and Coastal Flood Modeling",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Abstract Over the past decades, the scientific community has made significant efforts to simulate flooding conditions using a variety of complex physically based models. Despite all advances, these models still fall short in accuracy and reliability and are often considered computationally intensive to be fully operational. This could be attributed to insufficient comprehension of the causative mechanisms of flood processes, assumptions in model development and inadequate consideration of uncertainties. We suggest adopting an approach that accounts for the influence of human activities, soil saturation, snow processes, topography, river morphology, and land‐use type to enhance our understanding of flood generating mechanisms. We also recommend a transition to the development of innovative earth system modeling frameworks where the interaction among all components of the earth system are simultaneously modeled. Additionally, more nonselective and rigorous studies should be conducted to provide a detailed comparison of physical models and simplified methods for flood inundation mapping. Linking process‐based models with data‐driven/statistical methods offers a variety of opportunities that are yet to be explored and conveyed to researchers and emergency managers. The main contribution of this paper is to notify scientists and practitioners of the latest developments in flood characterization and modeling, identify challenges in understanding flood processes, associated uncertainties and risks in coupled hydrologic and hydrodynamic modeling for forecasting and inundation mapping, and the potential use of state‐of‐the‐art data assimilation and machine learning to tackle the complexities involved in transitioning such developments to operation.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.1029/2022rg000788",NA,NA,NA
"rayyan-100678106","A Novel Decomposition-Ensemble Learning Model Based on Ensemble Empirical Mode Decomposition and Recurrent Neural Network for Landslide Displacement Prediction",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"As vital comments on landslide early warning systems, accurate and reliable displacement prediction is essential and of significant importance for landslide mitigation. However, obtaining the desired prediction accuracy remains highly difficult and challenging due to the complex nonlinear characteristics of landslide monitoring data. Based on the principle of “decomposition and ensemble”, a three-step decomposition-ensemble learning model integrating ensemble empirical mode decomposition (EEMD) and a recurrent neural network (RNN) was proposed for landslide displacement prediction. EEMD and kurtosis criteria were first applied for data decomposition and construction of trend and periodic components. Second, a polynomial regression model and RNN with maximal information coefficient (MIC)-based input variable selection were implemented for individual prediction of trend and periodic components independently. Finally, the predictions of trend and periodic components were aggregated into a final ensemble prediction. The experimental results from the Muyubao landslide demonstrate that the proposed EEMD-RNN decomposition-ensemble learning model is capable of increasing prediction accuracy and outperforms the traditional decomposition-ensemble learning models (including EEMD-support vector machine, and EEMD-extreme learning machine). Moreover, compared with standard RNN, the gated recurrent unit (GRU)-and long short-term memory (LSTM)-based models perform better in predicting accuracy. The EEMD-RNN decomposition-ensemble learning model is promising for landslide displacement prediction.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate","https://doi.org/10.3390/app11104684",NA,NA,NA
"rayyan-100678125","Application of Nonlinear Time Series and Machine Learning Algorithms for Forecasting Groundwater Flooding in a Lowland Karst Area",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate | USER-NOTES: {""Maria""=>[""In karst limestone areas interactions between ground and surface waters can be frequent, particularly in low lying areas, linked to the unique hydrogeological dynamics of that bedrock aquifer. In extreme hydrological conditions, however, this can lead to wide-spread, long-duration flooding, resulting in significant cost and disruption. This study develops and compares a nonlinear time-series analysis based nonlinear autoregressive model with exogenous variables (NARX), machine learning based near support vector regression as well as a linear time-series ARX model in terms of their performance to predict groundwater flooding in a lowland karst area of Ireland. The models have been developed upon the results of several years of field data collected in the area, as well as the outputs of a highly calibrated semi-distributed hydraulic/hydrological model of the karst network. "", ""The prediction of total flooding volume indicates that the performances of all the models are similarly accurate up to 10 days into the future. A NARX model taking inputs of the past 5 days' flood volume; rainfall data and tidal amplitude data across the past 4 days, showed the best flood forecasting performance up to 30 days into the future. Existing real-time telemetric monitoring of water level data at two points in the catchment can be fed into the model to provide an early warning flood warning tool. The model also predicts freshwater discharge from the inter-tidal spring into the Atlantic Ocean which hitherto had not been possible to monitor.""]}","https://doi.org/10.1029/2021wr029576",NA,NA,NA
"rayyan-100678139","Severe COVID-19 and related hyperferritinaemia: more than an innocent bystander?",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"<h3>Abstract</h3> Climate change is impacting ecosystem structure and function, with potentially drastic downstream effects on human and animal health. Emerging zoonotic diseases are expected to be particularly vulnerable to climate and biodiversity disturbance. Anthrax is an archetypal zoonosis that manifests its most significant burden on vulnerable pastoralist communities. The current study sought to investigate the influence of temperature increases on the landscape suitability of anthrax in the temperate, boreal, and arctic North, where observed climate impact has been rapid. This study also explored the influence of climate relative to more traditional factors, such as livestock distribution, ungulate biodiversity, and soil-water balance, in demarcating high risk landscapes. Machine learning was used to model landscape suitability as the ecological niche of anthrax in northern latitudes. The model identified climate, livestock density and wild ungulate species richness as the most influential landscape features in predicting suitability. These findings highlight the significance of warming temperatures for anthrax ecology in northern latitudes, and suggest potential mitigating effects of interventions targeting megafauna biodiversity conservation in grassland ecosystems, and animal health promotion among small to midsize livestock herds. <h3>Significance Statement</h3> We present evidence that a warming climate may be associated with the current distribution of anthrax risk in the temperate, boreal, and arctic North. Moreover, projected warming over the coming decades was associated with substantive expansion of this risk. In addition, livestock distribution, ungulate biodiversity, and soil-water balance were also influential to anthrax risk. While these results are sobering for the future health of livestock and pastoralist communities in the northern latitudes, the coincident modulating effect of ungulate biodiversity may suggest targeted ecosystem conservation as a possible buffer against a growing anthrax niche.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.1136/annrheumdis-2020-217618",NA,NA,NA
"rayyan-100678145","Machine learning–based observation-constrained projections reveal elevated global socioeconomic risks from wildfire",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Reliable projections of wildfire and associated socioeconomic risks are crucial for the development of efficient and effective adaptation and mitigation strategies. The lack of or limited observational constraints for modeling outputs impairs the credibility of wildfire projections. Here, we present a machine learning framework to constrain the future fire carbon emissions simulated by 13 Earth system models from the Coupled Model Intercomparison Project phase 6 (CMIP6), using historical, observed joint states of fire-relevant variables. During the twenty-first century, the observation-constrained ensemble indicates a weaker increase in global fire carbon emissions but higher increase in global wildfire exposure in population, gross domestic production, and agricultural area, compared with the default ensemble. Such elevated socioeconomic risks are primarily caused by the compound regional enhancement of future wildfire activity and socioeconomic development in the western and central African countries, necessitating an emergent strategic preparedness to wildfires in these countries.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1038/s41467-022-28853-0",NA,NA,NA
"rayyan-100678175","A Small Target Forest Fire Detection Model Based on YOLOv5 Improvement",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Forest fires are highly unpredictable and extremely destructive. Traditional methods of manual inspection, sensor-based detection, satellite remote sensing and computer vision detection all have their obvious limitations. Deep learning techniques can learn and adaptively extract features of forest fires. However, the small size of the forest fire target in the long-range-captured forest fire images causes the model to fail to learn effective information. To solve this problem, we propose an improved forest fire small-target detection model based on YOLOv5. This model requires cameras as sensors for detecting forest fires in practical applications. First, we improved the Backbone layer of YOLOv5 and adjust the original Spatial Pyramid Pooling-Fast (SPPF) module of YOLOv5 to the Spatial Pyramid Pooling-Fast-Plus (SPPFP) module for a better focus on the global information of small forest fire targets. Then, we added the Convolutional Block Attention Module (CBAM) attention module to improve the identifiability of small forest fire targets. Second, the Neck layer of YOLOv5 was improved by adding a very-small-target detection layer and adjusting the Path Aggregation Network (PANet) to the Bi-directional Feature Pyramid Network (BiFPN). Finally, since the initial small-target forest fire dataset is a small sample dataset, a migration learning strategy was used for training. Experimental results on an initial small-target forest fire dataset produced by us show that the improved structure in this paper improves mAP@0.5 by 10.1%. This demonstrates that the performance of our proposed model has been effectively improved and has some application prospects.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.3390/f13081332",NA,NA,NA
"rayyan-100678192","Improving regional wheat drought risk assessment for insurance application by integrating scenario-driven crop model, machine learning, and satellite data",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate | USER-NOTES: {""Maria""=>[""Accurate estimation of yield losses from natural disasters on a regional scale can guide agronomic management and agricultural insurance, transfer disaster risk, and ensure food security. Conventional yield losses, however, mainly depend on historical events, for which detailed records of locations and losses are unavailable. OBJECTIVE The development of a disaster vulnerability model has consequently been hindered by the lack of sufficient samples. To improve regional yield estimates, a novel method for estimating yield losses and the pure insurance rate is thus strongly needed. METHODS Using 129 major wheat-producing counties on the North China Plain as an example, we developed a drought assessment system based on crop growth modeling, machine learning, and satellite data. "", ""The initial model used was the model to simulate the crop–weather relationship over a large area (MCWLA) applied to wheat (MCWLA-Wheat), which was then calibrated by multi-step-assimilation with multi-source data. We first established various drought scenarios to simulate the impacts of drought at different growth stages on wheat yield by driving the calibrated MCWLA-Wheat. RESULTS AND CONCLUSIONS According to our results, drought-sensitive stages of wheat varied by drought severity, with insufficient water supply during the Greenup–Heading stage having the most significant impact on grain yields. "", ""Based on the outputs of the simulation and three drought indicators—standardized precipitation index (SPI), standardized soil moisture index (SSMI), and relative leaf area index (RLAI), we then established vulnerability models coupling the MCWLA-Wheat with statistical models (random forest [RF] and multiple linear regression [MLR]). The vulnerability model MCWLA+RF showed higher accuracy, with a root mean square error (RMSE) of 6% (i.e., 365 kg ha−1), relative to that of the MCWLA+MLR (RMSE = 1175 kg ha−1). Ranked in descending order, the relative importance of drought indicators at Greenup–Heading was SPI, SMMI-20 cm, and RLAI.""]}","https://doi.org/10.1016/j.agsy.2021.103141",NA,NA,NA
"rayyan-100678198","Projection of future drought and its impact on simulated crop yield over South Asia using ensemble machine learning approach",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: hazard,climate | USER-NOTES: {""Maria""=>[""Understanding the development mechanism of drought events, characterization of future drought metrics, and its impact on crop yield is crucial to ensure food security globally, and more importantly, in South Asia. Therefore, the present study assessed the changes in future projected drought metrics and evaluated the future risk of yield reduction under drought intensity. We characterized the magnitude, intensity, and duration of future drought by means of the SPEI drought index using CMIP6 (Coupled Model Inter-comparison Phase-6) climate models. The impact of future drought on crop yield was quantified from the ISI-MP (Inter-Sectoral Impact Model Inter-comparison Project) crop model by a proposed non-linear ensemble of Random Forest (RF) and Gradient Boosting Machine (GBM). Results suggested that high drought magnitude with a longer drought duration is projected in some regions of South Asia while high drought intensity comes with a shorter duration. "", ""It was also found that Afghanistan, Pakistan, and India will experience a longer drought duration in the future. Our proposed ensemble machine learning (EML) approach had high predictive skill with a minimum value of RMSE (0.358–0.390), MAE (0.222–0.299), and a maximum value of R2 (0.705–0.918) compared to the stand-alone methods of RF and GBM for yield loss risk projection. The drought-driven impact on crop yield demonstrates a high risk of yield loss under extreme drought events, which will encounter 54.15%, 29.30%, and 50.66% loss in the future for rice, wheat, and maize crops, respectively. Furthermore, drought and yield loss risk dynamics suggested a one unit decrease in SPEI value would lead to a 14.2%, 7.5%, and 10.9% decrease in yield for rice, wheat, and maize crops, respectively. This study will provide a notable direction for policy agencies to build resistance to crop production against the drought impact in the regions that are critical to climate change.""]}","https://doi.org/10.1016/j.scitotenv.2021.151029",NA,NA,NA
"rayyan-100678204","Vegetable waste and by-products to feed a healthy gut microbiota: Current evidence, machine learning and computational tools to design novel microbiome-targeted foods",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Food waste management is a key issue to global food security and friendly environmental governance. Worldwide, one-third of food produced for human consumption is lost or wasted along the food supply chain, primary production and food processing representing the most significant loses. Therefore, the need to achieve zero waste production schemes is becoming a priority to meet Sustainable Development Goals. Increasing evidence points towards vegetable food waste as a rich source of a wide array of carbohydrate structures and fibres providing the opportunity to identify and develop alternative approaches to valorize agro-food waste. This review describes the valorization of vegetable waste and by-products via production of (novel) substrates targeted to gut microbiota modulation, emphasizing the importance of raw materials and structural-functional properties of carbohydrates. Furthermore, we propose a novel framework for the rational selection of vegetable sources with potential prebiotic activity, based on machine learning and other computational tools applied to available literature and public database information. Integration of the body of knowledge within the field of vegetable food waste valorization, from different perspectives, allows a rational selection of carbohydrate-based substrates with promising prebiotic activities. By exploring the interactions among dietary fibre and gut microbial ecosystems using computational tools fed with structural, functional and genomic data, we can identify substrates with potential to selectively stimulate gut commensals, in agreement with experimental evidence. Our approach establishes a new framework that can be extended to a wide range of commensal microbes and carbohydrate structures.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.1016/j.tifs.2021.10.002",NA,NA,NA
"rayyan-100678206","Multidimensional assessment of global dryland changes under future warming in climate projections",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate | USER-NOTES: {""Maria""=>[""Drylands are the homes to over one-third of the world’s population, and are vulnerable to anthropogenic climate change. Based on climate projections, recent studies reported a substantial expansion of global drylands in the coming decades and attributed that expansion to future warming. However, the expansion of drylands contradicts a widespread vegetation greening and a slight runoff increase in dryland ecosystems in the same climate models. Here we re-examine changes in global drylands for the coming century and at two future warming targets (1.5 °C and 2 °C warming relative to the preindustrial level) based on outputs of climate models who participated in the Fifth Coupled Model Intercomparison Project (CMIP5). In addition to aridity index (AI) that has been widely used to measure the atmospheric aridity, we also assess changes in drylands from the hydrologic and agro-ecological perspectives, using runoff (Q) and leaf area index (LAI) as indicators, respectively. "", ""Our results show that when the impact of elevated atmospheric CO2 concentration ([CO2]) on vegetation water consumption is considered in the estimation of potential evapotranspiration (EP) and AI, the expansion of atmospheric drylands is at a much slower rate (~0.16% per decade under RCP4.5 and ~0.30% per decade under RCP8.5) than previously reported. Moreover, the additional 0.5 °C warming does not lead to an evident further expansion of atmospheric drylands. In terms of hydrologic and agro-ecological drylands, both of them show significant shrinks over the coming decades, suggesting reduced hydrologic and agro-ecological aridity in the region."", ""Finally, contrasting with previous perceptions, our results demonstrate that warming only plays a minor role in altering global drylands from all three perspectives. Increases in net radiation are primarily responsible for the expansion of atmospheric drylands, and increases in P and [CO2]-induced increases in vegetation water use efficiency are the key drivers of changes in hydrologic and agro-ecological drylands.""]}","https://doi.org/10.1016/j.jhydrol.2020.125618",NA,NA,NA
"rayyan-100678218","Greater increases in China's dryland ecosystem vulnerability in drier conditions than in wetter conditions",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate | USER-NOTES: {""Maria""=>[""Dryland ecosystems are experiencing dramatic climate change, either drier or wetter. However, the differences in response amplitudes of dryland ecosystems to drier and wetter climates have not been frequently discussed, especially when using composite indicators at large scales. This study explores the changing patterns of ecosystem vulnerability in China's drylands by comprehensively considering exposure, sensitivity and resilience indicators using leaf area index (LAI) datasets and meteorological data within two periods from 1982 to 1999 (P1) and from 2000 to 2016 (P2). The results show that nearly 57% of China's drylands have experienced drier conditions in 2000–2016 based on the average aridity index (AI) values compared with the conditions in 1982–1999. "", ""Compared with the conditions in 1982–1999, ecosystem vulnerability has increased in 78% of dryland, and ecosystem resilience has decreased in 46% of the area in 2000–2016. The amplitudes of vulnerability increase are higher in drier conditions than in wetter conditions. Ecosystem resilience has obviously increased in wetter conditions but has decreased in drier conditions, especially in farming-pastoral ecotones with an obvious land use change. Consequently, vegetation-climate composite indicators provide a holistic pattern of China's dryland ecosystem response to climate change, and the decreased ecosystem resilience in drier conditions in northeast China should be a warning signal under the national vegetation greening background. This research highlights that the impact of drying on ecosystem resilience leads the response of ecosystems to drier environment.""]}","https://doi.org/10.1016/j.jenvman.2021.112689",NA,NA,NA
"rayyan-100678230","Multi-task machine learning improves multi-seasonal prediction of the Indian Ocean Dipole",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"As one of the most predominant interannual variabilities, the Indian Ocean Dipole (IOD) exerts great socio-economic impacts globally, especially on Asia, Africa, and Australia. While enormous efforts have been made since its discovery to improve both climate models and statistical methods for better prediction, current skills in IOD predictions are mostly limited up to three months ahead. Here, we challenge this long-standing problem using a multi-task deep learning model that we name MTL-NET. Hindcasts of the IOD events during the past four decades indicate that the MTL-NET can predict the IOD well up to 7-month ahead, outperforming most of world-class dynamical models used for comparison in this study. Moreover, the MTL-NET can help assess the importance of different predictors and correctly capture the nonlinear relationships between the IOD and predictors. Given its merits, the MTL-NET is demonstrated to be an efficient model for improved IOD prediction.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1038/s41467-022-35412-0",NA,NA,NA
"rayyan-100678275","Quantifying aboveground biomass dynamics from charcoal degradation in Mozambique using GEDI Lidar and Landsat",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Understanding changes to aboveground biomass (AGB) in forests undergoing degradation is crucial for accurately and completely quantifying carbon emissions from forest loss and for environmental monitoring in the context of climate change. Monitoring forest degradation as compared to deforestation presents technical challenges because degradation involves widespread, low-intensity AGB removal under varying temporal dynamics. Charcoal production is a key driver for forest degradation in Africa and is projected to increase in the future years. In Sub-Saharan Africa (SSA), where charcoal production drives widespread ABG removal, the utility of optical remote sensing for degradation quantification is challenged by the large inter-seasonal variation and high complexities in ecosystem structure. Limited field measurements on tree structure and aboveground biomass density (AGBD) in many parts of the SSA also impose constraints. In this study, we present a novel data fusion approach combining 3D forest structure from NASA's GEDI Lidar with optical time-series data from Landsat to quantify biomass losses associated with charcoal-related forest degradation over a 10-year time period. We used machine learning models with Landsat spectral indices from the time period of limited hydric stress (LHS) as predictor variables. By applying the best performing Random Forest (RF) model to LandTrendr-stabilized annual LHS Landsat composites, we produced annual forest AGBD maps from 2007 to 2019 over the Mabalane district in southern Mozambique where the dry forest ecosystem was under active charcoal-related degradation since 2008. The RF model achieved an RMSE value of 7.05 Mg/ha (RMSE% = 42%) and R2 value of 0.64 using a 10-fold cross-validation dataset. We quantified a total AGB loss of 2.12 ± 0.06 Megatons (Mt) over the 10-year period, which is only 6.35 ± 2.56% less than the total loss estimated using field-based data as previously published for the same area and time. In addition to quantifying biomass loss, we constructed annual AGBD maps that enabled the characterization of disturbance and recovery. Our framework demonstrates that fusing GEDI and Landsat data through predictive modeling can be used to quantify past forest AGBD dynamics in low biomass forests. This approach provides a satellite-based method to support REDD+ monitoring and evaluation activities in areas where field data is limited and has the potential to be extended to investigate a variety of different disturbance events.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1016/j.rse.2022.113367",NA,NA,NA
"rayyan-100678280","Predictive mapping of two endemic oak tree species under climate change scenarios in a semiarid region: Range overlap and implications for conservation",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Quercus infectoria and Quercus libani are two important species distributed across most of the Kurdistan Region of Iraq's mountain ranges (KRI). They have significant ecological, medicinal, and socioeconomic values. Recent studies have documented how plant distributions have been impacted by climate change. This study's goal is to establish the existing distributions of both species, measure the consequences of prospective environmental conditions on their distributions, predict possible habitat distributions, map the overlapped habitat ranges for the species in the KRI, and identify the key factors influencing their distributions. For these aims, distribution data points of the species, different environmental factors, including the existing climate, three emission predictions for the 2050s, 2070s, and 2090s of two general circulation models (GCMs), a machine learning approach, and geospatial techniques were used. Modeling revealed that the total magnitude of the habitat increase for the species would be less than the overall magnitude of the habitat contraction. The yearly mean temperature, yearly precipitation, and minimum temperature during the coldest period mostly alter the target species' geographic dispersion. Across the three emission scenarios of the both models, Q. infectoria habitat would contract by 2760.9–2856.9 km2 (5.36–5.55%), 2856.9–3357.2 km2 (5.55–6.52%) and 2822.1–3400.2 km2 (5.48–6.60%), whereas it would expand by 1153.3–1638.9 km2 (2.24–3.18%), 761.0–1556.8 km2 (1.48–3.02%), and 721.5–1547.1 km2 (1.40–3.00%) for the 2050s, 2070s, and 2090s, respectively. A similar pattern was also noted for Q. libani. The two species' habitat ranges in KRI would be considerably reduced due to climate change. The species' estimated area would extend mostly to the east and southeast of the KRI at high altitudes. The mountain areas, notably those where the species overlap by 1767.2–1807.5 km2 (3.43–3.51%) for the two GCMs, must be the primary objective of conservation efforts. This research presents new baseline data for future research on mountain forest ecosystems and the techniques of biodiversity conservation to reduce climate change's effects in Iraq.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.1016/j.ecoinf.2022.101930",NA,NA,NA
"rayyan-100678289","The Temporal Dynamics of Multiple Stressor Effects: From Individuals to Ecosystems",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Multiple anthropogenic stressors rarely overlap in perfect synchrony in time, yet most research quantifying how they interact assumes that they do.Stressor sequence and the degree of temporal overlap will have implications for ecological memory – the influence of past stressors on future ecological responses – from genes to ecosystems.Adding to this complexity, organisms with different generation times will experience multiple stressors (and the degree to which they overlap in time) in different ways.We propose that lifespan and associated metabolic rates can be used to define stressor type (continuous or discrete) and temporal overlap for different focal organisms.Moving forward, we need to embrace the temporal complexity of multiple stressors and quantify how various realistic asynchronous scenarios will alter their cumulative impacts across different ecosystems. Multiple stressors, such as warming and invasions, often occur together and have nonadditive effects. Most studies to date assume that stressors operate in perfect synchrony, but this will rarely be the case in reality. Stressor sequence and overlap will have implications for ecological memory – the ability of past stressors to influence future responses. Moreover, stressors are usually defined in an anthropocentric context: what we consider a short-term stressor, such as a flood, will span multiple generations of microbes. We argue that to predict responses to multiple stressors from individuals to the whole ecosystem, it is necessary to consider metabolic rates, which determine the timescales at which individuals operate and therefore, ultimately, the ecological memory at different levels of ecological organization. Multiple stressors, such as warming and invasions, often occur together and have nonadditive effects. Most studies to date assume that stressors operate in perfect synchrony, but this will rarely be the case in reality. Stressor sequence and overlap will have implications for ecological memory – the ability of past stressors to influence future responses. Moreover, stressors are usually defined in an anthropocentric context: what we consider a short-term stressor, such as a flood, will span multiple generations of microbes. We argue that to predict responses to multiple stressors from individuals to the whole ecosystem, it is necessary to consider metabolic rates, which determine the timescales at which individuals operate and therefore, ultimately, the ecological memory at different levels of ecological organization. Predicting how anthropogenic stressors, such as pollution events, warming, and novel pathogens, affect natural ecosystems is a major challenge for contemporary ecology. The presence, frequency, and magnitude of multiple stressors varies over time and space, however, with implications for their independent and combined effects on responses from individual behavior to entire ecosystem processes [1.Ryo M. et al.Basic principles of temporal dynamics.Trends Ecol. Evol. 2019; 34: 723-733Abstract Full Text Full Text PDF PubMed Scopus (51) Google Scholar]. Here, we argue that the timing and duration of both the initial impacts of and recovery from stressors are particularly critical, and at least as important as the spatial component that has been the primary focus of most research to date. In particular, the temporal dynamics of multiple stressor events has been largely overlooked, yet time is critically important because stressors rarely, if ever, act in perfect synchrony, and the order and overlap duration will shape their combined impacts. Furthermore, stressors do not need to overlap in time to have cumulative effects, since the ‘legacy’ of previous stressors can alter the response of the ecosystem (and its component populations) to future stress. Stressor–response relationships are not consistent through time [1.Ryo M. et al.Basic principles of temporal dynamics.Trends Ecol. Evol. 2019; 34: 723-733Abstract Full Text Full Text PDF PubMed Scopus (51) Google Scholar]. Time can be important through both evolutionary adaptation (see Glossary) and ‘ecological memory’. The former refers to the relatively well-known process of natural selection over multiple generations, where species’ adapt to function better in the face of environmental challenges [2.Pawar S. et al.From metabolic constraints on individuals to the dynamics of ecosystems.in: Belgrano A. Aquatic Functional Biodiversity: An Ecological and Evolutionary Perspective. Academic Press, 2015: 3-36Crossref Scopus (31) Google Scholar,3.Gibert J.P. et al.Scaling-up trait variation from individuals to ecosystems.Adv. Ecol. Res. 2015; 52: 1-17Crossref Scopus (30) Google Scholar]. Here, we focus on the latter phenomenon, defined as the ability of past stressors to influence the future ecological responses of a population, community, or ecosystem [4.Hughes T.P. et al.Ecological memory modifies the cumulative impact of recurrent climate extremes.Nat. Clim. Chang. 2019; 9: 40-43Crossref Scopus (159) Google Scholar,5.Ogle K. et al.Quantifying ecological memory in plant and ecosystem processes.Ecol. Lett. 2015; 18: 221-235Crossref PubMed Scopus (214) Google Scholar]. This can include acclimation, parental effects, and species sorting due to past stress, including lagged indirect legacy effects. For instance, a recent single-stressor study found that marine sticklebacks (Gasterosteus aculeatus) exhibit carry-over effects of a high CO2 environment, causing offspring to perform poorly under conditions that differ from those experienced by their parents (even if these new conditions were actually less ‘stressful’) [6.Schade F.M. et al.Within- and transgenerational effects of ocean acidification on life history of marine three-spined stickleback (Gasterosteus aculeatus).Mar. Biol. 2014; 161: 1667-1676Crossref Scopus (49) Google Scholar]. At the wider community level, another study has demonstrated how the impacts of a severe heatwave in 2017 on corals in the Great Barrier Reef were shaped by the effects of a heatwave in the preceding year [4.Hughes T.P. et al.Ecological memory modifies the cumulative impact of recurrent climate extremes.Nat. Clim. Chang. 2019; 9: 40-43Crossref Scopus (159) Google Scholar] (i.e., multiple heatwave stressors separated in time but not space). Impacts can also manifest long after the stressor itself has disappeared, because of dispersal limitations or lagged effects due to species interactions (e.g., trophic cascades) [7.Essl F. et al.Historical legacies accumulate to shape future biodiversity in an era of rapid global change.Divers. Distrib. 2015; 21: 534-547Crossref Scopus (90) Google Scholar]. For instance, lichen diversity remained unchanged 2.5 years after adjacent forest clearance, but declined after a further 14 years [8.Johansson V. et al.Time-lagged lichen extinction in retained buffer strips 16.5 years after clear-cutting.Biol. Conserv. 2018; 225: 53-65Crossref Scopus (10) Google Scholar] due to dispersal limitations. Another example is the Nitrogen Cascade, whereby increased soil nitrogen content (from deposition) causes changes in plant composition, which then causes lagged changes in herbivore, and subsequently predator, communities [9.Vogels J.J. et al.Can changes in soil biochemistry and plant stoichiometry explain loss of animal diversity of heathlands?.Biol. Conserv. 2017; 212: 432-447Crossref Scopus (23) Google Scholar]. These studies demonstrate that ecological memory can alter our ability to detect and predict multiple stressor impacts: even if a past stressor has long since disappeared, its legacy may still be playing out in a system facing new stressors. Despite multiple anthropogenic stressors being an increasingly common phenomenon, most research to date has focused on a single stressor, a single trophic level (but see [10.Jackson M.C. et al.Net effects of multiple stressors in freshwater ecosystems: a meta-analysis.Glob. Chang. Biol. 2016; 22: 180-189Crossref PubMed Scopus (429) Google Scholar,11.Bracewell S. et al.Qualifying the effects of single and multiple stressors on the food web structure of Dutch drainage ditches using a literature review and conceptual models.Sci. Total Environ. 2019; 684: 727-740Crossref PubMed Scopus (20) Google Scholar]), and either the spatial component – comparing locations with and without a stressor – or simple ‘before’ and ‘after’ stress ecosystem states, ignoring temporal oscillations in magnitude and dissipation time (Figure 1). Recent reviews have discussed the complexity of temporal dynamics [1.Ryo M. et al.Basic principles of temporal dynamics.Trends Ecol. Evol. 2019; 34: 723-733Abstract Full Text Full Text PDF PubMed Scopus (51) Google Scholar] and the importance of species’ traits in determining timescales [7.Essl F. et al.Historical legacies accumulate to shape future biodiversity in an era of rapid global change.Divers. Distrib. 2015; 21: 534-547Crossref Scopus (90) Google Scholar,12.Cator L.J. et al.The role of vector trait variation in vector-borne disease dynamics.Front. Ecol. Evol. 2020; 8: 189Crossref PubMed Scopus (20) Google Scholar], and time is now recognized as being particularly critical for gauging climate change impacts, especially in terms of the progressive rises in average temperature that continue to ramp up under global warming [13.Wolkovich E.M. et al.Temporal ecology in the Anthropocene.Ecol. Lett. 2014; 17: 1365-1379Crossref PubMed Scopus (149) Google Scholar]. This thinking has started to enter the wider realm of empirical multiple stressor ecology [14.Feckler A. et al.History matters: heterotrophic microbial community structure and function adapt to multiple stressors.Glob. Chang. Biol. 2018; 24: e402-e415Crossref PubMed Scopus (22) Google Scholar, 15.Gunderson A.R. et al.Multiple stressors in a changing world: the need for an improved perspective on physiological responses to the dynamic marine environment.Annu. Rev. Mar. Sci. 2016; 8: 357-378Crossref PubMed Scopus (324) Google Scholar, 16.Romero F. et al.Effects of multiple stressors on river biofilms depend on the time scale.Sci. Rep. 2019; 915810Crossref PubMed Scopus (16) Google Scholar, 17.Wu P. et al.Timing anthropogenic stressors to mitigate their impact on marine ecosystem resilience.Nat. Commun. 2017; 8: 1263Crossref PubMed Scopus (31) Google Scholar, 18.Garnier A. et al.Temporal scale dependent interactions between multiple environmental disturbances in microcosm ecosystems.Glob. Chang. Biol. 2017; 23: 5237-5248Crossref PubMed Scopus (26) Google Scholar], including the introduction of an initial conceptual framework for the marine realm [15.Gunderson A.R. et al.Multiple stressors in a changing world: the need for an improved perspective on physiological responses to the dynamic marine environment.Annu. Rev. Mar. Sci. 2016; 8: 357-378Crossref PubMed Scopus (324) Google Scholar], but temporal dynamics in multiple stressor ecology are still largely ignored and have yet to be formalized more generally. As a first step towards filling these knowledge gaps, we need to embrace the temporal complexity of single stressors before we can start to address them in combination. Stressors are either discrete or continuous and their magnitude (strength of impact) can also vary over time (Figure 1A). For instance, agricultural nutrient pollution is often a discrete stressor that oscillates over time (e.g., seasonal spikes in run-off reflecting farming activity; Figure 1B). By contrast, global increases in average temperature are a more continuous stressor, increasing progressively in severity over time (i.e., a ramped stressor; Figure 1C). Since temperature sets the pace of life through its effects on metabolic rate, climatic warming can be considered as an umbrella or master stressor under which all other stressors, which are increasingly prevalent across the world, play out [19.Donohue I. et al.Navigating the complexity of ecological stability.Ecol. Lett. 2016; 19: 1172-1185Crossref PubMed Scopus (258) Google Scholar, 20.Hughes T.P. et al.Coral reefs in the Anthropocene.Nature. 2017; 546: 82-90Crossref PubMed Scopus (845) Google Scholar, 21.Lewis S.L. Maslin M.A. Defining the Anthropocene.Nature. 2015; 519: 171-180Crossref PubMed Scopus (1291) Google Scholar, 22.Ratajczak Z. et al.Abrupt change in ecological systems: inference and diagnosis.Trends Ecol. Evol. 2018; 33: 513-526Abstract Full Text Full Text PDF PubMed Scopus (121) Google Scholar]. Multiple stressor impacts can be additive or more (or less) than the sum of their single effects (synergistic and antagonistic interactions, respectively). This creates huge challenges for both science and policy, as most of the past century of ecotoxicology and biomonitoring has focused on single stressors, and the much smaller subset of studies on two or more stressors has typically assumed additivity by default [23.Côté I.M. et al.Interactions among ecosystem stressors and their importance in conservation.Proc. Biol. Sci. 2016; 28320152592Crossref PubMed Scopus (392) Google Scholar, 24.Kroeker K.J. et al.Embracing interactions in ocean acidification research: confronting multiple stressor scenarios and context dependence.Biol. Lett. 2017; 1320160802Crossref PubMed Scopus (86) Google Scholar, 25.Orr J. et al.Towards a unified concept of multiple stressor research across disciplines.Proc. Biol. Sci. 2020; 28720200421PubMed Google Scholar]. This is a gross oversimplification, as global meta-analyses and reviews have revealed that nonadditive effects are common [26.Darling E.S. Cote I.M. Quantifying the evidence for ecological synergies.Ecol. Lett. 2008; 11: 1278-1286Crossref PubMed Scopus (508) Google Scholar,27.Jackson M.C. Interactions among multiple invasive animals.Ecology. 2015; 96: 2035-2041Crossref PubMed Scopus (82) Google Scholar] in both the aquatic [10.Jackson M.C. et al.Net effects of multiple stressors in freshwater ecosystems: a meta-analysis.Glob. Chang. Biol. 2016; 22: 180-189Crossref PubMed Scopus (429) Google Scholar,28.Nõges P. et al.Quantified biotic and abiotic responses to multiple stress in freshwater, marine and ground waters.Sci. Total Environ. 2016; 540: 43-52Crossref PubMed Scopus (146) Google Scholar, 29.Ban S.S. et al.Evidence for multiple stressor interactions and effects on coral reefs.Glob. Chang. Biol. 2014; 20: 681-697Crossref PubMed Scopus (236) Google Scholar, 30.Piggott J. et al.Reconceptualizing synergism and antagonism among multiple stressors.Ecol. Evol. 2016; 5: 1538-1547Crossref Scopus (315) Google Scholar, 31.Przeslawski R. et al.A review and meta-analysis of the effects of multiple abiotic stressors on marine embryos and larvae.Glob. Chang. Biol. 2015; 21: 2122-2140Crossref PubMed Scopus (298) Google Scholar] and terrestrial [32.Dieleman W.I.J. et al.Simple additive effects are rare: a quantitative review of plant biomass and soil process responses to combined manipulations of CO2 and temperature.Glob. Chang. Biol. 2012; 18: 2681-2693Crossref PubMed Scopus (283) Google Scholar,33.Yue K. et al.Effects of three global change drivers on terrestrial C:N:P stoichiometry: a global synthesis.Glob. Chang. Biol. 2017; 23: 2450-2463Crossref PubMed Scopus (131) Google Scholar] realms, although we still do not fully understand how and why they arise [25.Orr J. et al.Towards a unified concept of multiple stressor research across disciplines.Proc. Biol. Sci. 2020; 28720200421PubMed Google Scholar]. Some initial evidence suggests that when stressors occur simultaneously, nonadditive effects are the rule rather than the exception [34.Dafforn K. et al.Big data opportunities and challenges for assessing multiple stressors across scales in aquatic ecosystems.Mar. Freshw. 2016; 67: 393-413Crossref Scopus (56) Google Scholar]. Beyond this overly simplistic scenario of synchronous stressors, the current limited data and theory constrain our ability to predict (and mitigate) multiple stressor impacts over time [25.Orr J. et al.Towards a unified concept of multiple stressor research across disciplines.Proc. Biol. Sci. 2020; 28720200421PubMed Google Scholar]. Discrete and continuous stressors are usually defined in an anthropocentric context: what we might consider a discrete stressor, such as a flooding event, might be present for the entire generation time of a shorter-lived species (Figure 1D). We argue that it is more meaningful to think in terms of the timescales at which focal organisms operate rather than absolute time per se. In particular, the distribution of generation times of different species in a community is critical in determining the ecological memory of the ecosystem as a whole. This is because generation time sets the characteristic timescale at which individuals experience, and respond to, environmental perturbations (Figure 2). Organisms operating at different timescales will do so in different ways to a given set of stressors over a particular absolute timescale [36.Saether B.E. et al.Generation time and temporal scaling of bird population dynamics.Nature. 2005; 436: 99-102Crossref PubMed Scopus (142) Google Scholar]. Therefore, the taxonomic composition of species with different generation times in an ecosystem should arguably shape overall ecological memory. To get to grips with this, we must first be able to predict the generation times of different species and how these species respond differentially to a particular sequence of stressor events. To this end, Ecological Metabolic Theories (EMTs), drawn from both the Dynamic Energy Budget and the Metabolic Theory of Ecology (MTE) frameworks [37.Brown J.H. et al.Toward a metabolic theory of ecology.Ecology. 2004; 85: 1771-1789Crossref Scopus (4637) Google Scholar,38.Kooijman B. Kooijman S. Dynamic Energy Budget Theory for Metabolic Organisation. Cambridge University Press, 2010Google Scholar], provide a good starting point. At the most fundamental level, EMTs describe how rates of metabolic processes in cells predict how individual organisms take up resources from the environment, convert them into other forms for growth and reproduction, and excrete the altered forms. This sets the stage for the prediction of rates and, at higher levels of biological organization, biomass and diversity. Importantly, at the population level, EMTs can predict our key trait of interest: generation time. The simplest such prediction comes from the MTE, that across species,G=G0m1–αeE/kT,[1] where G is the generation time, m is the species’ average body mass, α is the (scaling) constant of the whole-organism metabolic rate, E is an activation energy, k is Boltzmann’s constant (8.62 × 105 eV/K), T is the temperature in kelvin, and G0 is a size- and temperature-independent constant that captures variation from other sources such as trophic level [37.Brown J.H. et al.Toward a metabolic theory of ecology.Ecology. 2004; 85: 1771-1789Crossref Scopus (4637) Google Scholar,39.Burger J.R. et al.Toward a metabolic theory of life history.Proc. Natl. Acad. Sci. U. S. A. 2019; 116: 26653-26661Crossref Scopus (21) Google Scholar, 40.McCoy M.W. Gillooly J.F. Predicting natural mortality rates of plants and animals.Ecol. Lett. 2008; 11: 710-716Crossref PubMed Scopus (109) Google Scholar, 41.Munch S.B. Salinas S. Latitudinal variation in generation time within species is explained by the metabolic theory of ecology.Proc. Natl. Acad. Sci. U. S. A. 2009; 106: 13860-13864Crossref PubMed Scopus (128) Google Scholar]. According to the MTE, variation in generation times across species is largely determined by their body sizes and (physiologically) operational temperatures. In multicellular eukaryotes, typically α ≈ 0.75 and E ≈ 0.65 [37.Brown J.H. et al.Toward a metabolic theory of ecology.Ecology. 2004; 85: 1771-1789Crossref Scopus (4637) Google Scholar], with significant and systematic deviations from these values for unicellular eukaryotes and prokaryotes that drive ecosystem processes lower in the food web [42.Kontopoulos D. et al.Adaptive evolution shapes the present-day distribution of the thermal sensitivity of population growth rate.PLoS Biol. 2020; 18e3000894Crossref PubMed Scopus (11) Google Scholar,43.Smith T.P. et al.Community-level respiration of prokaryotic microbes may rise with global warming.Nat. Commun. 2019; 10: 5124Crossref PubMed Scopus (23) Google Scholar]. Equation 1 stems from a well-known general inverse relationship between generation time and mass-specific metabolic rate [44.Savage V.M. et al.Effects of body size and temperature on population growth.Am. Nat. 2004; 163: 429-441Crossref PubMed Scopus (589) Google Scholar,45.Brown J.H. et al.Equal fitness paradigm explained by a trade-off between generation time and energy production rate.Nat. Ecol. Evol. 2018; 2: 262-268Crossref PubMed Scopus (30) Google Scholar]. Metabolic rate and the life-history traits it drives (and in particular, generation time) therefore holds great potential to help us predict which stressors are relevant for a given organism, as well as the rate and magnitude of its response. With EMT as a foundation, a stressor’s impact can potentially be quantified based on a focal species’ generation times, inferred from its average adult body size and thermal physiology (Figure 2). Metabolic rate and generation time are the fundamental reasons why a small single-celled alga and a large fish experience and respond to stressors in vastly different timescales (Figure 1D) and through different mechanisms. For example, a recent study showed that size can explain considerable variation in the rate of thermal acclimation across a diverse range of taxa, with smaller organisms apparently acclimating faster, but to a lesser extent, than larger ones [46.Rohr J.R. et al.The complex drivers of thermal acclimation and breadth in ectotherms.Ecol. Lett. 2018; 21: 1425-1439Crossref PubMed Scopus (114) Google Scholar]. In terms of evolutionary (as opposed to just ecological) responses, the relationship between generation time and metabolic rate can also play a significant role, with faster adaptation expected for smaller organisms [47.Martin A.P. Palumbi S.R. Body size, metabolic rate, generation time, and the molecular clock.Proc. Natl. Acad. Sci. U. S. A. 1993; 90: 4087-4091Crossref PubMed Scopus (924) Google Scholar]. Furthermore, the critical timescale that one should consider to understand or quantify the overlap between multiple stressors for a given organism should be predictable from relatively simple measures of metabolic rate, via body mass and temperature (Figure 2). As a general rule, smaller, short-lived individuals in a warmer environment would be likely to experience fewer events and perceive them as rather constant or ramping, while larger, long-lived organisms in a colder environment would be likely to experience more events and perceive them as pulses (Figure 1). Ultimately, all ecosystems comprise individuals nested within species’ populations, which are themselves nested within food webs, and the combination of the metabolic activity and the dynamics of these populations shapes responses at the higher organizational levels. The simplest way to go from individuals, through populations, to ecosystem-level responses to stressors is to assume that the ecosystem response is a weighted sum of the individual populations’ responses to one or more stressors [43.Smith T.P. et al.Community-level respiration of prokaryotic microbes may rise with global warming.Nat. Commun. 2019; 10: 5124Crossref PubMed Scopus (23) Google Scholar,48.Enquist B.J. et al.Scaling metabolism from organisms to ecosystems.Nature. 2003; 423: 639-642Crossref PubMed Scopus (292) Google Scholar]. For example, Smith et al. [43.Smith T.P. et al.Community-level respiration of prokaryotic microbes may rise with global warming.Nat. Commun. 2019; 10: 5124Crossref PubMed Scopus (23) Google Scholar] weight the responses of different functional groups of species according to their biomass to predict that warming will permanently elevate ecosystem-level respiration. Such approaches, which allow simple linear up-scaling from individuals to ecosystems, can easily accommodate multiple stressors provided the (multivariate) physiological response of the component species’ populations can be quantified. However, this simple upscaling approach necessarily assumes that species’ biomasses are constant relative to the timescale of the stressor’s influence (such that it changes species’ physiological rates but not their abundances). This approach can be extended via trait-driver theory, which allows species’ traits (e.g., body size) that determine population responses to (multiple) stressors to dynamically modify population biomasses in the system [49.Enquist B.J. et al.Scaling from traits to ecosystems: developing a general trait driver theory via integrating trait-based and metabolic scaling theories.Adv. Ecol. Res. 2015; 52: 249-318Crossref Scopus (210) Google Scholar,50.Savage V.M. et al.A general multi-trait-based framework for studying the effects of biodiversity on ecosystem functioning.J. Theor. Biol. 2007; 247 (213–212)Crossref PubMed Scopus (77) Google Scholar]. However, this approach is currently limited in that it cannot accommodate realistic trophic and nontrophic interactions between species’ populations. To truly go from the multistressor responses of individual organisms to entire communities and ecosystems, the complexity of time-varying, nonlinear species interactions (i.e., food-web mediated) needs to be considered (Box 1).Box 1Species Interactions and Temporal StressorsIn addition to considering how multiple drivers operate in the temporal dimension, we also need to understand the importance of indirect cascading effects due to species interactions (i.e., food-web mediated; Figure I). Nonlinearities and cascading effects can generate unpredictable fluctuations in the sizes of interacting populations and, therefore, the propagation of stressor-induced perturbations across the food web (e.g., from primary producers to top consumers). Furthermore, the effects of stressors often attenuate or change directionally as they move through the food web because of systematic differences in timescales or interacting species. For example, in aquatic systems where the primary producers are (small-bodied and fast-lived) algae, one may observe very rapid responses at the base of the web, with time-lagged and more aggregated effects at the higher trophic levels (which are larger-bodied and longer-lived; Figure I). The body-mass structure of the web drives these dynamics via metabolic rates and generation times (Figure I), but also its effects on trophic status and abundance.As an example, if we know the absolute timescale at which a stressor affects the base of the food web, we can start to put bounds on how long it will take to ‘ripple’ upwards to the top (to account for the temporal dimension of indirect effects) and disentangle this from more direct effects (Figure IB). Of course, food webs are not simply unidirectional networks: consumers can also exert top-down control. Stressors can modulate these cascading effects, as seen with pesticides, nutrients, acidification, and climate change, for instance; as yet, how multiple stressors play out over time across the food web remains poorly understood. In addition to considering how multiple drivers operate in the temporal dimension, we also need to understand the importance of indirect cascading effects due to species interactions (i.e., food-web mediated; Figure I). Nonlinearities and cascading effects can generate unpredictable fluctuations in the sizes of interacting populations and, therefore, the propagation of stressor-induced perturbations across the food web (e.g., from primary producers to top consumers). Furthermore, the effects of stressors often attenuate or change directionally as they move through the food web because of systematic differences in timescales or interacting species. For example, in aquatic systems where the primary producers are (small-bodied and fast-lived) algae, one may observe very rapid responses at the base of the web, with time-lagged and more aggregated effects at the higher trophic levels (which are larger-bodied and longer-lived; Figure I). The body-mass structure of the web drives these dynamics via metabolic rates and generation times (Figure I), but also its effects on trophic status and abundance. As an example, if we know the absolute timescale at which a stressor affects the base of the food web, we can start to put bounds on how long it will take to ‘ripple’ upwards to the top (to account for the temporal dimension of indirect effects) and disentangle this from more direct effects (Figure IB). Of course, food webs are not simply unidirectional networks: consumers can also exert top-down control. Stressors can modulate these cascading effects, as seen with pesticides, nutrients, acidification, and climate change, for instance; as yet, how multiple stressors play out over time across the food web remains poorly understood. Despite these complexities, even in the currently nascent state of the theoretical development of the field, we can start to make empirically relevant predictions about the combined effects of multiple stressors by qualitatively mapping stressor temporal dynamics onto the variation in organisms’ generation times within a community or ecosystem. We can hypothesize that every community has a characteristic absolute timescale depending on the relative proportions of organisms with different generation times within it, and how these organisms interact with each other (Box 1 and Figure 1A). For instance, if we take a stressor perceived as discrete in an anthropogenic context (e.g., a pollution event) and compare the response in a ‘slow’ community comprising large, long-lived organisms (i.e., a forest of trees) versus a ‘fast’ community dominated by smaller, short-lived organisms (i.e., oceanic phytoplankton), even with a ‘zeroth-order’ model for the community response as a biomass-weighted sum of species’ population responses one would expect fundamentally different community-level ecological memory. The slow community will experience a discrete stressor and the fast community a continuous stressor (Figure 2). However, most communities comprise both large long-lived and small short-lived organisms experiencing the stressors at different absolute timescales while interacting with one another, adding further complexity to multiple stressor ecology. As a first step towards this goal, we can at least start to group systems into broad typologies based on their size structure and hence metabolic capacity and the temporal envelopes of response, as in our two (extreme exemplar) cases earlier. How to precisely quantify this community structure and how it maps onto ecosystem processes remains an open question, but considerable progress has been made in the past two decades or so and we now have a set of fairly simple rules that can be used to help narrow the field. For instance, aquatic systems have strongly size-structured food webs, with energy flowing from small, abundant, and diverse but short-lived organisms at the base (e.g., algae) to those that are increasingly larger, rarer, and less diverse but longer-lived at the top of the web (e.g., predatory fishes; Box 1). At each ‘trophic-level’ step there is an approximate order of magnitude drop in biomass transfer (Box 1) and similar degrees of change in terms of the other traits listed above – and many of these have well-described allometric scaling relationships that can be used here to predict broad shifts in communities and how these map onto temporal shifts in stressors. One approach could involve the resolution of community size spectra using a combination of molecular techniques (e.g., next-generation sequencing and metabarcoding of environmental DNA or bulk-sample DNA) to obtain detailed taxonomic information, and machine learning using images to gain complementary community body-mass distributions [51.Jackson M.C. et al.Recommendations for the next generation of global freshwater biological monitoring tools.Adv. Ecol. Res. 2016; 55: 615-636Crossref Scopus (56) Google Scholar]. This could then be used to calculate the relative proportions of generation times distributed within the community, so that ecologists can decide when in absolute time to most accurately measure a community response to multiple stressor events. Moving forward, we will need to focus future research effort into unraveling how temporally realistic stressors – occurring at different time points, with different degrees of overlap in absolute time – alter the response of organisms across the full scale of generation times, so we can start to resolve communities into ‘temporal types’ to start testing these predictions with both data and models. Multiple stressor ecology has taken off in the past few years [23.Côté I.M. et al.Interactions among ecosystem stressors and their importance in conservation.Proc. Biol. Sci. 2016; 28320152592Crossref PubMed Scopus (392) Google Scholar,25.Orr J. et al.Towards a unified concept of multiple stressor research across disciplines.Proc. Biol. Sci. 2020; 28720200421PubMed Google Scholar,52.Tockner K. et al.Multiple stressors in coupled river–floodplain ecosystems.Freshw. Biol. 2010; 55: 135-151Crossref Scopus (288) Google Scholar,53.Birk S. et al.Impacts of multiple stressors on freshwater biota across spatial scales and ecosystems.Nat. Ecol. Evol. 2020; 4: 1060-1068Crossref PubMed Scopus (154) Google Scholar], but there is still a huge chasm to be bridged between theory and reality. In particular, ecologists need to bring greater temporal realism to multiple stressor research to address some of the biggest challenges facing our rapidly changing world [25.Orr J. et al.Towards a unified concept of multiple stressor research across disciplines.Proc. Biol. Sci. 2020; 28720200421PubMed Google Scholar]. Here, we have shown the need to move beyond current overly simplistic scenarios of perfectly overlapping stressors: future research needs to consider stressor sequences, the degree of stressor overlap, and how these change for organisms of different generation times in ‘fast’ versus ‘slow’ communities (see Outstanding Questions). Most of our current knowledge on multiple stressor interactions is based on unrealistic synchronous scenarios. We argue that ecological responses will change dramatically under realistic asynchronous scenarios due to ecological memory, with implications for the prevalence of additive versus nonadditive effects and, by extension, the management and conservation of natural ecosystems.Outstanding QuestionsHow do the order, frequency, and duration of multiple stressors affect their cumulative ecological impact on species’ populations?Does previous exposure to a particular stressor promote or erode resilience to future stressors (through ecological memory) and does this response depend on stressor similarity?Do temporally asynchronous multiple stressor events have outcomes (e.g., dominance of nonadditive affects) comparable with those of stressors acting in perfect synchrony?If we rescale responses in terms of generation time (rather than absolute time), do we see a consistent response across species of different lifespans?How do we scale up population-level responses to temporally variable stressors of organisms operating at different timescales to entire ecosystems? How do the order, frequency, and duration of multiple stressors affect their cumulative ecological impact on species’ populations? Does previous exposure to a particular stressor promote or erode resilience to future stressors (through ecological memory) and does this response depend on stressor similarity? Do temporally asynchronous multiple stressor events have outcomes (e.g., dominance of nonadditive affects) comparable with those of stressors acting in perfect synchrony? If we rescale responses in terms of generation time (rather than absolute time), do we see a consistent response across species of different lifespans? How do we scale up population-level responses to temporally variable stressors of organisms operating at different timescales to entire ecosystems? All authors are supported by Natural Environment Research Council (NERC) grants NE/M020843/1 and NE/S000348/1 . M.C.J. is also supported by NERC grant NE/V001396/1 . No interests are declared. time that exists independent of any events or processes. the acquisition or recombination of traits that improve performance or survival over multiple generations. physiological, morphological, or behavioral adjustments within a single organism that improve performance or survival under certain conditions. the effect of two stressors is less than the sum of their parts. a stressor present throughout the generation time of a focal organism. a stressor present for part of the generation time of a focal organism. the ability of past conditions and experiences to influence the future ecological responses of a population, community, or ecosystem. the average time between two consecutive generations in the lineages of a population. the phenotype of an individual is affected by the phenotype or environment of its parents. the effect of two stressors is more than the sum of their parts.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.1016/j.tree.2021.01.005",NA,NA,NA
"rayyan-100678292","Improving epidemic surveillance and response: big data is dead, long live big data",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Epidemics pose a growing threat. Our cities are increasingly densely populated, we are more connected than ever before, and in recent years we have witnessed successive waves of new (severe acute respiratory syndrome [SARS], Zika virus, Ebola virus, and now coronavirus disease 2019 [COVID-19]) and old (influenza) infectious disease threats causing global pandemics. Urgent investment in surveillance systems and global partnerships are needed to prepare for the pandemics that will continue to emerge in the coming decades. There has been discussion of the promise of integrating sophisticated epidemiological models and new big data streams—for example, from mobile phones, satellites, or social media—at various stages of the public health response, particularly in the context of epidemic forecasting and decision making.1George DB Taylor W Shaman J et al.Technology to advance infectious disease forecasting for outbreak management.Nat Commun. 2019; 10: 3932Crossref PubMed Scopus (26) Google Scholar These new data streams provide important, real-time information about travel patterns that spread disease and spatial shifts in populations at risk, which until recently have been very difficult to quantify on timescales relevant to a fast-moving epidemic. With growing mobility and increasing global connectivity, this information will be key to planning surveillance and containment strategies. In theory, with appropriate data sharing protocols in place, it should be possible to produce useful, up-to-date epidemic forecasts informed by these data streams. For this process to be effective, individuals from different institutions, including academia, industry, non-governmental organisations (NGOs), and governments, need to be in frequent communication. Key privacy concerns must be addressed for the routine use of new data streams, in particular the most appropriate way to robustly aggregate these data streams to ensure the anonymity of individuals. But even if privacy is addressed, there are additional structural challenges to the translation of new approaches in a decision-making context. Here, I focus on three of these challenges as they pertain to creating useful epidemic forecasts during an outbreak. The first challenge is that incentives across the analytical pipeline are misaligned.2Kahn R Mahmud AS Schroeder A et al.Rapid forecasting of cholera risk in Mozambique: translational challenges and opportunities.Prehosp Disaster Med. 2019; 34: 557-562Crossref PubMed Scopus (11) Google Scholar Academics are largely incentivised to write scientific articles and to fund their work through individually led grants. These activities are not conducive to the rapid response to a crisis (although many academics do respond), or to sustained engagement and training of corporate and government teams. Companies are incentivised by profit, and are rightly beholden to national regulatory frameworks and the public with respect to the data they collect. Ministries of health have complex relationships with both the companies that have access to personal data and with the public. They face many competing health priorities and complex political choices when it comes to open sharing of epidemiological data. Disease control programmes are often hampered by limited capacity and high turnover of personnel, and health workers on the ground during an epidemic have their hands full responding to the immediate crisis and might not be trained or incentivised to report epidemiological data accurately. Taken together, these incentive structures create multiple barriers to rapid data generation and the development of streamlined epidemic forecasting systems that use these new data types. The second challenge for implementing real-time epidemic forecasting is the gap between technological or methodological innovation, which often occurs in academic settings in high-income countries, and implementation in field settings, frequently done by NGOs or governments in low-income and middle-income countries. Many funders have adopted a financing model intended to spur innovation through short-term pilot projects that place greater emphasis on the novelty of a technology or method than on the validation of its impact, but this approach exacerbates this separation. Pilot funding also fails to acknowledge the long timelines required to engage with health systems effectively and to measure health impact rigorously, instead promoting one-off bilateral collaborative projects that do not scale up and are not sustained after the lifetime of the project. Much more investment is needed in the validation, implementation, and scaling up of innovations in close intellectual partnership with stakeholders who are responsible for delivering them,3Gates B The next epidemic—lessons from Ebola.N Engl J Med. 2015; 372: 1381-1384Crossref PubMed Scopus (172) Google Scholar rather than continuing to fund the proliferation of solutions divorced from the problems themselves. In a recent editorial, Seye Abimbola noted that ""while the gulf between discovery and delivery exists in other fields, what makes global health peculiar is that discoveries and the decisions on whether or how to deliver them are typically made at a distance, removed from the realities of their targets or intended beneficiaries.""4Abimbola S On the meaning of global health and the role of global health journals.Int Health. 2018; 10: 63-65Crossref PubMed Scopus (49) Google Scholar The third challenge is methodological: epidemic forecasting is inherently uncertain. There is sometimes an underlying assumption in the big data and artificial intelligence (AI) narrative that complex simulation models and mobile phone data or statistical covariates can bypass the need for the collection of basic epidemiological information. However, for emerging outbreaks—with COVID-19 highlighting this point—we often lack accurate data about case counts and biological processes driving an epidemic, let alone the behavioural responses of people affected, making it challenging to swiftly adapt or interpret very complex models on the spatiotemporal scales relevant for decision making. Arguably, the most useful frameworks will tend to be simple,5Viboud C Sun K Gaffey R et al.The RAPIDD ebola forecasting challenge: synthesis and lessons learnt.Epidemics. 2018; 22: 13-21Crossref PubMed Scopus (126) Google Scholar, 6May RM Uses and abuses of mathematics in biology.Science. 2004; 303: 790-793Crossref PubMed Scopus (302) Google Scholar both because of the need for flexible models that yield rapid answers given the large uncertainty surrounding epidemiological data during an emergency, and because simple models are more easily interpreted and communicated.2Kahn R Mahmud AS Schroeder A et al.Rapid forecasting of cholera risk in Mozambique: translational challenges and opportunities.Prehosp Disaster Med. 2019; 34: 557-562Crossref PubMed Scopus (11) Google Scholar Clear communication of both the value and limitations of model outputs is a prerequisite to their useful deployment, but is often absent. Since policy makers generally do not have in-depth modelling expertise, lack of clear communication risks two negative outcomes: believe models without scepticism and decisions will be misinformed, or dismiss modelling out of hand and fail to use the evidence we have to contain outbreaks as effectively as possible. Decisions must be made quickly during epidemics based on patchy and uncertain data, and models can be powerful tools to help guide them. Despite the challenges above, ongoing advances in computational power, methods, and new data streams offer genuine hope for better surveillance and useful forecasting systems. New data sources at our disposal include not only the passively observed big data streams from mobile phones but also detailed environmental data and local sensor information from distributed devices, internet search information, pathogen genomic data that can be generated rapidly during an outbreak to inform the response,7Armstrong GL MacCannell DR Taylor J et al.Pathogen genomics in public health.N Engl J Med. 2019; 381: 2569-2580Crossref PubMed Scopus (110) Google Scholar and crowd-sourced approaches to monitoring rapidly evolving emergencies.8Schiavo R May Leung M Brown M Communicating risk and promoting disease mitigation measures in epidemics and emerging disease settings.Pathog Glob Health. 2014; 108: 76-94Crossref PubMed Scopus (32) Google Scholar Data sharing platforms and standardised aggregation approaches that protect the privacy of personal data are being developed,9de Montjoye YA Gambs S Blondel V et al.On the privacy-conscientious use of mobile phone data.Sci Data. 2018; 5: 180286Crossref PubMed Scopus (74) Google Scholar and increasing internet connectivity allows for rapid data transfer and communication between geographically disparate teams of responders. Methodologically, powerful ensemble modelling approaches are being developed that combine multiple forecasts to minimise uncertainty.5Viboud C Sun K Gaffey R et al.The RAPIDD ebola forecasting challenge: synthesis and lessons learnt.Epidemics. 2018; 22: 13-21Crossref PubMed Scopus (126) Google Scholar, 10Ray EL Reich NG Prediction of infectious disease epidemics via weighted density ensembles.PLoS Comput Biol. 2018; 14: e1005910Crossref PubMed Scopus (72) Google Scholar We have seen an unprecedented, collaborative approach unfolding in response to the COVID-19 outbreak between academic groups: for example, using Twitter and other platforms to share, analyse, and openly discuss the implications of new data as they come out. (Ironically, managing the disinformation that also proliferates on social media during emergencies will probably become one of the most important issues for epidemic containment moving forward.) These innovations will remain dislocated and impractical until the challenges above are addressed. Encouragingly, all three issues could be improved by moving much of the focus of funding and expertise to the populations most vulnerable to epidemics. The unpredictable nature of epidemics and the increasingly technical components of these approaches means that flexible, distributed teams of people who span the analytical and operational aspects of the outbreak response will be needed if we are to use new big data approaches to complement and clarify the small data—epidemiological, geographical, and social—that are essential for the development of meaningful forecasts. Regional or local teams with analytical training and ongoing relationships with government and industry partners should lead to flexible modelling approaches that leverage large curated datasets and international analytical expertise, building on collaborations developed when there are no emerging or ongoing outbreaks.5Viboud C Sun K Gaffey R et al.The RAPIDD ebola forecasting challenge: synthesis and lessons learnt.Epidemics. 2018; 22: 13-21Crossref PubMed Scopus (126) Google Scholar This approach could also help to alleviate some of the political issues associated with data sharing, even when crucial information cannot be shared publicly. For this concept to work, substantial long-term investment is needed to train and support individuals, particularly in low-income and middle-income countries, to fill what is currently a major gap in the analytical pipeline. Indeed, if we are to confront future epidemics with the best tools at our disposal, a new kind of interdisciplinary training—distinct from traditional Master of Public Health (MPH) and graduate programmes—is needed, as well as the development of non-traditional public health careers, to foster individuals who can direct flexible forecasting efforts, engage with governments and industry partners, validate approaches, and create channels for communication. I declare no competing interests.","RAYYAN-INCLUSION: {""Erik""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.1016/s2589-7500(20)30059-5",NA,NA,NA
"rayyan-130075002","The Application Trend of Digital Finance and Technological Innovation in the Development of Green Economy",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Based on the perspective of digital finance and technological innovation, this paper analyzes its application in economic development, green economy, and sustainable development. With the continuous development of technological economy, methods such as artificial intelligence, Internet of Things, big data, and cloud computing become increasingly mature. Economic development is inseparable from the empowerment of technology. In this paper, firstly, we introduce the basic concepts and main forms of digital finance and technological economy and list the cutting-edge technologies including blockchain, VR, sharing economy, and other modes. Then, we analyze the application trend of technology economy. Finally, we analyze the examples of digital finance and technological innovation in detail, including tourism economy, digital marketing, sharing economy, smart city, digital healthcare, and personalized education, three hot topics of technology intersection and integration. In the end, we put forward prospects for the development of a digital economy, digital finance, and technological innovation.","RAYYAN-INCLUSION: {""Maria""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.1155/2022/1064558",NA,NA,NA
"rayyan-130075006","Unpacking data-centric geotechnics",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"The purpose of this paper (presented online as a keynote lecture at the 25th Annual Indonesian Geotechnical Conference on 10 Nov 2021) is to broadly conceptualize the agenda for data-centric geotechnics, an emerging field that attempts to prepare geotechnical engineering for digital transformation. The agenda must include (1) development of methods that make sense of all real-world data (not selective input data for a physical model), (2) offering insights of significant value to critical real-world decisions for current or future practice (not decisions for an ideal world or decisions of minor concern to geotechnical engineers), and (3) sensitivity to the physical context of geotechnics (not abstract data-driven analysis connected to geotechnics in a peripheral way, i.e., engagement with the knowledge and experience base should be substantial). These three elements are termed “data centricity”, “fit for (and transform) practice”, and “geotechnical context” in the agenda. Given that a knowledge of the site is central to any geotechnical engineering project, data-driven site characterization (DDSC) must constitute one key application domain in data-centric geotechnics, although other infrastructure lifecycle phases such as project conceptualization, design, construction, operation, and decommission/reuse would benefit from data-informed decision support as well. One part of DDSC that addresses numerical soil data in a site investigation report and soil property databases is pursued under Project DeepGeo. In principle, the source of data can also go beyond site investigation, and the type of data can go beyond numbers, such as categorical data, text, audios, images, videos, and expert opinion. The purpose of Project DeepGeo is to produce a 3D stratigraphic map of the subsurface volume below a full-scale project site and to estimate relevant engineering properties at each spatial point based on actual site investigation data and other relevant Big Indirect Data (BID). Uncertainty quantification is necessary, as current real-world data is insufficient, incomplete, and/or not directly relevant to construct a deterministic map. The value of a deterministic map for decision support is debatable. The computational cost to do this for a 3D true scale subsurface volume must be reasonable. Ultimately, geotechnical structures need to be a part of a completely smart infrastructure that fits the circular economy and need to focus on delivering service to end-users and the community from project conceptualization to decommission/reuse with full integration to smart city and smart society. Although current geotechnical practice has been very successful in taking “calculated risk” informed by limited data, imperfect theories, prototype testing, observations, among others and exercising judicious caution and engineering judgment, there is no clear pathway forward to leverage on big data and digital technologies such as machine learning, BIM, and digital twin to meet more challenging needs such as sustainability and resilience engineering.","RAYYAN-INCLUSION: {""Maria""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.1016/j.undsp.2022.04.001",NA,NA,NA
"rayyan-130075010","Mapping Human Activity Volumes Through Remote Sensing Imagery",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"The spatial concentration of the human activity is a crucial indication of socioeconomic vitality. Accurately mapping activity volumes is fundamental to support the regional sustainable development. Current approaches rely on mobile positioning data, which record information about human daily activity but are inaccessible in most cities due to privacy and data sharing concerns. Alternative methods are needed to provide more generalized predictions on extensive areas while maintaining low cost. This study demonstrates how remote sensing imagery can be used through an end-to-end deep learning framework for reliable estimates of human activity volumes. The neighbor effect, representing the inherent nature of spatial autocorrelation in the volumes, is incorporated to improve the network. The proposed model exhibits strong predictive power and demonstrates great explainability of physical environment on variations of activity volumes. Landscape interpretations based on hierarchical features provide both object-based and region-based insights into the coevolvement of landscape and human activity. Our findings indicate the possibility of extensively predicting activity volumes, especially in areas with limited access to mobile data, and provide support for the promising framework to better comprehend broad aspects of the human society from observable physical environments.","RAYYAN-INCLUSION: {""Maria""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.1109/jstars.2020.3023730",NA,NA,NA
"rayyan-131797036","Tourism, urbanization and natural resources rents matter for environmental sustainability: The leading role of AI and ICT on sustainable development goals in the digital era",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"","RAYYAN-INCLUSION: {""Maria""=>""Included""} | RAYYAN-LABELS: anthropocene | USER-NOTES: {""Maria""=>[""In the era of development, the world is facing severe challenges, and environmental degradation is one of them. However, the globe has tried to introduce several initiatives to fight for environmental sustainability, such as the Sustainable Development Goals. The leading role of the proposed goals is to balance development and environmental anxiety. Therefore, to these issues, artificial intelligence and technological advancements play a vital role in the natural resource economy in the digital age. Policy analysts are always looking for solutions and have come up with several viable remedies to this problem. Consequently, information & communication technology (ICT) plays a significant role in sustainability in the digital era. However, under the theme of natural resource sustainability, the effectiveness of ICT has a significant impact on sustainability."", ""Accordingly, the current study investigates the long-run effect of income per capita, tourism, natural resources rents, urbanization, and ICT on environmental sustainability in 36 OECD economies from 2000 to 2018. The current research employs an Augmented Mean Group (AMG) and two-step GMM to investigate the study's objectives. Results show the positive contribution of urbanization, natural resources, and tourism to CO2 emissions, while ICT reduces emissions. Besides, an inverted EKC curve is also validated for selected economies. In addition, the moderate effect of ICT on urbanization, natural resources, and tourism shows a significant decline in CO2 emissions. In light of the findings, this study recommends several crucial measures for environmental sustainability.""]}","https://doi.org/10.1016/j.resourpol.2023.103445",NA,NA,NA
"rayyan-131797071","Artificial Intelligence and Ten Societal Megatrends: An Exploratory Study Using GPT-3",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"This paper examines the potential of artificial intelligence (AI) to address societal megatrends, with a specific focus on OpenAI’s Generative Pre-Trained Transformer 3 (GPT-3). To do this, we conducted an analysis using GPT-3 in order to explore the benefits of AI for digitalization, urbanization, globalization, climate change, automation and mobility, global health issues, and the aging population. We also looked at emerging markets as well as sustainability in this study. Interaction with GPT-3 was conducted solely through prompt questions, and generated responses were analyzed. Our results indicate that AI can significantly improve our understanding of these megatrends by providing insights into how they develop over time and which solutions could be implemented. Further research is needed to determine how effective AI will be in addressing them successfully, but initial findings are encouraging. Our discussion focuses on the implications of our findings for society going forward and suggests that further investigation should be conducted into how best to utilize new technologies such as GPT-3 when tackling these challenges. Lastly, we conclude that, while there is still much work left to do before any tangible effects can be seen from utilizing AI tools such as GPT-3 on societal megatrends, early indications suggest it may have a positive impact if used correctly.","RAYYAN-INCLUSION: {""Maria""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.3390/systems11030120",NA,NA,NA
"rayyan-131797138","Decision Tree-Based Ensemble Model for Predicting National Greenhouse Gas Emissions in Saudi Arabia",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Greenhouse gas (GHG) emissions must be precisely estimated in order to predict climate change and achieve environmental sustainability in a country. GHG emissions are estimated using empirical models, but this is difficult since it requires a wide variety of data and specific national or regional parameters. In contrast, artificial intelligence (AI)-based methods for estimating GHG emissions are gaining popularity. While progress is evident in this field abroad, the application of an AI model to predict greenhouse gas emissions in Saudi Arabia is in its early stages. This study applied decision trees (DT) and their ensembles to model national GHG emissions. Three AI models, namely bagged decision tree, boosted decision tree, and gradient boosted decision tree, were investigated. Results of the DT models were compared with the feed forward neural network model. In this study, population, energy consumption, gross domestic product (GDP), urbanization, per capita income (PCI), foreign direct investment (FDI), and GHG emission information from 1970 to 2021 were used to construct a suitable dataset to train and validate the model. The developed model was used to predict Saudi Arabia’s national GHG emissions up to the year 2040. The results indicated that the bagged decision tree has the highest coefficient of determination (R2) performance on the testing dataset, with a value of 0.90. The same method also has the lowest root mean square error (0.84 GtCO2e) and mean absolute percentage error (0.29 GtCO2e), suggesting that it exhibited the best performance. The model predicted that GHG emissions in 2040 will range between 852 and 867 million tons of CO2 equivalent. In addition, Shapley analysis showed that the importance of input parameters can be ranked as urbanization rate, GDP, PCI, energy consumption, population, and FDI. The findings of this study will aid decision makers in understanding the complex relationships between the numerous drivers and the significance of diverse socioeconomic factors in defining national GHG inventories. The findings will enhance the tracking of national GHG emissions and facilitate the concentration of appropriate activities to mitigate climate change.","RAYYAN-INCLUSION: {""Maria""=>""Included""} | RAYYAN-LABELS: climate","https://doi.org/10.3390/app13063832",NA,NA,NA
"rayyan-131797140","The Convergence of Artificial Intelligence and Blockchain: The State of Play and the Road Ahead",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"Artificial intelligence (AI) and blockchain technology have emerged as increasingly prevalent and influential elements shaping global trends in Information and Communications Technology (ICT). Namely, the synergistic combination of blockchain and AI introduces beneficial, unique features with the potential to enhance the performance and efficiency of existing ICT systems. However, presently, the confluence of these two disruptive technologies remains in a rather nascent stage, undergoing continuous exploration and study. In this context, the work at hand offers insight regarding the most significant features of the AI and blockchain intersection. Sixteen outstanding, recent articles exploring the combination of AI and blockchain technology have been systematically selected and thoroughly investigated. From them, fourteen key features have been extracted, including data security and privacy, data encryption, data sharing, decentralized intelligent systems, efficiency, automated decision systems, collective decision making, scalability, system security, transparency, sustainability, device cooperation, and mining hardware design. Moreover, drawing upon the related literature stemming from major digital databases, we constructed a timeline of this technological convergence comprising three eras: emerging, convergence, and application. For the convergence era, we categorized the pertinent features into three primary groups: data manipulation, potential applicability to legacy systems, and hardware issues. For the application era, we elaborate on the impact of this technology fusion from the perspective of five distinct focus areas, from Internet of Things applications and cybersecurity, to finance, energy, and smart cities. This multifaceted, but succinct analysis is instrumental in delineating the timeline of AI and blockchain convergence and pinpointing the unique characteristics inherent in their integration. The paper culminates by highlighting the prevailing challenges and unresolved questions in blockchain and AI-based systems, thereby charting potential avenues for future scholarly inquiry.","RAYYAN-INCLUSION: {""Maria""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.3390/info15050268",NA,NA,NA
"rayyan-131797171","Empowering Wildlife Guardians: An Equitable Digital Stewardship and Reward System for Biodiversity Conservation Using Deep Learning and 3/4G Camera Traps",NA,1,1,NA,NA,NA,NA,NA,NA,NA,"en",NA,NA,"The biodiversity of our planet is under threat, with approximately one million species expected to become extinct within decades. The reason: negative human actions, which include hunting, overfishing, pollution, and the conversion of land for urbanisation and agricultural purposes. Despite significant investment from charities and governments for activities that benefit nature, global wildlife populations continue to decline. Local wildlife guardians have historically played a critical role in global conservation efforts and have shown their ability to achieve sustainability at various levels. In 2021, COP26 recognised their contributions and pledged USD 1.7 billion per year; however this is a fraction of the global biodiversity budget available (between USD 124 billion and USD 143 billion annually) given they protect 80% of the planets biodiversity. This paper proposes a radical new solution based on “Interspecies Money”, where animals own their own money. Creating a digital twin for each species allows animals to dispense funds to their guardians for the services they provide. For example, a rhinoceros may release a payment to its guardian each time it is detected in a camera trap as long as it remains alive and well. To test the efficacy of this approach, 27 camera traps were deployed over a 400 km2 area in Welgevonden Game Reserve in Limpopo Province in South Africa. The motion-triggered camera traps were operational for ten months and, using deep learning, we managed to capture images of 12 distinct animal species. For each species, a makeshift bank account was set up and credited with GBP 100. Each time an animal was captured in a camera and successfully classified, 1 penny (an arbitrary amount—mechanisms still need to be developed to determine the real value of species) was transferred from the animal account to its associated guardian. The trial demonstrated that it is possible to achieve high animal detection accuracy across the 12 species with a sensitivity of 96.38%, specificity of 99.62%, precision of 87.14%, F1 score of 90.33%, and an accuracy of 99.31%. The successful detections facilitated the transfer of GBP 185.20 between animals and their associated guardians.","RAYYAN-INCLUSION: {""Maria""=>""Included""} | RAYYAN-LABELS: anthropocene","https://doi.org/10.3390/rs15112730",NA,NA,NA
